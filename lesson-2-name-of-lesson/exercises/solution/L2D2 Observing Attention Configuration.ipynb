{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Observing Attention Configuration (MHA, MQA, GQA)\n",
    "\n",
    "**Goal:** This demo shows how to inspect a model's configuration to determine its attention mechanism (Multi-Head, Multi-Query, or Grouped-Query Attention). Understanding this is key to predicting a model's memory usage for the KV Cache.\n",
    "\n",
    "We will:\n",
    "1. **Setup:** Install libraries and log in to Hugging Face.\n",
    "2. **Load Configuration:** Efficiently load only the model's configuration file.\n",
    "3. **Inspect Heads:** Extract the number of query heads and key/value heads.\n",
    "4. **Determine Type:** Apply logic to identify the attention type.\n",
    "5. **Interpret Results:** Understand the implications of the finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we log in to Hugging Face to get access to models like Llama.\n",
    "\n",
    "**Action Required:**\n",
    "1. Go to `huggingface.co/settings/tokens`.\n",
    "2. Create a new token with \"read\" permissions.\n",
    "3. Paste your token into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your Hugging Face token here\n",
    "HF_TOKEN = \"<YOUR_HUGGING_FACE_TOKEN>\"\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, import the libraries we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model Configuration\n",
    "\n",
    "For this task, we don't need to load the entire model (which can be billions of parameters and take up a lot of VRAM). We only need its architectural details, which are stored in the `config.json` file. The `AutoConfig` class lets us load just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration for: meta-llama/Llama-3.2-1B\n",
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(f\"Loading configuration for: {model_name}\")\n",
    "try:\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    print(\"Configuration loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inspect Key Attention Attributes\n",
    "\n",
    "The two most important attributes for determining the attention type are:\n",
    "- `num_attention_heads`: The number of attention heads for the **Query (Q)** projections.\n",
    "- `num_key_value_heads`: The number of attention heads for the **Key (K) and Value (V)** projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Attributes for 'meta-llama/Llama-3.2-1B':\n",
      "  Number of Query Heads (N_q):         32\n",
      "  Number of Key/Value Heads (N_kv):    8\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of heads from the configuration object\n",
    "num_q_heads = config.num_attention_heads\n",
    "num_kv_heads = config.num_key_value_heads # This field is specific to MQA/GQA models\n",
    "\n",
    "print(f\"Extracted Attributes for '{model_name}':\")\n",
    "print(f\"  Number of Query Heads (N_q):         {num_q_heads}\")\n",
    "print(f\"  Number of Key/Value Heads (N_kv):    {num_kv_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Determine the Attention Type\n",
    "\n",
    "Now we can apply simple logic based on the two numbers we just extracted:\n",
    "- If `N_q == N_kv`, it's **Multi-Head Attention (MHA)**.\n",
    "- If `N_kv == 1`, it's **Multi-Query Attention (MQA)**.\n",
    "- If `1 < N_kv < N_q`, it's **Grouped-Query Attention (GQA)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the head counts, the detected attention type is: Grouped-Query Attention (GQA)\n"
     ]
    }
   ],
   "source": [
    "attention_type = \"Unknown\"\n",
    "\n",
    "if num_kv_heads == num_q_heads:\n",
    "    attention_type = \"Multi-Head Attention (MHA)\"\n",
    "elif num_kv_heads == 1:\n",
    "    attention_type = \"Multi-Query Attention (MQA)\"\n",
    "elif 1 < num_kv_heads < num_q_heads:\n",
    "    attention_type = \"Grouped-Query Attention (GQA)\"\n",
    "\n",
    "print(f\"Based on the head counts, the detected attention type is: {attention_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Grouping Factor for GQA\n",
    "\n",
    "Since we detected GQA, we can calculate the *grouping factor*—that is, how many Query heads share a single Key/Value head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping Factor = 32 (Query Heads) / 8 (KV Heads) = 4\n",
      "This means every 4 Query heads share a single set of Key and Value heads.\n"
     ]
    }
   ],
   "source": [
    "if attention_type == \"Grouped-Query Attention (GQA)\":\n",
    "    if num_q_heads % num_kv_heads == 0:\n",
    "        group_factor = num_q_heads // num_kv_heads\n",
    "        print(f\"Grouping Factor = {num_q_heads} (Query Heads) / {num_kv_heads} (KV Heads) = {group_factor}\")\n",
    "        print(f\"This means every {group_factor} Query heads share a single set of Key and Value heads.\")\n",
    "    else:\n",
    "        print(\"GQA detected, but heads are not evenly divisible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Interpretation\n",
    "\n",
    "For the model **`meta-llama/Llama-3.2-1B`**, we have confirmed it uses **Grouped-Query Attention (GQA)**.\n",
    "\n",
    "**Why this matters:**\n",
    "- **Reduced Memory:** A standard MHA model would have needed 32 sets of Key/Value heads in its KV Cache. By using only 8, GQA reduces the KV Cache size by a factor of 4 (32 / 8).\n",
    "- **Faster Inference:** A smaller KV Cache means less data needs to be read from slow GPU memory (HBM) at each generation step, which reduces the memory bandwidth bottleneck and speeds up inference.\n",
    "- **Longer Context:** The memory savings from GQA allow the model to handle longer sequences of text before running out of VRAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
