{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Exercise 2: Analyzing GQA Memory Savings by Comparing Llama-3.2-1B and GPT-2 XL\n",
    "\n",
    "**Goal:** Understand and quantify the memory efficiency of GQA by comparing the KV cache sizes of Llama-3.2-1B (GQA) and GPT-2 XL (MHA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers accelerate datasets rouge_score kagglehub evaluate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# TODO: Replace \"YOUR_HUGGING_FACE_TOKEN_HERE\" with your actual Hugging Face token\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HUGGING_FACE_TOKEN_HERE\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM # AutoModel for dtype inference\n",
    "\n",
    "# --- Configuration ---\n",
    "# Model 1: Expected to use GQA\n",
    "model_name_gqa = \"meta-llama/Llama-3.2-1B\"\n",
    "# Model 2: Expected to use MHA\n",
    "model_name_mha = \"openai-community/gpt2-xl\" \n",
    "\n",
    "print(f\"Comparing GQA Model: {model_name_gqa}\")\n",
    "print(f\"With MHA Model: {model_name_mha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis Helper Function\n",
    "\n",
    "You will complete the `analyze_model_kv_cache` function below. This function should:\n",
    "1. Load the model's configuration.\n",
    "2. (Recommended) Temporarily load the model itself to accurately infer its `dtype`, then delete the model to free resources.\n",
    "3. Extract key parameters: `num_hidden_layers`, `num_attention_heads` (query heads), `num_key_value_heads`, `hidden_size`.\n",
    "4. Determine the size in bytes of the model's data type.\n",
    "5. Calculate the `head_dim`.\n",
    "6. Identify the attention mechanism type (MHA, MQA, or GQA) and the GQA grouping factor if applicable.\n",
    "7. Calculate the KV cache memory added **per layer, per generated token**.\n",
    "8. Calculate the **total** KV cache memory added **per generated token (across all layers)**.\n",
    "9. Return these values in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_kv_cache(model_name_to_analyze, model_label):\n",
    "    print(f\"\\n--- Analyzing {model_label}: {model_name_to_analyze} ---\")\n",
    "    config = None\n",
    "    model_dtype = None\n",
    "    \n",
    "    # TODO 1: Load model configuration and infer dtype\n",
    "    # Try to load the full model temporarily to get its actual dtype for accuracy.\n",
    "    # If it fails (e.g., due to memory), fall back to AutoConfig and assume a common dtype (e.g., torch.float16).\n",
    "    try:\n",
    "        pass # Replace with your loading logic\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load full model for {model_name_to_analyze} to infer dtype ({e}). Loading config only.\")\n",
    "        # config = _ \n",
    "        # model_dtype = _ \n",
    "        # print(f\"Config loaded. Assumed/Config dtype: {model_dtype}\")\n",
    "        pass # Replace with your fallback config loading logic\n",
    "\n",
    "    # TODO 2: Extract parameters from the configuration object (config)\n",
    "    # num_hidden_layers = _\n",
    "    # num_attention_heads_query = _\n",
    "    # num_key_value_heads_actual = _\n",
    "    # hidden_size_model = _\n",
    "    # Placeholders - replace with your extracted values\n",
    "    num_hidden_layers, num_attention_heads_query, num_key_value_heads_actual, hidden_size_model = 0,0,0,0\n",
    "\n",
    "    # TODO 3: Determine dtype_size_bytes based on model_dtype (2 for float16/bfloat16, 4 for float32)\n",
    "    dtype_size_bytes = 0 # Placeholder\n",
    "\n",
    "    print(f\"\\n  --- Extracted Configuration for {model_label} ---\")\n",
    "    # ... (print statements for extracted params go here)\n",
    "\n",
    "    # TODO 4: Calculate head_dim (hidden_size_model / num_attention_heads_query)\n",
    "    # Ensure it's an integer.\n",
    "    head_dim = 0 # Placeholder\n",
    "\n",
    "    # TODO 5: Determine attention_type_str (MHA, MQA, or GQA) and gqa_group_factor if GQA\n",
    "    attention_type_str = \"Unknown\" \n",
    "    gqa_group_factor = None\n",
    "\n",
    "    print(f\"  Identified Attention Type: {attention_type_str}\")\n",
    "\n",
    "    # TODO 6: Calculate actual KV cache size PER LAYER, PER TOKEN in bytes\n",
    "    # Formula: 2 (for K and V) * num_key_value_heads_actual * head_dim * dtype_size_bytes\n",
    "    size_actual_per_layer_per_token_bytes = 0 # Placeholder\n",
    "\n",
    "    # TODO 7: Calculate TOTAL actual KV cache size PER TOKEN (across all layers)\n",
    "    # Formula: size_actual_per_layer_per_token_bytes * num_hidden_layers\n",
    "    total_size_actual_per_token_bytes = 0 # Placeholder\n",
    "\n",
    "    results = {\n",
    "        \"model_name\": model_name_to_analyze,\n",
    "        \"label\": model_label,\n",
    "        \"L\": num_hidden_layers,\n",
    "        \"N_q\": num_attention_heads_query,\n",
    "        \"N_kv_actual\": num_key_value_heads_actual,\n",
    "        \"D_head\": head_dim,\n",
    "        \"dtype_size_bytes\": dtype_size_bytes,\n",
    "        \"attention_type\": attention_type_str,\n",
    "        \"cache_per_layer_per_token_bytes\": size_actual_per_layer_per_token_bytes,\n",
    "        \"total_cache_per_token_bytes\": total_size_actual_per_token_bytes,\n",
    "        # Add gqa_group_factor if you calculate it\n",
    "        \"gqa_group_factor\": gqa_group_factor \n",
    "    }\n",
    "    \n",
    "    # After implementing the TODOs, print the extracted parameters neatly\n",
    "    print(f\"  Number of Hidden Layers (L):           {results['L']}\")\n",
    "    print(f\"  Number of Query Heads (N_q):         {results['N_q']}\")\n",
    "    print(f\"  Number of Key/Value Heads (N_kv):    {results['N_kv_actual']}\")\n",
    "    print(f\"  Model Hidden Size (D_model):         {hidden_size_model}\")\n",
    "    print(f\"  Data Type Size (bytes):              {results['dtype_size_bytes']}\")\n",
    "    print(f\"  Calculated Head Dimension (D_head):    {results['D_head']}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gqa_model = analyze_model_kv_cache(model_name_gqa, \"Llama-3.2-1B (GQA)\")\n",
    "results_mha_model = analyze_model_kv_cache(model_name_mha, \"GPT-2 XL (MHA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Step\n",
    "\n",
    "Before proceeding to the detailed comparison, answer these questions based on your initial understanding and the configurations you just extracted (or will extract once your `analyze_model_kv_cache` function is complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- Prediction Step ---\")\n",
    "# TODO: After your analyze_model_kv_cache function is working and has populated results_gqa_model and results_mha_model:\n",
    "# 1. For Llama-3.2-1B: \n",
    "#    - Based on its N_q and N_kv, what attention type do you predict (MHA, MQA, or GQA)? \n",
    "#    - If GQA, what is its grouping factor?\n",
    "#    - How much smaller (as a factor) do you expect its per-layer KV cache to be compared to a hypothetical MHA version of *itself* (i.e., if it used N_q K/V heads)?\n",
    "#\n",
    "# 2. For GPT-2 XL: \n",
    "#    - Based on its N_q and N_kv, what attention type do you predict?\n",
    "#\n",
    "# 3. Comparison Prediction:\n",
    "#    - Do you expect Llama-3.2-1B's per-layer-per-token KV cache to be smaller, larger, or similar to GPT-2 XL's? Why?\n",
    "#      (Consider N_kv_actual, D_head, and dtype_size for both in your reasoning).\n",
    "\n",
    "# Write your predictions and reasoning as comments or in a separate markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis_results(res, label_override=None):\n",
    "    if not res:\n",
    "        print(f\"Could not analyze results for {label_override or 'model'}\")\n",
    "        return\n",
    "\n",
    "    label = label_override or res[\"label\"]\n",
    "    print(f\"\\n--- KV Cache Analysis for {label} ---\")\n",
    "    print(f\"  Attention Type: {res['attention_type']}\")\n",
    "    print(f\"  Parameters: L={res['L']}, N_q={res['N_q']}, N_kv_actual={res['N_kv_actual']}, D_head={res['D_head']}, dtype_size={res['dtype_size_bytes']} bytes\")\n",
    "    print(f\"  Actual KV Cache Memory Per Layer, Per Token: {res['cache_per_layer_per_token_bytes']} Bytes \"\n",
    "          f\"({res['cache_per_layer_per_token_bytes']/1024:.2f} KB)\")\n",
    "    print(f\"  Total Actual KV Cache Memory Per Token (all layers): {res['total_cache_per_token_bytes']/(1024*1024):.4f} MB\")\n",
    "    \n",
    "    # Internal saving factor for GQA/MQA models\n",
    "    if res['N_kv_actual'] < res['N_q'] and res['N_kv_actual'] > 0:\n",
    "        internal_saving = res['N_q'] / res['N_kv_actual']\n",
    "        print(f\"  Internal Saving Factor (vs. its own hypothetical MHA): {internal_saving:.2f}x\")\n",
    "\n",
    "print(\"\\n\\n--- DETAILED ANALYSIS & COMPARISON ---\")\n",
    "if results_gqa_model:\n",
    "    print_analysis_results(results_gqa_model)\n",
    "if results_mha_model:\n",
    "    print_analysis_results(results_mha_model)\n",
    "\n",
    "if results_gqa_model and results_mha_model:\n",
    "    gqa_cache_plt = results_gqa_model['cache_per_layer_per_token_bytes']\n",
    "    mha_cache_plt = results_mha_model['cache_per_layer_per_token_bytes']\n",
    "\n",
    "    print(\"\\n--- Direct Comparison (Per Layer, Per Token KV Cache) ---\")\n",
    "    print(f\"  {results_gqa_model['label']}: {gqa_cache_plt} Bytes/layer/token ({gqa_cache_plt/1024:.2f} KB)\")\n",
    "    print(f\"  {results_mha_model['label']}: {mha_cache_plt} Bytes/layer/token ({mha_cache_plt/1024:.2f} KB)\")\n",
    "\n",
    "    # TODO: Add a sentence here comparing gqa_cache_plt and mha_cache_plt.\n",
    "    #       Is one smaller? By what factor (mha_cache_plt / gqa_cache_plt)?\n",
    "    #       Relate this to their respective N_kv_actual, D_head, and dtype_size.\n",
    "    # Example print (you'll need to fill in the logic):\n",
    "    # if gqa_cache_plt < mha_cache_plt:\n",
    "    #     comparison_factor = mha_cache_plt / gqa_cache_plt\n",
    "    #     print(f\"  Observation: {results_gqa_model['label']}'s per-layer cache is ~{comparison_factor:.2f}x smaller than {results_mha_model['label']}'s.\")\n",
    "    # else: ...\n",
    "\n",
    "    print(\"\\n--- Practical Implication - Max Sequence Length Estimation ---\")\n",
    "    cache_gqa_total_mb_per_token = results_gqa_model['total_cache_per_token_bytes'] / (1024*1024)\n",
    "    cache_mha_total_mb_per_token = results_mha_model['total_cache_per_token_bytes'] / (1024*1024)\n",
    "    available_vram_for_cache_gb = 6 # Example: 6GB VRAM available JUST for KV cache\n",
    "    available_vram_for_cache_mb = available_vram_for_cache_gb * 1024\n",
    "\n",
    "    print(f\"\\nAssuming {available_vram_for_cache_gb} GB of VRAM is available *exclusively for the KV cache*:\")\n",
    "    if cache_gqa_total_mb_per_token > 0:\n",
    "        max_tokens_gqa = available_vram_for_cache_mb / cache_gqa_total_mb_per_token\n",
    "        print(f\"  {results_gqa_model['label']} (L={results_gqa_model['L']}) could theoretically support ~{int(max_tokens_gqa)} tokens.\")\n",
    "\n",
    "    if cache_mha_total_mb_per_token > 0:\n",
    "        max_tokens_mha = available_vram_for_cache_mb / cache_mha_total_mb_per_token\n",
    "        print(f\"  {results_mha_model['label']} (L={results_mha_model['L']}) could theoretically support ~{int(max_tokens_mha)} tokens.\")\n",
    "\n",
    "    # TODO: Add a discussion here:\n",
    "    #       - How do the per-layer and total per-token KV cache sizes, influenced by GQA vs MHA \n",
    "    #         (considering N_kv, D_head) and the total number of layers (L), affect the estimated \n",
    "    #         maximum sequence length capabilities for these two models?\n",
    "    #       - Which architectural choices make a bigger difference for this estimation?\n",
    "\n",
    "print(\"\\nExercise Complete. Fill in the TODOs and your analysis.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
