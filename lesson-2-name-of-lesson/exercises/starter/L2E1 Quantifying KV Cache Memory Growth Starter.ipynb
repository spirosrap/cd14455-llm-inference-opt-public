{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Exercise 1: Quantifying KV Cache Memory Growth\n",
    "\n",
    "**Goal:** Empirically measure and analyze the growth in peak GPU memory consumption during LLM inference as the number of generated tokens increases, thereby quantifying the memory footprint of the Key-Value (KV) Cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers accelerate matplotlib datasets rouge_score kagglehub evaluate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# TODO: Replace \"YOUR_HUGGING_FACE_TOKEN_HERE\" with your actual Hugging Face token\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HUGGING_FACE_TOKEN_HERE\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc # For garbage collection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "model_name = \"meta-llama/Llama-3.2-1B\" \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: This exercise requires a CUDA-enabled GPU for meaningful memory measurements. Results on CPU will not reflect KV cache growth on GPU.\")\n",
    "\n",
    "# TODO: Determine appropriate dtype\n",
    "if device == \"cuda\":\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        dtype = torch.bfloat16\n",
    "        print(\"Using torch.bfloat16\")\n",
    "    else:\n",
    "        dtype = torch.float16\n",
    "        print(\"Using torch.float16 (bf16 not supported)\")\n",
    "else:\n",
    "    dtype = torch.float32 # Default for CPU\n",
    "    print(\"Using torch.float32 for CPU\")\n",
    "\n",
    "prompt = \"The best way to optimize LLM inference is\"\n",
    "\n",
    "# TODO: Define a list of different max_new_tokens values to test.\n",
    "generation_lengths = [] \n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generation lengths to test: {generation_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Function to Measure Peak Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_gpu_memory_mb(device_to_check=\"cuda\"):\n",
    "    \"\"\"Records the peak GPU memory allocated since the last reset for the specified device.\"\"\"\n",
    "    if torch.cuda.is_available() and device_to_check == \"cuda\":\n",
    "        torch.cuda.synchronize() # Ensure all CUDA operations are complete\n",
    "        peak_mem_bytes = # TODO: check the documentation https://docs.pytorch.org/docs/stable/cuda.html for function that can give peak memory\n",
    "        return peak_mem_bytes / (1024 * 1024) # Convert to MB\n",
    "    return 0 # Return 0 if not on CUDA or for CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading model and tokenizer for {model_name}...\")\n",
    "try:\n",
    "    # TODO: Load the tokenizer using AutoTokenizer.from_pretrained()\n",
    "    tokenizer = None\n",
    "\n",
    "    # TODO: Set tokenizer.pad_token if it's None (e.g., tokenizer.pad_token = tokenizer.eos_token)\n",
    "\n",
    "    # TODO: Load the model using AutoModelForCausalLM.from_pretrained()\n",
    "    #       Specify torch_dtype=dtype and attn_implementation=\"sdpa\"\n",
    "    #       Move the model to the specified device (.to(device))\n",
    "    model = None\n",
    "\n",
    "    # TODO: Set the model to evaluation mode using model.eval()\n",
    "\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/tokenizer: {e}\")\n",
    "    # exit() # Consider exiting if loading fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Configuration & Measurement Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_memory_results_mb = []\n",
    "actual_tokens_generated_list = []\n",
    "generation_times = []\n",
    "\n",
    "print(\"\\n--- Measuring KV Cache Memory Growth ---\")\n",
    "\n",
    "if device == \"cuda\" and model is not None: # Proceed only if model loaded and on CUDA\n",
    "    # Initial memory measurement (model loaded, no generation yet)\n",
    "    torch.cuda.reset_peak_memory_stats(device) # Reset before taking baseline\n",
    "    _ = model(torch.tensor([[0]], device=device)) # Dummy forward pass to ensure model is fully on GPU\n",
    "    torch.cuda.synchronize(device)\n",
    "    initial_peak_memory_mb = get_peak_gpu_memory_mb(device)\n",
    "    print(f\"Initial peak memory (model loaded on GPU): {initial_peak_memory_mb:.2f} MB\")\n",
    "\n",
    "    for length in generation_lengths:\n",
    "        print(f\"\\nGenerating {length} new tokens...\")\n",
    "        \n",
    "        # TODO: Tokenize the prompt using the loaded tokenizer.\n",
    "        #       Ensure tensors are moved to the correct device.\n",
    "        # inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        inputs = None # Placeholder\n",
    "\n",
    "        if inputs is None:\n",
    "            print(\"  Skipping due to uninitialized inputs.\")\n",
    "            peak_memory_results_mb.append(float('nan'))\n",
    "            actual_tokens_generated_list.append(0)\n",
    "            generation_times.append(float('nan'))\n",
    "            continue\n",
    "\n",
    "        # TODO: Reset GPU memory stats before this specific generation run\n",
    "        # torch.cuda.reset_peak_memory_stats(device)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # TODO: Generate tokens using model.generate()\n",
    "                #       - Pass inputs[\"input_ids\"]\n",
    "                #       - Set max_new_tokens=length\n",
    "                #       - Ensure use_cache=True (it's usually default but good to be explicit)\n",
    "                #       - Set pad_token_id=tokenizer.pad_token_id\n",
    "                outputs = None # Placeholder\n",
    "\n",
    "                if outputs is None:\n",
    "                    raise ValueError(\"Outputs from model.generate() is None.\")\n",
    "\n",
    "                # TODO: Calculate actual number of new tokens generated\n",
    "                num_input_tokens = # tokens in the input prompt\n",
    "                num_output_tokens = # tokens in the output\n",
    "                actual_new_tokens = num_output_tokens - num_input_tokens\n",
    "                actual_tokens_generated_list.append(actual_new_tokens)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error during generation for length {length}: {e}\")\n",
    "                peak_memory_results_mb.append(float('nan')) \n",
    "                actual_tokens_generated_list.append(0)\n",
    "                generation_times.append(float('nan'))\n",
    "                continue\n",
    "\n",
    "        # TODO: Synchronize CUDA operations before recording memory and time\n",
    "        # torch.cuda.synchronize(device)\n",
    "        end_time = time.perf_counter()\n",
    "        gen_time = end_time - start_time\n",
    "        generation_times.append(gen_time)\n",
    "\n",
    "        # TODO: Record peak GPU memory allocated during THIS generation run\n",
    "        current_peak_memory_mb = 0 # Placeholder\n",
    "        peak_memory_results_mb.append(current_peak_memory_mb)\n",
    "\n",
    "        print(f\"  Actual new tokens generated: {actual_new_tokens}\")\n",
    "        print(f\"  Peak memory allocated for this generation: {current_peak_memory_mb:.2f} MB\")\n",
    "        print(f\"  Generation time: {gen_time:.4f} seconds\")\n",
    "\n",
    "        # Cleanup\n",
    "        del outputs\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "else:\n",
    "    print(\"Skipping measurement loop: Model not loaded or not on CUDA device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deliverables: Analysis and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Results Summary ---\")\n",
    "print(\"Max New Tokens | Actual New Tokens | Peak Memory (MB) during Gen | Time (s)\")\n",
    "print(\"---------------|-------------------|-----------------------------|----------\")\n",
    "for i, requested_length in enumerate(generation_lengths):\n",
    "    if i < len(peak_memory_results_mb) and i < len(actual_tokens_generated_list) and i < len(generation_times):\n",
    "        actual_len = actual_tokens_generated_list[i]\n",
    "        mem_val = peak_memory_results_mb[i]\n",
    "        time_val = generation_times[i]\n",
    "        print(f\"{requested_length:15} | {actual_len:17} | {mem_val:27.2f} | {time_val:8.4f}\")\n",
    "    else:\n",
    "        # This case handles if a generation run failed and wasn't recorded\n",
    "        print(f\"{requested_length:15} | {'N/A':17} | {'N/A':27} | {'N/A':8}\")\n",
    "\n",
    "# TODO: Trend Analysis:\n",
    "# 1. Describe the relationship you observe between 'Actual New Tokens' and 'Peak Memory (MB) during Gen'.\n",
    "#    Is it linear? Does it increase as expected?\n",
    "#    (You can also consider 'Peak Memory (MB) during Gen' - initial_peak_memory_mb to see the growth above the model size).\n",
    "\n",
    "# TODO: Estimate Memory per Token:\n",
    "# 2. Calculate an estimate of how much *additional* memory the KV cache consumes per *actually generated new token*.\n",
    "#    - Select two data points from your results (e.g., generation for 50 tokens and 150 tokens).\n",
    "#    - Calculate ΔMemory = PeakMemory_LongerRun - PeakMemory_ShorterRun\n",
    "#    - Calculate ΔTokens = ActualTokens_LongerRun - ActualTokens_ShorterRun\n",
    "#    - Estimate MemoryPerToken = ΔMemory / ΔTokens (if ΔTokens > 0)\n",
    "#    - Print your chosen data points and the calculated memory per token.\n",
    "\n",
    "# TODO: Practical Impact Discussion:\n",
    "# 3. Briefly discuss how this growing KV cache memory footprint can become a limiting factor for generating \n",
    "#    very long sequences, especially on GPUs with constrained VRAM. How does this relate to the concept \n",
    "#    of \"context window limits\" in practice (even if the model theoretically supports a large window)?\n",
    "\n",
    "# --- Plotting ---\n",
    "if device == \"cuda\" and any(m > 0 for m in peak_memory_results_mb if isinstance(m, (int, float)) and not torch.isnan(torch.tensor(m))):\n",
    "    # Filter out NaN or non-numeric before plotting\n",
    "    valid_indices = [i for i, mem in enumerate(peak_memory_results_mb) \n",
    "                     if isinstance(mem, (int, float)) and not torch.isnan(torch.tensor(mem))]\n",
    "    plot_lengths = [actual_tokens_generated_list[i] for i in valid_indices]\n",
    "    plot_memory = [peak_memory_results_mb[i] for i in valid_indices]\n",
    "    \n",
    "    if plot_lengths and plot_memory and len(plot_lengths) == len(plot_memory):\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(plot_lengths, plot_memory, marker='o', linestyle='-')\n",
    "            plt.title(f'Peak GPU Memory vs. Number of Actual New Tokens Generated\\nModel: {model_name}')\n",
    "            plt.xlabel('Number of Actual New Tokens Generated')\n",
    "            plt.ylabel('Peak GPU Memory Allocated During Generation (MB)')\n",
    "            plt.grid(True)\n",
    "            # plt.xticks(plot_lengths) # May be too crowded if many points\n",
    "            plt.tight_layout()\n",
    "            plot_filename = \"kv_cache_memory_growth.png\"\n",
    "            plt.savefig(plot_filename)\n",
    "            print(f\"\\nPlot saved to {plot_filename}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate plot: {e}. Ensure matplotlib is installed.\")\n",
    "    else:\n",
    "        print(\"Not enough valid data points to plot.\")\n",
    "elif device != \"cuda\":\n",
    "    print(\"\\nPlotting is designed for CUDA memory measurements.\")\n",
    "\n",
    "print(\"\\nExercise Complete. Please fill in the TODO sections for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
