{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Exercise 1: Profile GPT-2 and Analyze Top Operators\n",
    "\n",
    "**Goal:** Apply the profiling setup learned in the demo, run it on a smaller model (GPT-2), and practice accessing and interpreting the detailed profiler results to identify key performance bottlenecks.\n",
    "\n",
    "**Task Overview:**\n",
    "1.  Set the model name to `\"gpt2\"`.\n",
    "2.  Re-run the CPU and GPU profiling sections, ensuring you capture results in `prof_cpu` and `prof_gpu`.\n",
    "3.  Add code to print the detailed operator tables from the profiler results.\n",
    "4.  Identify and list the Top 5 operators for both CPU and GPU.\n",
    "5.  Interpret what these top operators likely represent.\n",
    "6.  Compare the overall wall clock time of `gpt2` (this exercise) with `gpt2-medium` (from the demo or your own run if you did it).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.profiler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Define Prompt\n",
    "\n",
    "**TODO:**\n",
    "- Set `model_name` variable to `\"gpt2\"`.\n",
    "- The rest of the cell (loading tokenizer, model, setting pad token, and preparing inputs) can remain largely the same as the demo code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the model name for GPT-2 (the smallest variant)\n",
    "model_name = \"TODO: YOUR MODEL NAME HERE\"  # Replace with \"gpt2\"\n",
    "\n",
    "print(f\"Loading model and tokenizer for: {model_name}...\")\n",
    "try:\n",
    "    # TODO: Load the tokenizer and model\n",
    "    tokenizer = _\n",
    "    model = _\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model {model_name}. Please ensure it's correct. Error: {e}\")\n",
    "    # If running in a restricted environment, you might need to use a pre-downloaded model or a different one.\n",
    "    # For now, we'll stop if loading fails.\n",
    "    raise\n",
    "\n",
    "# Add a padding token if tokenizer doesn't have one\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Setting pad_token to eos_token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Prepare a sample prompt\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "num_new_tokens_to_generate = 50\n",
    "\n",
    "print(f\"Input prompt: '{prompt}'\")\n",
    "print(f\"Generating {num_new_tokens_to_generate} new tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Profiling\n",
    "\n",
    "**TODO:**\n",
    "- Adapt the CPU profiling code from the demo.\n",
    "- Ensure the inference is run within the `torch.profiler.profile` context.\n",
    "- After the profiling block, add the code to print the top 5 CPU operators using `prof_cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Profiling on CPU ---\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "model.to(cpu_device)\n",
    "inputs_cpu = {k: v.to(cpu_device) for k, v in inputs.items()}\n",
    "\n",
    "def run_cpu_inference(model_to_run, input_data, max_tokens, pad_id):\n",
    "    # TODO: inference code to run the model\n",
    "    return None\n",
    "\n",
    "print(\"Running inference on CPU and capturing profile...\")\n",
    "start_time_cpu_wall = time.time()\n",
    "\n",
    "# TODO: Setup the torch.profiler.profile context manager for CPU\n",
    "# Store the profiler object in 'prof_cpu'\n",
    "with None as prof_cpu: # Replace 'None' with the profiler setup\n",
    "    # TODO: Add the record_function context manager (optional, but good practice)\n",
    "    with None: # Replace 'None' with record_function if you use it\n",
    "        run_cpu_inference(model, inputs_cpu, num_new_tokens_to_generate, tokenizer.pad_token_id)\n",
    "\n",
    "end_time_cpu_wall = time.time()\n",
    "cpu_wall_time = end_time_cpu_wall - start_time_cpu_wall\n",
    "print(f\"CPU Wall clock time: {cpu_wall_time:.4f} seconds\")\n",
    "\n",
    "# TODO: Add code here to print the key_averages table for prof_cpu,\n",
    "# showing top 5 operators sorted by self_cpu_time_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Profiling\n",
    "\n",
    "**TODO:**\n",
    "- Adapt the GPU profiling code from the demo.\n",
    "- Include the check for CUDA availability (`torch.cuda.is_available()`).\n",
    "- Remember the warm-up run and `torch.cuda.synchronize()` for accurate timing.\n",
    "- Ensure the inference is run within the `torch.profiler.profile` context, capturing both CPU and CUDA activities.\n",
    "- After the profiling block, add the code to print the top 5 GPU operators using `prof_gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_wall_time = -1.0 # Initialize in case GPU is not available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n--- Profiling on GPU ---\")\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "    model.to(gpu_device)\n",
    "    inputs_gpu = {k: v.to(gpu_device) for k, v in inputs.items()}\n",
    "    print(f\"CUDA device found: {torch.cuda.get_device_name(gpu_device)}\")\n",
    "\n",
    "    def run_gpu_inference(model_to_run, input_data, max_tokens, pad_id):\n",
    "        with torch.no_grad():\n",
    "            # TODO: add inference code here \n",
    "            # Remember to use torch.cuda.synchronize() before and after generation\n",
    "            return None\n",
    "\n",
    "    print(\"Performing GPU warm-up run...\")\n",
    "    # TODO: Call run_gpu_inference for warm-up\n",
    "    print(\"Warm-up complete.\")\n",
    "\n",
    "    print(\"Running inference on GPU and capturing profile...\")\n",
    "    start_time_gpu_wall = time.time()\n",
    "\n",
    "    # TODO: Setup the torch.profiler.profile context manager for GPU (CPU & CUDA activities)\n",
    "    # Store the profiler object in 'prof_gpu'\n",
    "    with None as prof_gpu: # Replace 'None' with the profiler setup\n",
    "        # TODO: Add the record_function context manager (optional)\n",
    "        with None: # Replace 'None' with record_function if you use it\n",
    "            run_gpu_inference(model, inputs_gpu, num_new_tokens_to_generate, tokenizer.pad_token_id)\n",
    "\n",
    "    end_time_gpu_wall = time.time()\n",
    "    gpu_wall_time = end_time_gpu_wall - start_time_gpu_wall\n",
    "    print(f\"GPU Wall clock time: {gpu_wall_time:.4f} seconds\")\n",
    "\n",
    "    # TODO: Add code here to print the key_averages table for prof_gpu,\n",
    "    # showing top 5 operators sorted by self_cuda_time_total\n",
    "\n",
    "else:\n",
    "    print(\"\\nCUDA not available on this system. Skipping GPU profiling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deliverables & Analysis\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "Based on the profiler tables you printed above, answer the following questions. Write your answers in this markdown cell.\n",
    "\n",
    "**A. Top 5 CPU Operators:**\n",
    "   1.  Operator Name: `TODO` | Self CPU Time %: `TODO` | Likely Represents: `TODO`\n",
    "   2.  Operator Name: `TODO` | Self CPU Time %: `TODO` | Likely Represents: `TODO`\n",
    "   3.  Operator Name: `TODO` | Self CPU Time %: `TODO` | Likely Represents: `TODO`\n",
    "   4.  Operator Name: `TODO` | Self CPU Time %: `TODO` | Likely Represents: `TODO`\n",
    "   5.  Operator Name: `TODO` | Self CPU Time %: `TODO` | Likely Represents: `TODO`\n",
    "\n",
    "**B. Top 5 GPU Operators (if CUDA was available):**\n",
    "   1.  Operator Name: `TODO` | Self CUDA Time %: `TODO` | Likely Represents: `TODO`\n",
    "   2.  Operator Name: `TODO` | Self CUDA Time %: `TODO` | Likely Represents: `TODO`\n",
    "   3.  Operator Name: `TODO` | Self CUDA Time %: `TODO` | Likely Represents: `TODO`\n",
    "   4.  Operator Name: `TODO` | Self CUDA Time %: `TODO` | Likely Represents: `TODO`\n",
    "   5.  Operator Name: `TODO` | Self CUDA Time %: `TODO` | Likely Represents: `TODO`\n",
    "\n",
    "**C. Wall Clock Time Comparison (gpt2 vs gpt2-medium from demo):**\n",
    "   - CPU Wall Time (gpt2 from this exercise): `TODO: Fill in your measured time` seconds\n",
    "   - CPU Wall Time (gpt2-medium from demo, approx.): `TODO: Recall or estimate from demo, e.g., ~7-10 seconds` seconds\n",
    "   - GPU Wall Time (gpt2 from this exercise, if available): `TODO: Fill in your measured time` seconds\n",
    "   - GPU Wall Time (gpt2-medium from demo, approx., if available): `TODO: Recall or estimate from demo, e.g., ~0.5-1.5 seconds` seconds\n",
    "   \n",
    "   - Did `gpt2` (smaller model) run faster than `gpt2-medium` (larger model) on both CPU and GPU as expected? `TODO: Yes/No, and any brief observation`\n",
    "\n",
    "**D. Brief Interpretation of Top Operators:**\n",
    "   - What kind of operations generally dominate the top spots on both CPU and GPU for this Transformer model? `TODO: Your interpretation, e.g., matrix multiplications, attention components, activation functions, etc.`\n",
    "   - Were there any surprising operators in the top 5 for either CPU or GPU? `TODO: Yes/No, and if yes, which one and why?`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
