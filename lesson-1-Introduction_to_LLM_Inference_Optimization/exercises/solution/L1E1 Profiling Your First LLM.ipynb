{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Profiling Your First LLM (GPT-2)\n",
    "\n",
    "Welcome! In this lesson, we'll walk through the fundamental process of profiling a large language model (LLM). Profiling is the first and most critical step in understanding and optimizing model performance.\n",
    "\n",
    "Our goals for this exercise are:\n",
    "1.  Set up the environment with the necessary libraries.\n",
    "2.  Load a pre-trained LLM (`gpt2`) from the Hugging Face Hub.\n",
    "3.  Use the `torch.profiler` to measure the model's performance on a **CPU**.\n",
    "4.  Use the `torch.profiler` to measure the model's performance on a **GPU**.\n",
    "5.  Analyze and compare the results to identify computational bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Importing Necessary Libraries\n",
    "\n",
    "With the packages installed, let's import the specific modules we'll need for this exercise. We'll be using `torch` for core tensor operations, `transformers` for our model, and `torch.profiler` for the performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.profiler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loading the Language Model and Tokenizer\n",
    "\n",
    "Now, let's load our model. We'll use **`gpt2`**, a small yet powerful model that is perfect for demonstration purposes because it loads quickly.\n",
    "\n",
    "-   **Model (`AutoModelForCausalLM`):** This is the neural network itselfâ€”the \"brain\" that will generate text.\n",
    "-   **Tokenizer (`AutoTokenizer`):** This is a crucial utility that translates human-readable text (strings) into numerical IDs that the model can understand, and vice-versa.\n",
    "\n",
    "We also set a `pad_token`. This is important for ensuring that all inputs in a batch have the same length, which is a requirement for many deep learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer for: gpt2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e730bb171a84f49a7accb7d5cc583c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e8a69defa2457daffd1d1327afa238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce0f2f817f94ca1b53953ce009ebb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2bcbbb8ccf4668923ac0e89cabbb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e051fde5b147f892e3cc0d833b72c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba4f6c315594a9eaf5a14fa9c879a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edef75f6eda4124b34ac61f93e51875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading model and tokenizer for: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token if the tokenizer doesn't have one.\n",
    "# This is good practice for batching inputs.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preparing the Input Prompt\n",
    "\n",
    "Next, we'll prepare a sample prompt. The tokenizer converts our prompt string into a tensor of `input_ids` that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: 'The future of artificial intelligence is'\n",
      "Task: Generate 50 new tokens.\n"
     ]
    }
   ],
   "source": [
    "# Prepare a sample prompt\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "num_new_tokens_to_generate = 50\n",
    "\n",
    "print(f\"Input prompt: '{prompt}'\")\n",
    "print(f\"Task: Generate {num_new_tokens_to_generate} new tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Profiling Model Inference on CPU\n",
    "\n",
    "It's time for our first performance test. We will profile the model's text generation process on the CPU.\n",
    "\n",
    "Here's the plan:\n",
    "1.  Ensure the model and inputs are on the CPU.\n",
    "2.  Define a simple inference function `run_cpu_inference`.\n",
    "3.  Use `torch.no_grad()` to disable gradient calculations, which are unnecessary for inference and would add overhead.\n",
    "4.  Wrap the function call in the `torch.profiler.profile` context manager to capture performance data.\n",
    "5.  Print the results, sorted by the operations that took the most time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Profiling on CPU ---\n",
      "Running inference on CPU and capturing profile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-06-16 00:02:30 17122:17122 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-06-16 00:02:32 17122:17122 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-06-16 00:02:32 17122:17122 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Wall clock time: 5.8407 seconds\n",
      "\n",
      "CPU Profiler Analysis (Top 5 Operators by Self CPU Time):\n",
      "---------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  aten::addmm        42.91%     726.872ms        43.87%     743.046ms     309.603us          2400  \n",
      "                          model_inference_cpu        23.05%     390.360ms       100.00%        1.694s        1.694s             1  \n",
      "                                     aten::mm        19.79%     335.248ms        19.79%     335.248ms       6.705ms            50  \n",
      "                                    aten::cat         2.65%      44.967ms         2.85%      48.254ms      37.817us          1276  \n",
      "    aten::_scaled_dot_product_flash_attention         1.51%      25.600ms         1.85%      31.401ms      52.335us           600  \n",
      "---------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.694s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Profiling on CPU ---\")\n",
    "model.to(\"cpu\")\n",
    "inputs_cpu = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "\n",
    "def run_cpu_inference(input_data, max_tokens):\n",
    "    # Use torch.no_grad() to disable gradients for efficiency\n",
    "    with torch.no_grad():\n",
    "        model.generate(\n",
    "            input_data[\"input_ids\"],\n",
    "            attention_mask=input_data.get(\"attention_mask\"),\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "print(\"Running inference on CPU and capturing profile...\")\n",
    "start_time_cpu = time.time()\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "    record_shapes=False, # Set to True if shape info is needed, but adds overhead\n",
    "    profile_memory=False # Set to True for memory profiling, but adds significant overhead\n",
    ") as prof_cpu:\n",
    "    with torch.profiler.record_function(\"model_inference_cpu\"):\n",
    "        run_cpu_inference(inputs_cpu, num_new_tokens_to_generate)\n",
    "\n",
    "end_time_cpu = time.time()\n",
    "print(f\"CPU Wall clock time: {end_time_cpu - start_time_cpu:.4f} seconds\\n\")\n",
    "\n",
    "print(\"CPU Profiler Analysis (Top 5 Operators by Self CPU Time):\")\n",
    "print(prof_cpu.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the CPU Profile\n",
    "\n",
    "After running the cell above, look at the output table.\n",
    "-   **Wall Clock Time:** This is the total real-world time it took to run the generation.\n",
    "-   **`aten::addmm`:** You'll likely see this operation at or near the top. It stands for `ADD Matrix-Matrix` multiplication and is the core of the linear layers in the Transformer model. This is where most of the heavy lifting happens.\n",
    "-   **`aten::mm`**: General matrix multiplication.\n",
    "-   **`aten::_scaled_dot_product_flash_attention`**: This indicates that an optimized attention mechanism is being used, even on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Profiling Model Inference on GPU\n",
    "\n",
    "Now, let's see how much a GPU can speed things up. The process is similar, but with a few key differences for GPU profiling:\n",
    "\n",
    "1.  **Check for GPU:** We first check if `torch.cuda.is_available()`.\n",
    "2.  **Move to GPU:** We move the model and input tensors to the `\"cuda\"` device.\n",
    "3.  **Warm-up Run:** The first time you run an operation on a GPU, it has to perform setup tasks (like loading CUDA kernels) that take extra time. We'll do a \"warm-up\" run first so these one-time costs don't pollute our actual measurement.\n",
    "4.  **Synchronize:** GPU operations are asynchronous. The CPU gives a command to the GPU and immediately moves on. To get accurate timing, we must use `torch.cuda.synchronize()` to make the CPU wait for the GPU to finish its work.\n",
    "5.  **Profile:** We include `ProfilerActivity.CUDA` in the profiler's activities list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Profiling on GPU ---\n",
      "Performing GPU warm-up run...\n",
      "Running inference on GPU and capturing profile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-06-16 00:02:40 17122:17122 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-06-16 00:02:41 17122:17122 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-06-16 00:02:41 17122:17122 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Wall clock time: 4.7810 seconds\n",
      "\n",
      "GPU Profiler Analysis (Top 5 Operators by Self CUDA Time):\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm         9.55%      62.230ms        12.36%      80.494ms      33.539us      71.839ms        56.47%      73.135ms      30.473us          2400  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      47.354ms        37.23%      47.354ms      26.845us          1764  \n",
      "                                               aten::mm         0.19%       1.218ms         0.26%       1.669ms      33.380us      31.062ms        24.42%      31.066ms     621.320us            50  \n",
      "void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      29.793ms        23.42%      29.793ms     608.020us            49  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      22.692ms        17.84%      22.692ms      38.592us           588  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 651.293ms\n",
      "Self CUDA time total: 127.210ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- GPU Profiling only if CUDA is available ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"--- Profiling on GPU ---\")\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "    inputs_gpu = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    def run_gpu_inference(input_data, max_tokens):\n",
    "         with torch.no_grad():\n",
    "             # Synchronize before starting to get an accurate start time\n",
    "             torch.cuda.synchronize() \n",
    "             outputs = model.generate(\n",
    "                 input_data[\"input_ids\"],\n",
    "                 attention_mask=input_data.get(\"attention_mask\"),\n",
    "                 max_new_tokens=max_tokens,\n",
    "                 pad_token_id=tokenizer.pad_token_id\n",
    "             )\n",
    "             # Synchronize again to wait for the generation to finish\n",
    "             torch.cuda.synchronize()\n",
    "\n",
    "    # Perform a warm-up run to load CUDA kernels, etc.\n",
    "    print(\"Performing GPU warm-up run...\")\n",
    "    run_gpu_inference(inputs_gpu, num_new_tokens_to_generate)\n",
    "\n",
    "    print(\"Running inference on GPU and capturing profile...\")\n",
    "    start_time_gpu = time.time()\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        record_shapes=False,\n",
    "        profile_memory=False\n",
    "    ) as prof_gpu:\n",
    "        with torch.profiler.record_function(\"model_inference_gpu\"):\n",
    "            run_gpu_inference(inputs_gpu, num_new_tokens_to_generate)\n",
    "\n",
    "    end_time_gpu = time.time()\n",
    "    print(f\"GPU Wall clock time: {end_time_gpu - start_time_gpu:.4f} seconds\\n\")\n",
    "\n",
    "    print(\"GPU Profiler Analysis (Top 5 Operators by Self CUDA Time):\")\n",
    "    print(prof_gpu.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
    "\n",
    "else:\n",
    "    print(\"\\nCUDA not available on this system, skipping GPU profiling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the GPU Profile\n",
    "\n",
    "Now, examine the GPU profiler output.\n",
    "-   **Wall Clock Time:** Compare this to the CPU time. You should see a significant speedup!\n",
    "-   **Top Operators:** The operator names will be different. Instead of `aten::` operations, you will see low-level CUDA kernels. Names like `gemv` or `gemm` (General Matrix-Vector/Matrix-Matrix multiplication) are common. These are the highly-optimized functions that run on the GPU hardware.\n",
    "-   **`Self CUDA %`:** This column is now the most important one. It tells you the percentage of total GPU time that was spent inside a specific kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Conclusion and Key Takeaways\n",
    "\n",
    "Congratulations! You've successfully profiled an LLM on both CPU and GPU. Let's summarize what we've learned.\n",
    "\n",
    "**1. Performance Comparison:**\n",
    "- We observed a clear performance gain when moving from CPU to GPU. The total wall clock time for generation was significantly lower on the GPU, demonstrating the power of parallel processing for deep learning workloads.\n",
    "\n",
    "**2. Identifying Bottlenecks:**\n",
    "- On both CPU and GPU, the profiling results pointed to the same root cause of computational load: **matrix multiplications** (`addmm`, `mm` on CPU; `gemm`/`gemv` kernels on GPU). This is the fundamental building block of Transformer models, and it's where most of the time is spent.\n",
    "\n",
    "**3. The Power of Profiling:**\n",
    "- This exercise demonstrates that before you can optimize anything, you must first measure it. The PyTorch Profiler is an indispensable tool that gives you a detailed view of where your program is spending its time.\n",
    "\n",
    "**4. Next Steps:**\n",
    "- Now that we can identify bottlenecks, we can start exploring ways to fix them. Future lessons will cover techniques like quantization, using optimized kernels like Flash Attention, and other strategies to speed up these expensive matrix multiplication operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
