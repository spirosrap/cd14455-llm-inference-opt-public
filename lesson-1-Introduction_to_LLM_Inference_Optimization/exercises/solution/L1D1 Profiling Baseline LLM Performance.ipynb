{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 Demo: Profiling Baseline LLM Performance\n",
    "\n",
    "### Goal\n",
    "This notebook demonstrates how to establish a baseline performance profile for a Large Language Model (LLM) using `torch.profiler`. We will measure key metrics like latency, throughput, and memory usage on both CPU and GPU.\n",
    "\n",
    "### Learning Objectives\n",
    "*   Set up `torch.profiler` for both CPU and GPU activities.\n",
    "*   Measure and understand end-to-end latency.\n",
    "*   Calculate and interpret single-stream throughput (tokens/second).\n",
    "*   Measure the model's memory footprint.\n",
    "*   Appreciate the performance difference between CPU and GPU for LLM inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "First, we'll install the necessary libraries and import our dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies\n",
    "Now we import all the necessary modules. We'll use `torch` for the core operations, `transformers` to get our model, `time` for basic timing, and `gc` for garbage collection to get cleaner memory readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.profiler\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Model and Data Preparation\n",
    "\n",
    "Next, we'll load our pre-trained model and tokenizer, and prepare the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load Model and Tokenizer\n",
    "We'll use `gpt2-medium`, a moderately sized model that's good for demonstration purposes. It's large enough to show a significant CPU vs. GPU difference but small enough to run on most modern GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c60c535ed74acf9e0111e96173c54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0065839913804979826a3c4d10f4c3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61782fcecd914471b989fcb955200612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2b3e082e374e0090b1a80fc91a059f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b83851479da4cda83b57b1533889cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5b9648ae2245b2b7441c52ca57e0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770129b595b7409da5eb00b3acac6b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer for: gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(f\"Loaded model and tokenizer for: {model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model {model_name}. Make sure you have internet connection \"\n",
    "          f\"and the model name is correct. Error: {e}\")\n",
    "    # In a real script, you might exit or raise the error\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Prepare Inputs\n",
    "We define our prompt and set the number of new tokens we want to generate. It's also important to handle the `pad_token` for models that don't have one set by default, which is common for decoder-only models like GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer does not have a pad token, setting it to eos_token.\n",
      "Input prompt: 'Alan Turing was a pioneering computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist.' (Length: 23 tokens)\n",
      "Will generate 50 new tokens.\n"
     ]
    }
   ],
   "source": [
    "# Handle padding token for open-ended generation\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer does not have a pad token, setting it to eos_token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Prepare input text\n",
    "prompt = \"Alan Turing was a pioneering computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "num_tokens_in_prompt = inputs['input_ids'].shape[1]\n",
    "num_new_tokens_to_generate = 50\n",
    "total_tokens_for_calc = num_new_tokens_to_generate # For tokens/sec, we only care about *newly* generated tokens\n",
    "\n",
    "print(f\"Input prompt: '{prompt}' (Length: {num_tokens_in_prompt} tokens)\")\n",
    "print(f\"Will generate {num_new_tokens_to_generate} new tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Utility Functions\n",
    "\n",
    "To keep our profiling code clean and reusable, we'll create a couple of helper functions: one to run inference and another to measure memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_to_run, input_data, max_tokens, pad_id):\n",
    "    \"\"\"Runs model.generate within a torch.no_grad() context to disable gradients.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model_to_run.generate(\n",
    "            input_ids=input_data[\"input_ids\"],\n",
    "            attention_mask=input_data.get(\"attention_mask\"),\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=pad_id,\n",
    "            # We set eos_token_id to an unused value to ensure the model generates\n",
    "            # the full number of requested tokens for consistent benchmarking.\n",
    "            eos_token_id=-1, \n",
    "        )\n",
    "    return outputs\n",
    "\n",
    "def get_memory_usage_gb(device_type=\"cpu\", device_index=0):\n",
    "    \"\"\"Gets the memory usage in Gigabytes for a specified device.\"\"\"\n",
    "    gc.collect() # Force garbage collection for more accurate readings\n",
    "    \n",
    "    if device_type == \"cuda\" and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache() # Clear unused memory from PyTorch's cache\n",
    "        mem_allocated_bytes = torch.cuda.memory_allocated(device_index)\n",
    "        mem_reserved_bytes = torch.cuda.memory_reserved(device_index)\n",
    "        return mem_allocated_bytes / (1024**3), mem_reserved_bytes / (1024**3)\n",
    "    elif device_type == \"cpu\":\n",
    "        # This is an approximation of CPU memory for the model's parameters only.\n",
    "        # For a full process memory profile, external tools are better.\n",
    "        model_mem_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        return model_mem_bytes / (1024**3), -1 # -1 indicates no 'reserved' memory concept here\n",
    "    return 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Profile on CPU\n",
    "\n",
    "Let's establish our baseline by running the model on the CPU. We'll measure the memory footprint, latency, and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Profiling on CPU ---\n",
      "CPU Model Parameter Memory (Approx.): 1.322 GB\n",
      "Running inference on CPU and capturing profile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-06-15 23:51:54 7687:7687 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "`eos_token_id` should consist of positive integers, but is tensor([-1]). Your generation will not stop until the maximum length is reached. Depending on other flags, it may even crash.\n",
      "STAGE:2025-06-15 23:52:01 7687:7687 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-06-15 23:52:01 7687:7687 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Profiling on CPU ---\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "# Ensure model and inputs are on the CPU\n",
    "model.to(cpu_device)\n",
    "inputs_cpu = {k: v.to(cpu_device) for k, v in inputs.items()}\n",
    "\n",
    "# Measure memory\n",
    "cpu_model_load_mem_allocated_gb, _ = get_memory_usage_gb(device_type=\"cpu\")\n",
    "print(f\"CPU Model Parameter Memory (Approx.): {cpu_model_load_mem_allocated_gb:.3f} GB\")\n",
    "\n",
    "# Profile the inference call\n",
    "print(\"Running inference on CPU and capturing profile...\")\n",
    "start_time_cpu_wall = time.time()\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "    record_shapes=True\n",
    ") as prof_cpu:\n",
    "    with torch.profiler.record_function(\"model_inference_cpu\"):\n",
    "        generated_outputs_cpu = run_inference(model, inputs_cpu, num_new_tokens_to_generate, tokenizer.pad_token_id)\n",
    "\n",
    "end_time_cpu_wall = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Analyze CPU Results\n",
    "Now we calculate the latency and throughput from our CPU run. Wall-clock time (`time.time()`) is sufficient for measuring synchronous CPU operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU End-to-End Latency: 18.6154 seconds\n",
      "CPU Single-Stream Throughput: 2.69 tokens/second\n",
      "CPU Profiling data captured in 'prof_cpu' object. You can analyze it further with prof_cpu.key_averages().\n"
     ]
    }
   ],
   "source": [
    "# Calculate Latency\n",
    "cpu_e2e_latency_s = end_time_cpu_wall - start_time_cpu_wall\n",
    "print(f\"CPU End-to-End Latency: {cpu_e2e_latency_s:.4f} seconds\")\n",
    "\n",
    "# Calculate Throughput\n",
    "cpu_tokens_per_second = 0\n",
    "if cpu_e2e_latency_s > 0:\n",
    "    cpu_tokens_per_second = total_tokens_for_calc / cpu_e2e_latency_s\n",
    "    print(f\"CPU Single-Stream Throughput: {cpu_tokens_per_second:.2f} tokens/second\")\n",
    "\n",
    "print(\"CPU Profiling data captured in 'prof_cpu' object. You can analyze it further with prof_cpu.key_averages().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Profile on GPU (if available)\n",
    "\n",
    "Now, let's do the same on a GPU and see the difference. This section will only run if a CUDA-enabled GPU is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Profiling on GPU ---\n",
      "CUDA device found: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU metrics to default values\n",
    "gpu_e2e_latency_s_event = -1.0\n",
    "gpu_model_load_mem_allocated_gb = -1.0\n",
    "gpu_model_load_mem_reserved_gb = -1.0\n",
    "gpu_tokens_per_second = 0\n",
    "prof_gpu = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"--- Profiling on GPU ---\")\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA device found: {torch.cuda.get_device_name(gpu_device)}\")\n",
    "else:\n",
    "    print(\"\\nCUDA not available. Skipping GPU profiling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. GPU Memory Usage\n",
    "\n",
    "First, we move the model to the GPU and measure its memory footprint. On GPUs, we look at two metrics:\n",
    "- **Memory Allocated:** The memory actively used by tensors.\n",
    "- **Memory Reserved:** The total memory pool reserved by the PyTorch caching allocator. This is often larger than the allocated memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model and inputs to GPU...\n",
      "GPU Model Load Memory (Allocated): 1.345 GB\n",
      "GPU Model Load Memory (Reserved by PyTorch): 1.348 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Moving model and inputs to GPU...\")\n",
    "    model.to(gpu_device)\n",
    "    inputs_gpu = {k: v.to(gpu_device) for k, v in inputs.items()}\n",
    "\n",
    "    gpu_model_load_mem_allocated_gb, gpu_model_load_mem_reserved_gb = get_memory_usage_gb(device_type=\"cuda\")\n",
    "    print(f\"GPU Model Load Memory (Allocated): {gpu_model_load_mem_allocated_gb:.3f} GB\")\n",
    "    print(f\"GPU Model Load Memory (Reserved by PyTorch): {gpu_model_load_mem_reserved_gb:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. GPU Warm-up and Accurate Timing\n",
    "\n",
    "**Warm-up:** The first time you run an operation on a GPU, it needs to compile underlying CUDA kernels. This adds significant overhead. We perform a \"warm-up\" run with a small number of tokens to get this compilation out of the way, ensuring our actual benchmark is more accurate.\n",
    "\n",
    "**Accurate Timing:** GPU operations are asynchronous. The CPU can issue a command and move on before the GPU has finished. Using `time.time()` would be inaccurate. Instead, we use `torch.cuda.Event` to record timestamps directly on the GPU's execution stream, and `torch.cuda.synchronize()` to ensure all operations are complete before we stop the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GPU warm-up run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`eos_token_id` should consist of positive integers, but is tensor([-1], device='cuda:0'). Your generation will not stop until the maximum length is reached. Depending on other flags, it may even crash.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up complete.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Performing GPU warm-up run...\")\n",
    "    try:\n",
    "        # A small, quick run to compile CUDA kernels\n",
    "        _ = run_inference(model, inputs_gpu, 5, tokenizer.pad_token_id)\n",
    "        torch.cuda.synchronize() # Wait for the warm-up to finish\n",
    "        print(\"Warm-up complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPU warm-up: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Run GPU Profiler\n",
    "Now we perform the actual profiled run on the GPU, measuring both CPU and CUDA activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-06-15 23:52:26 7687:7687 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "`eos_token_id` should consist of positive integers, but is tensor([-1], device='cuda:0'). Your generation will not stop until the maximum length is reached. Depending on other flags, it may even crash.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on GPU and capturing profile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-06-15 23:52:28 7687:7687 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-06-15 23:52:28 7687:7687 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Running inference on GPU and capturing profile...\")\n",
    "    \n",
    "    # --- Timing with CUDA Events for accurate latency ---\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU, \n",
    "            torch.profiler.ProfilerActivity.CUDA\n",
    "        ],\n",
    "        record_shapes=True\n",
    "    ) as prof_gpu:\n",
    "        with torch.profiler.record_function(\"model_inference_gpu\"):\n",
    "            start_event.record()\n",
    "            generated_outputs_gpu = run_inference(model, inputs_gpu, num_new_tokens_to_generate, tokenizer.pad_token_id)\n",
    "            end_event.record()\n",
    "            torch.cuda.synchronize() # Wait for all GPU work to complete\n",
    "            \n",
    "    # Calculate latency from events\n",
    "    gpu_e2e_latency_s_event = start_event.elapsed_time(end_event) / 1000.0 # Convert ms to s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Analyze GPU Results\n",
    "Finally, we calculate the throughput based on our accurate latency measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU End-to-End Latency (CUDA Event): 1.3128 seconds\n",
      "GPU Single-Stream Throughput: 38.09 tokens/second\n",
      "GPU Profiling data captured in 'prof_gpu' object. You can analyze it further with prof_gpu.key_averages().\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and gpu_e2e_latency_s_event > 0:\n",
    "    print(f\"GPU End-to-End Latency (CUDA Event): {gpu_e2e_latency_s_event:.4f} seconds\")\n",
    "    # Throughput\n",
    "    gpu_tokens_per_second = total_tokens_for_calc / gpu_e2e_latency_s_event\n",
    "    print(f\"GPU Single-Stream Throughput: {gpu_tokens_per_second:.2f} tokens/second\")\n",
    "    print(\"GPU Profiling data captured in 'prof_gpu' object. You can analyze it further with prof_gpu.key_averages().\")\n",
    "else:\n",
    "    print(\"GPU metrics not calculated due to error or CUDA unavailability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Comparison and Analysis\n",
    "\n",
    "Let's put the results side-by-side to see the performance impact of using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary of Baseline Metrics ---\n",
      "\n",
      "--- CPU Metrics ---\n",
      "Model Memory (Approx. Parameters): 1.322 GB\n",
      "E2E Latency:                     18.6154 s\n",
      "Tokens/sec (single-stream):      2.69 tokens/s\n",
      "\n",
      "--- GPU Metrics ---\n",
      "Model Memory (Allocated):        1.345 GB\n",
      "Model Memory (Reserved):         1.348 GB\n",
      "E2E Latency (CUDA Event):        1.3128 s\n",
      "Tokens/sec (single-stream):      38.09 tokens/s\n",
      "\n",
      "--- Performance Speedup ---\n",
      "Latency Speedup (CPU / GPU):     14.18x\n",
      "Throughput Speedup (GPU / CPU):  14.18x\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Summary of Baseline Metrics ---\")\n",
    "\n",
    "print(f\"\\n--- CPU Metrics ---\")\n",
    "print(f\"Model Memory (Approx. Parameters): {cpu_model_load_mem_allocated_gb:.3f} GB\")\n",
    "print(f\"E2E Latency:                     {cpu_e2e_latency_s:.4f} s\")\n",
    "print(f\"Tokens/sec (single-stream):      {cpu_tokens_per_second:.2f} tokens/s\")\n",
    "\n",
    "if torch.cuda.is_available() and gpu_e2e_latency_s_event > 0:\n",
    "    print(f\"\\n--- GPU Metrics ---\")\n",
    "    print(f\"Model Memory (Allocated):        {gpu_model_load_mem_allocated_gb:.3f} GB\")\n",
    "    print(f\"Model Memory (Reserved):         {gpu_model_load_mem_reserved_gb:.3f} GB\")\n",
    "    print(f\"E2E Latency (CUDA Event):        {gpu_e2e_latency_s_event:.4f} s\")\n",
    "    print(f\"Tokens/sec (single-stream):      {gpu_tokens_per_second:.2f} tokens/s\")\n",
    "    \n",
    "    print(f\"\\n--- Performance Speedup ---\")\n",
    "    speedup_latency = cpu_e2e_latency_s / gpu_e2e_latency_s_event\n",
    "    speedup_throughput = gpu_tokens_per_second / cpu_tokens_per_second\n",
    "    print(f\"Latency Speedup (CPU / GPU):     {speedup_latency:.2f}x\")\n",
    "    print(f\"Throughput Speedup (GPU / CPU):  {speedup_throughput:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Conclusion & Key Takeaways\n",
    "\n",
    "In this demonstration, we successfully established a performance baseline for the `gpt2-medium` model.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **Profiling is Essential:** We now have concrete numbers for latency, throughput, and memory. This is the starting point for any optimization effort.\n",
    "- **GPUs Provide Massive Speedups:** As expected, the GPU outperformed the CPU by a significant margin (~14x in this case), highlighting its necessity for efficient LLM inference.\n",
    "- **Accurate Measurement Matters:** We saw the importance of using the right tools (`torch.cuda.Event`) and techniques (warm-up runs) to get reliable performance data, especially on asynchronous hardware like GPUs.\n",
    "- **Memory Footprint is Similar:** The model's parameter memory is roughly the same on CPU and GPU, but GPU memory management is more complex (allocated vs. reserved).\n",
    "\n",
    "With this baseline, we are now ready to explore various optimization techniques in future lessons to see how we can improve these metrics even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
