{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: How Input Sequence Length Impacts Inference Speed\n",
    "\n",
    "### Goal\n",
    "To demonstrate that longer input sequences (prompts) increase the total time it takes for a model to generate a response, even when the number of *new* tokens being generated is the same.\n",
    "\n",
    "We will compare the end-to-end latency and tokens per second for a short prompt versus a long prompt on both CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Model and Tokenizer\n",
    "\n",
    "We will use `gpt2-medium`, a moderately sized model, for this demonstration. We also need to ensure the tokenizer has a `pad_token` defined. If it doesn't, we'll set it to the `eos_token` (end-of-sentence token), which is a common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model and tokenizer for: gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure a pad token is set for the tokenizer and model configuration\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(f\"Successfully loaded model and tokenizer for: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparing the Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Prompts of Different Lengths\n",
    "\n",
    "Here, we create two prompts: a very short one and a much longer one. We will instruct the model to generate the exact same number of new tokens for both, allowing us to isolate the effect of the initial prompt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_new_tokens_to_generate = 50 # The number of new tokens we want the model to generate\n",
    "\n",
    "short_prompt = \"The three primary colors are\"\n",
    "\n",
    "long_prompt = (\"Modern deep learning models, especially transformers like GPT and BERT, \"\n",
    "               \"rely heavily on matrix multiplications and attention mechanisms. \"\n",
    "               \"While GPUs offer massive parallel processing capabilities via thousands of cores, \"\n",
    "               \"the speed is often limited not by computation (FLOPs) but by the rate at which \"\n",
    "               \"data (weights, activations, intermediate states) can be fetched from High Bandwidth Memory (HBM). \"\n",
    "               \"This memory bandwidth bottleneck becomes particularly pronounced during the inference phase of \"\n",
    "               \"large language models where parameter counts reach billions, requiring constant data movement \"\n",
    "               \"which can leave the powerful compute units waiting. \"\n",
    "               \"The core of the issue is that for every single token generated, the model must read its enormous weights \"\n",
    "               \"and the state of all previous tokens (the key-value cache). As the context grows, this cache gets larger, \"\n",
    "               \"demanding more memory bandwidth and slowing down the time to generate the next token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenize Prompts and Verify Lengths\n",
    "\n",
    "Now, we'll use the tokenizer to convert our text prompts into numerical IDs that the model can understand. We'll print the number of tokens in each prompt to confirm their difference in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short prompt length: 5 tokens\n",
      "Long prompt length:  172 tokens\n",
      "\n",
      "We will generate 50 new tokens for each prompt.\n"
     ]
    }
   ],
   "source": [
    "inputs_short = tokenizer(short_prompt, return_tensors=\"pt\")\n",
    "inputs_long = tokenizer(long_prompt, return_tensors=\"pt\")\n",
    "\n",
    "short_prompt_len = inputs_short['input_ids'].shape[1]\n",
    "long_prompt_len = inputs_long['input_ids'].shape[1]\n",
    "\n",
    "print(f\"Short prompt length: {short_prompt_len} tokens\")\n",
    "print(f\"Long prompt length:  {long_prompt_len} tokens\")\n",
    "print(f\"\\nWe will generate {num_new_tokens_to_generate} new tokens for each prompt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Benchmarking Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Benchmarking on CPU\n",
    "\n",
    "First, we'll measure the performance on a standard CPU. We will:\n",
    "1.  Move the model and both prompts to the CPU.\n",
    "2.  Time how long it takes to generate 50 tokens for the short prompt.\n",
    "3.  Time how long it takes to generate 50 tokens for the long prompt.\n",
    "4.  Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model and data to CPU...\n",
      "Setup complete.\n",
      "\n",
      "Running inference for a SHORT prompt on CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU - Short Prompt: Latency=4.0583s, TPS=12.32\n",
      "\n",
      "Running inference for a LONG prompt on CPU...\n",
      "CPU - Long Prompt:  Latency=4.7266s, TPS=10.58\n",
      "\n",
      "CPU Latency Increase (Long/Short): 1.16x\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Setup CPU Environment ---\n",
    "print(\"Moving model and data to CPU...\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "model.to(cpu_device)\n",
    "inputs_short_cpu = {k: v.to(cpu_device) for k, v in inputs_short.items()}\n",
    "inputs_long_cpu = {k: v.to(cpu_device) for k, v in inputs_long.items()}\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "# --- Step 2: Time Short Prompt on CPU ---\n",
    "print(\"\\nRunning inference for a SHORT prompt on CPU...\")\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    model.generate(inputs_short_cpu[\"input_ids\"], max_new_tokens=num_new_tokens_to_generate)\n",
    "end_time = time.time()\n",
    "\n",
    "cpu_latency_short = end_time - start_time\n",
    "cpu_tps_short = num_new_tokens_to_generate / cpu_latency_short\n",
    "print(f\"CPU - Short Prompt: Latency={cpu_latency_short:.4f}s, TPS={cpu_tps_short:.2f}\")\n",
    "\n",
    "# --- Step 3: Time Long Prompt on CPU ---\n",
    "print(\"\\nRunning inference for a LONG prompt on CPU...\")\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    model.generate(inputs_long_cpu[\"input_ids\"], max_new_tokens=num_new_tokens_to_generate)\n",
    "end_time = time.time()\n",
    "\n",
    "cpu_latency_long = end_time - start_time\n",
    "cpu_tps_long = num_new_tokens_to_generate / cpu_latency_long\n",
    "print(f\"CPU - Long Prompt:  Latency={cpu_latency_long:.4f}s, TPS={cpu_tps_long:.2f}\")\n",
    "\n",
    "# --- Step 4: Compare CPU Performance ---\n",
    "if cpu_latency_short > 0:\n",
    "    latency_increase = cpu_latency_long / cpu_latency_short\n",
    "    print(f\"\\nCPU Latency Increase (Long/Short): {latency_increase:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Benchmarking on GPU\n",
    "\n",
    "Now, let's see how a GPU handles this. The process is similar, but with two key differences for accurate measurement:\n",
    "\n",
    "1.  **GPU Warm-up:** The first time you run an operation on a GPU, it involves some one-time setup costs (like loading kernels). We'll perform a small, untimed run first to get these out of the way.\n",
    "2.  **Accurate Timing:** CPU-based timers like `time.time()` are not ideal for GPU operations because GPU code runs asynchronously. We'll use `torch.cuda.Event` to precisely measure the time the GPU spends on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found: Tesla T4\n",
      "\n",
      "Moving model and data to GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n",
      "\n",
      "Performing GPU warm-up run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up complete.\n",
      "\n",
      "Running inference for a SHORT prompt on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU - Short Prompt: Latency=0.7099s, TPS=70.43\n",
      "\n",
      "Running inference for a LONG prompt on GPU...\n",
      "GPU - Long Prompt:  Latency=0.7304s, TPS=68.46\n",
      "\n",
      "GPU Latency Increase (Long/Short): 1.03x\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device found: {torch.cuda.get_device_name(0)}\\n\")\n",
    "\n",
    "    # --- Step 1: Setup GPU Environment ---\n",
    "    print(\"Moving model and data to GPU...\")\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "    model.to(gpu_device)\n",
    "    inputs_short_gpu = {k: v.to(gpu_device) for k, v in inputs_short.items()}\n",
    "    inputs_long_gpu = {k: v.to(gpu_device) for k, v in inputs_long.items()}\n",
    "    print(\"Setup complete.\")\n",
    "\n",
    "    # --- Step 2: GPU Warm-up ---\n",
    "    print(\"\\nPerforming GPU warm-up run...\")\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(inputs_short_gpu[\"input_ids\"], max_new_tokens=5)\n",
    "    torch.cuda.synchronize() # Wait for the warm-up to finish\n",
    "    print(\"Warm-up complete.\")\n",
    "\n",
    "    # --- Step 3: Time Short Prompt on GPU ---\n",
    "    print(\"\\nRunning inference for a SHORT prompt on GPU...\")\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start_event.record()\n",
    "    with torch.no_grad():\n",
    "        model.generate(inputs_short_gpu[\"input_ids\"], max_new_tokens=num_new_tokens_to_generate)\n",
    "    end_event.record()\n",
    "\n",
    "    torch.cuda.synchronize() # IMPORTANT: Wait for the GPU to finish the work\n",
    "    gpu_latency_short_ms = start_event.elapsed_time(end_event)\n",
    "    gpu_latency_short = gpu_latency_short_ms / 1000.0\n",
    "    gpu_tps_short = num_new_tokens_to_generate / gpu_latency_short\n",
    "    print(f\"GPU - Short Prompt: Latency={gpu_latency_short:.4f}s, TPS={gpu_tps_short:.2f}\")\n",
    "\n",
    "    # --- Step 4: Time Long Prompt on GPU ---\n",
    "    print(\"\\nRunning inference for a LONG prompt on GPU...\")\n",
    "    start_event.record()\n",
    "    with torch.no_grad():\n",
    "        model.generate(inputs_long_gpu[\"input_ids\"], max_new_tokens=num_new_tokens_to_generate)\n",
    "    end_event.record()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_latency_long_ms = start_event.elapsed_time(end_event)\n",
    "    gpu_latency_long = gpu_latency_long_ms / 1000.0\n",
    "    gpu_tps_long = num_new_tokens_to_generate / gpu_latency_long\n",
    "    print(f\"GPU - Long Prompt:  Latency={gpu_latency_long:.4f}s, TPS={gpu_tps_long:.2f}\")\n",
    "    \n",
    "    # --- Step 5: Compare GPU Performance ---\n",
    "    if gpu_latency_short > 0:\n",
    "        latency_increase_gpu = gpu_latency_long / gpu_latency_short\n",
    "        print(f\"\\nGPU Latency Increase (Long/Short): {latency_increase_gpu:.2f}x\")\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not available. Skipping GPU comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Key Takeaways\n",
    "\n",
    "Let's break down what we observed and why it happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "A **longer input prompt leads to higher end-to-end latency** (it takes more time) and therefore **lower throughput** (fewer tokens per second), even though the model is generating the same number of new tokens in both cases. \n",
    "\n",
    "This slowdown happens on both CPU and GPU.\n",
    "\n",
    "#### The \"Why\": Self-Attention\n",
    "\n",
    "This slowdown is a fundamental characteristic of the Transformer architecture. \n",
    "\n",
    "To generate **each subsequent token**, the model must perform a self-attention operation. \n",
    "\n",
    "In this operation, the new token must \"attend to\" (or look at) **all the previous tokens**. This includes the original prompt *and* all the tokens it has already generated.\n",
    "\n",
    "   - **Short Prompt:** To generate token #11, the model attends to the 5 prompt tokens + 10 generated tokens = **15 tokens**.\n",
    "   - **Long Prompt:** To generate token #11, the model attends to the 172 prompt tokens + 10 generated tokens = **~182 tokens**.\n",
    "\n",
    "#### Implication\n",
    "\n",
    "The cost of generation is not just about the number of tokens you create; it's heavily influenced by the total context length (prompt + generation). \n",
    "\n",
    "This is why handling long contexts efficiently is a major area of research and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
