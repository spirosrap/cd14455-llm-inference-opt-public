{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implementing and Tuning Speculative Decoding\n",
    "\n",
    "**Welcome to the Exercise!**\n",
    "\n",
    "In this exercise, we will implement a simplified version of a speculative decoding loop from scratch. This will give you a deep, hands-on understanding of how this powerful technique works to accelerate inference.\n",
    "\n",
    "**Our Goal:**\n",
    "1.  Implement a standard **autoregressive decoding** loop to serve as our performance baseline.\n",
    "2.  Implement a **speculative decoding** loop using a small \"draft\" model (`gpt2`) and a larger \"target\" model (`gpt2-medium`).\n",
    "3.  Run experiments by varying the number of draft tokens (`K`) to find the **optimal value** that balances speed and efficiency.\n",
    "4.  Analyze the results to understand the trade-offs between `K`, the number of target model calls, and overall wall-clock time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Experiment Configuration\n",
    "\n",
    "Next, we'll import our libraries and configure the key parameters for the exercise, including the models we'll use and the different values of `K` (draft length) we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "TARGET_MODEL_NAME = \"gpt2-medium\"\n",
    "DRAFT_MODEL_NAME = \"gpt2\"\n",
    "\n",
    "PROMPT_TEXT = \"The future of artificial intelligence is rapidly transforming the world by\"\n",
    "MAX_TOTAL_TOKENS = 60      # Total new tokens to generate in each run\n",
    "K_VALUES_TO_TEST = [1, 2, 3, 4, 5, 8, 10] # Different draft lengths to experiment with\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models and Tokenizer\n",
    "\n",
    "Let's load our two models: the small, fast `gpt2` as our **Draft Model** and the larger, more accurate `gpt2-medium` as our **Target Model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Target Model: gpt2-medium...\n",
      "Loading Draft Model: gpt2...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading Target Model: {TARGET_MODEL_NAME}...\")\n",
    "target_model = AutoModelForCausalLM.from_pretrained(TARGET_MODEL_NAME).to(device)\n",
    "target_model.eval()\n",
    "\n",
    "print(f\"Loading Draft Model: {DRAFT_MODEL_NAME}...\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(DRAFT_MODEL_NAME).to(device)\n",
    "draft_model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DRAFT_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 1: Establish a Baseline\n",
    "\n",
    "Before we test our new method, we need to know how the standard approach performs. We'll implement a simple **autoregressive decoding loop** that uses only the slow target model to generate one token at a time. We will measure its wall-clock time and count how many times it has to call the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Baseline Autoregressive Generation ---\n",
      "Baseline Time: 1.1746 s\n",
      "Baseline Target Model Passes: 60\n"
     ]
    }
   ],
   "source": [
    "def run_baseline_generation(target_model, tokenizer, prompt_text, max_tokens):\n",
    "    \"\"\"Generates text using standard autoregressive decoding and measures performance.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    target_passes = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        # The generate function encapsulates this loop, but we write it out for clarity\n",
    "        for _ in range(max_tokens):\n",
    "            outputs = target_model(input_ids)\n",
    "            target_passes += 1\n",
    "            \n",
    "            next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "            \n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time, target_passes\n",
    "\n",
    "print(\"--- Running Baseline Autoregressive Generation ---\")\n",
    "baseline_time, baseline_passes = run_baseline_generation(target_model, tokenizer, PROMPT_TEXT, MAX_TOTAL_TOKENS)\n",
    "print(f\"Baseline Time: {baseline_time:.4f} s\")\n",
    "print(f\"Baseline Target Model Passes: {baseline_passes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 2: Implement the Speculative Decoding Loop\n",
    "\n",
    "Now for the core of the exercise. We will implement the speculative decoding logic in a single, well-commented function. This function will take `K` (the draft length) as an argument and perform the three key phases:\n",
    "1.  **Draft:** The small model generates `K` candidate tokens.\n",
    "2.  **Verify:** The large model checks all `K` tokens in a single forward pass.\n",
    "3.  **Accept:** We compare the draft with the target's predictions and accept a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_speculative_decoding(draft_model, target_model, tokenizer, prompt_text, max_tokens, k):\n",
    "    \"\"\"Runs a speculative decoding loop for a given k and measures performance.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    target_passes = 0\n",
    "    total_accepted_tokens = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        while input_ids.shape[1] < (len(tokenizer.encode(prompt_text)) + max_tokens):\n",
    "            # 1. Draft Phase: The small model generates K candidate tokens\n",
    "            draft_ids = draft_model.generate(input_ids, max_new_tokens=k, pad_token_id=tokenizer.eos_token_id)\n",
    "            draft_candidates = draft_ids[:, input_ids.shape[1]:]\n",
    "            num_drafted = draft_candidates.shape[1]\n",
    "            if num_drafted == 0: break # No more tokens can be drafted\n",
    "\n",
    "            # 2. Verification Phase: The target model gets the draft + context\n",
    "            verification_input = torch.cat([input_ids, draft_candidates], dim=1)\n",
    "            target_logits = target_model(verification_input).logits\n",
    "            target_passes += 1\n",
    "\n",
    "            # 3. Acceptance Logic: Compare draft with target's preferences\n",
    "            num_matched = 0\n",
    "            for i in range(num_drafted):\n",
    "                # Get the target's prediction for the i-th draft token position\n",
    "                verification_logit_idx = input_ids.shape[1] + i - 1\n",
    "                target_pred_id = torch.argmax(target_logits[:, verification_logit_idx, :], dim=-1)\n",
    "                \n",
    "                if draft_candidates[0, i] == target_pred_id.item():\n",
    "                    num_matched += 1\n",
    "                else:\n",
    "                    break # Mismatch found, stop comparing\n",
    "            \n",
    "            # Accept all matched tokens\n",
    "            accepted_tokens = draft_candidates[:, :num_matched]\n",
    "            input_ids = torch.cat([input_ids, accepted_tokens], dim=1)\n",
    "            \n",
    "            # If there was a mismatch, accept the target's correction\n",
    "            if num_matched < num_drafted:\n",
    "                correction_logit_idx = input_ids.shape[1] -1\n",
    "                correction_id = torch.argmax(target_logits[:, correction_logit_idx, :], dim=-1, keepdim=True)\n",
    "                input_ids = torch.cat([input_ids, correction_id], dim=1)\n",
    "            \n",
    "            if tokenizer.eos_token_id in input_ids[0]: break\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_accepted_tokens = input_ids.shape[1] - len(tokenizer.encode(prompt_text))\n",
    "    avg_accepted_per_pass = total_accepted_tokens / target_passes if target_passes > 0 else 0\n",
    "    \n",
    "    return end_time - start_time, target_passes, avg_accepted_per_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 3: Run the Experiment with Varying K\n",
    "\n",
    "Now we can create a simple loop that calls our `run_speculative_decoding` function for each value of `K` we want to test. We'll store the results to analyze later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Speculative Decoding Experiment ---\n",
      "Testing with K = 1...\n",
      "Testing with K = 2...\n",
      "Testing with K = 3...\n",
      "Testing with K = 4...\n",
      "Testing with K = 5...\n",
      "Testing with K = 8...\n",
      "Testing with K = 10...\n"
     ]
    }
   ],
   "source": [
    "results_log = []\n",
    "\n",
    "print(\"--- Running Speculative Decoding Experiment ---\")\n",
    "for k in K_VALUES_TO_TEST:\n",
    "    print(f\"Testing with K = {k}...\")\n",
    "    spec_time, spec_passes, avg_accepted = run_speculative_decoding(\n",
    "        draft_model, target_model, tokenizer, PROMPT_TEXT, MAX_TOTAL_TOKENS, k\n",
    "    )\n",
    "    results_log.append({\n",
    "        \"K\": k,\n",
    "        \"Time (s)\": spec_time,\n",
    "        \"Target Passes\": spec_passes,\n",
    "        \"Avg. Accepted Tokens\": avg_accepted\n",
    "    })\n",
    "    # Clean up GPU memory between runs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Step 4: Consolidate and Analyze Results\n",
    "\n",
    "Finally, let's put all our data into a clean table and analyze the results to find the optimal `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Speculative Decoding Experiment Results Summary ---\n",
      "Baseline Performance: Time=1.17s, Target Passes=60\n",
      "    K  Time (s)  Target Passes  Avg. Accepted Tokens\n",
      "0   1  1.861169             60              1.000000\n",
      "1   2  1.211026             33              1.818182\n",
      "2   3  1.060563             24              2.541667\n",
      "3   4  1.047493             20              3.100000\n",
      "4   5  1.021241             17              3.529412\n",
      "5   8  1.186374             14              4.714286\n",
      "6  10  1.204962             12              5.000000\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results_log)\n",
    "\n",
    "print(\"--- Speculative Decoding Experiment Results Summary ---\")\n",
    "print(f\"Baseline Performance: Time={baseline_time:.2f}s, Target Passes={baseline_passes}\")\n",
    "print(df_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guiding Questions for Analysis\n",
    "\n",
    "1.  **Target Passes**: How did the number of expensive target model passes change as `K` increased? Was it always fewer than the baseline?\n",
    "2.  **Wall-Clock Time**: How did the total generation time change with `K`? Was there an optimal `K` value that resulted in the fastest time? Why do you think time might start to increase again for very large `K`?\n",
    "3.  **Average Accepted Tokens**: How did the average number of tokens accepted per verification step change with `K`? What does this metric tell you about the efficiency of the process?\n",
    "4.  **Trade-offs & Conclusion**: What are the trade-offs of choosing a small `K` versus a large `K`? Based on your results, what would be the best `K` to use for this specific draft/target model pair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Analysis\n",
    "\n",
    "1.  **Target Passes**: As `K` increased, the number of target model passes decreased significantly and consistently. The baseline required 60 passes, while `K=10` required only a fraction of that. This is because a larger `K` allows more tokens to be verified in a single batch, reducing the number of verification steps needed to generate the full sequence.\n",
    "\n",
    "2.  **Wall-Clock Time**: Time initially decreased as `K` went from 1 to 4, hitting an **optimal point around K=4 or K=5**. For `K` values larger than that, the total time began to increase again. This happens because while a large `K` reduces target passes, the draft model generates more tokens that are likely to be incorrect. The overhead of generating these useless draft tokens, combined with processing a larger verification batch in the target model, eventually outweighs the benefit of fewer verification steps.\n",
    "\n",
    "3.  **Average Accepted Tokens**: The average number of tokens accepted per verification step consistently increased with `K`. This metric is a great measure of efficiency; a value greater than 1.0 means speculative decoding is outperforming the baseline (which accepts 1 token per pass). A higher number indicates the draft model's predictions are often correct, allowing us to accept multiple tokens for the cost of one target pass.\n",
    "\n",
    "4.  **Trade-offs & Conclusion**:\n",
    "    *   **Small `K` (e.g., 1-2)**: Safe and low overhead. The draft tokens are more likely to be correct, but the potential speedup is limited because you aren't trying to accept many tokens at once.\n",
    "    *   **Large `K` (e.g., 8-10)**: High risk, high reward. It dramatically reduces target passes, but the draft model is more likely to make a mistake early on. The computational overhead of generating and verifying many draft tokens can negate the time savings.\n",
    "    *   **Optimal `K` (e.g., 4-5)**: This is the \"sweet spot.\" It's large enough to get a significant speedup by accepting multiple tokens per pass but small enough that the draft model remains accurate and the overhead doesn't become a bottleneck.\n",
    "\n",
    "For this `gpt2`/`gpt2-medium` pairing, a **`K` value of 4 or 5 is the optimal choice**, providing the best wall-clock time speedup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
