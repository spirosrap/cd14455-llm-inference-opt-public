{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Unstructured Pruning - The Mechanics of Zeroing Out Weights\n",
    "\n",
    "**Welcome!**\n",
    "\n",
    "In this demonstration, we'll dive into the mechanics of **unstructured model pruning**. Our goal isn't to create a perfectly optimized model, but to understand the fundamental process of how weights are identified and permanently removed (zeroed out) from a specific layer in a large language model.\n",
    "\n",
    "**Our Plan:**\n",
    "1.  Load a powerful model, `meta-llama/Llama-3.2-1B`.\n",
    "2.  Establish a **baseline** by generating text with the full model.\n",
    "3.  **Target** a single, specific layer within the model's vast architecture.\n",
    "4.  Apply **L1 unstructured pruning** to zero out 50% of the weights in that layer.\n",
    "5.  **Verify** that the layer has become sparse.\n",
    "6.  **Assess the impact** by generating text again and observing the (expected) degradation in quality.\n",
    "\n",
    "We'll be using `torch.nn.utils.prune` to see exactly how this works under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (1.1.4)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authentication and Configuration\n",
    "\n",
    "Next, we need to log in to Hugging Face to download the Llama model. \n",
    "\n",
    "**Action Required:** You need to get a Hugging Face User Access Token.\n",
    "1. Go to `huggingface.co/settings/tokens`.\n",
    "2. Create a new token with \"read\" permissions.\n",
    "3. Paste your token into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your Hugging Face token here\n",
    "HF_TOKEN = \"<YOUR_HUGGING_FACE_TOKEN>\"\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's configure our demo. We'll define the model we're using, the specific layer we want to prune, and the percentage of weights to remove.\n",
    "\n",
    "**How do you find the `TARGET_LAYER_NAME_STR`?** You would typically print the entire model object (`print(model)`) and inspect its architecture to find the name of the layer you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# The specific layer we will target for pruning. \n",
    "# This is a linear layer inside the MLP block of the very first decoder layer.\n",
    "TARGET_LAYER_NAME_STR = \"model.layers.0.mlp.gate_proj\"\n",
    "\n",
    "# We will prune 50% of the weights in this layer\n",
    "PRUNING_AMOUNT = 0.5 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "print(f\"Using device: {device}, Model dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "We need two small helper functions:\n",
    "1.  `get_module_by_name_str`: PyTorch models are nested objects. This function lets us access a deeply nested layer (like `model.layers.0...`) using its string name.\n",
    "2.  `calculate_sparsity`: A simple function to calculate what percentage of weights in a layer are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_by_name_str(model, module_name_str):\n",
    "    \"\"\"Gets a module from a model using its string name (e.g., 'model.layers.0.mlp.gate_proj').\"\"\"\n",
    "    names = module_name_str.split('.')\n",
    "    current_module = model\n",
    "    for name_part in names:\n",
    "        if hasattr(current_module, name_part):\n",
    "            current_module = getattr(current_module, name_part)\n",
    "        else:\n",
    "            try: # Handle numeric indices in ModuleLists\n",
    "                idx = int(name_part)\n",
    "                current_module = current_module[idx]\n",
    "            except (ValueError, TypeError, IndexError):\n",
    "                raise AttributeError(f\"Could not resolve name part '{name_part}' in '{module_name_str}'.\")\n",
    "    return current_module\n",
    "\n",
    "def calculate_sparsity(module, param_name='weight'):\n",
    "    \"\"\"Calculates sparsity of a named parameter in a module.\"\"\"\n",
    "    if hasattr(module, param_name):\n",
    "        param = getattr(module, param_name)\n",
    "        if param is not None:\n",
    "            return 100. * float(torch.sum(param == 0)) / float(param.nelement())\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the Llama-3.2-1B Model\n",
    "\n",
    "Now, let's load our model. This can be resource-intensive, so ensure you have enough RAM or VRAM. We use `device_map=\"auto\"` to let `accelerate` handle placing the model efficiently on our hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Model: meta-llama/Llama-3.2-1B ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff24c1280914411ae4561804376e73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d50ea27f524223bf6ddda4e39831a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd44c51162343718ea7ab39a0d1b293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e77a1e02494902851b3e180feeeb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df3df8ed32f47b5a032c7fad4dcddc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539097ff73924f5ea290d17f5fb5ef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Llama model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Loading Model: {MODEL_NAME} ---\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=model_dtype,\n",
    "    device_map=\"auto\" # Automatically handle device placement\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"\\nLlama model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Pruning Process: A Step-by-Step Walkthrough\n",
    "\n",
    "Let's begin the core of our demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Establish a Baseline\n",
    "\n",
    "Before we change anything, let's see how the original, unpruned model performs on a simple prompt. This gives us a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Quick Generation PRE-Pruning ---\n",
      "Prompt: The capital of France is\n",
      "Generated by full model: The capital of France is Paris, which is a great city for tourists.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEXT_DEMO = \"The capital of France is\"\n",
    "print(f\"--- Quick Generation PRE-Pruning ---\")\n",
    "\n",
    "inputs = tokenizer(PROMPT_TEXT_DEMO, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {PROMPT_TEXT_DEMO}\")\n",
    "print(f\"Generated by full model: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2: Target the Layer and Check Initial Sparsity\n",
    "\n",
    "Using our helper function, we'll grab the specific layer we want to prune. Then, we'll check its sparsity. As expected, a normal, trained layer has virtually zero sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Accessing Target Layer: model.layers.0.mlp.gate_proj ---\n",
      "Successfully accessed target layer of type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Sparsity of 'model.layers.0.mlp.gate_proj.weight' BEFORE pruning: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Accessing Target Layer: {TARGET_LAYER_NAME_STR} ---\")\n",
    "target_module = get_module_by_name_str(model, TARGET_LAYER_NAME_STR)\n",
    "print(f\"Successfully accessed target layer of type: {type(target_module)}\")\n",
    "\n",
    "sparsity_before = calculate_sparsity(target_module, 'weight')\n",
    "print(f\"Sparsity of '{TARGET_LAYER_NAME_STR}.weight' BEFORE pruning: {sparsity_before:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3: Apply the Pruning \"Mask\"\n",
    "\n",
    "This is the first key step. We use `prune.l1_unstructured` to identify the 50% of weights with the lowest magnitude (L1 norm).\n",
    "\n",
    "**Crucially, this does *not* immediately change the weights!** Instead, PyTorch attaches a `weight_mask` and a `weight_orig` attribute to the layer. During a forward pass, the model will now use a temporary, masked version of the weights. The original weights are preserved for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying L1 unstructured pruning (amount=0.5) ---\n",
      "Pruning hook has been applied.\n",
      "The layer now has a 'weight_mask' and 'weight_orig' attribute.\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Applying L1 unstructured pruning (amount={PRUNING_AMOUNT}) ---\")\n",
    "prune.l1_unstructured(target_module, name=\"weight\", amount=PRUNING_AMOUNT)\n",
    "\n",
    "print(\"Pruning hook has been applied.\")\n",
    "print(f\"The layer now has a 'weight_mask' and 'weight_orig' attribute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.4: Make the Pruning Permanent\n",
    "\n",
    "To make our changes permanent, we call `prune.remove`. This function does two things:\n",
    "1.  It removes the pruning mask and the original weight backup.\n",
    "2.  It updates the layer's `weight` attribute to be the final, zeroed-out tensor.\n",
    "\n",
    "After this step, the weights are permanently gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Making pruning permanent for 'model.layers.0.mlp.gate_proj.weight' ---\n",
      "Pruning has been made permanent. The 'weight' attribute is now the sparse tensor.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Making pruning permanent for '{TARGET_LAYER_NAME_STR}.weight' ---\")\n",
    "prune.remove(target_module, \"weight\")\n",
    "print(\"Pruning has been made permanent. The 'weight' attribute is now the sparse tensor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.5: Verify the Final Sparsity\n",
    "\n",
    "Now, let's recalculate the sparsity. It should be very close to the 50% we aimed for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of 'model.layers.0.mlp.gate_proj.weight' AFTER pruning: 50.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparsity_after = calculate_sparsity(target_module, 'weight')\n",
    "print(f\"Sparsity of '{TARGET_LAYER_NAME_STR}.weight' AFTER pruning: {sparsity_after:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.6: Assess the Impact on Performance\n",
    "\n",
    "We've successfully removed 50% of the weights from one layer. What's the cost? Let's run the same prompt again.\n",
    "\n",
    "**Expectation:** We should see a noticeable degradation in the quality or coherence of the output. This is because we've damaged the model and have **not** performed the essential final step of a real pruning workflow: **fine-tuning** the model to help it recover from the pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Quick Generation POST-Pruning (expect quality degradation) ---\n",
      "Prompt: The capital of France is\n",
      "Generated by pruned model: The capital of France is Paris, which has a population of 2.\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Quick Generation POST-Pruning (expect quality degradation) ---\")\n",
    "\n",
    "inputs = tokenizer(PROMPT_TEXT_DEMO, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "generated_text_pruned = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: {PROMPT_TEXT_DEMO}\")\n",
    "print(f\"Generated by pruned model: {generated_text_pruned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion & The Real-World Picture\n",
    "\n",
    "This demo focused purely on the 'zeroing out' mechanics within one layer. We saw how to target a layer, apply a pruning hook, and make the change permanent. \n",
    "\n",
    "In a real-world application, the process is much more involved:\n",
    "1.  **Global Pruning:** You would prune many layers across the entire model, not just one.\n",
    "2.  **Iterative Process:** Often, pruning is done iteratively—prune a little, fine-tune, prune a little more, fine-tune again.\n",
    "3.  **CRITICAL STEP: Fine-Tuning:** After pruning, the model **must** be fine-tuned on a relevant dataset. This allows the remaining weights to adjust and compensate for the ones that were removed, recovering much of the lost performance.\n",
    "\n",
    "This demo successfully highlights the first, mechanical step in that much larger journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "# Clean up model from memory\n",
    "del model\n",
    "del tokenizer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nCleaned up models and emptied CUDA cache.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
