{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3, Exercise 1: Post-Training Quantization - Beyond Memory: Speed and Quality Trade-offs\n",
    "\n",
    "**Goal:**\n",
    "The primary goal of this exercise is to move beyond simply observing memory reduction from quantization and to comprehensively evaluate its impact. You will quantify and analyze the trade-offs between model memory footprint, inference speed (latency), and the subjective quality of generated text when applying different levels of Post-Training Quantization (PTQ) to a GPT-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch bitsandbytes pandas accelerate\n",
    "# !accelerate config default # Run this if you haven't before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"gpt2\" # Standard GPT-2\n",
    "PROMPTS = [\n",
    "    \"The capital of France is\",\n",
    "    \"Once upon a time, in a land far, far away,\",\n",
    "    \"To be or not to be, that is the\"\n",
    "]\n",
    "MAX_NEW_TOKENS = 50\n",
    "NUM_TIMING_RUNS = 3 # Number of times to run generation for averaging latency\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_footprint(model):\n",
    "    \"\"\"Gets model memory footprint in MB.\"\"\"\n",
    "    mem_params = sum([___ for param in model.parameters()]) # TODO: extract memory size for each parameter. Ref: https://discuss.pytorch.org/t/finding-model-size/130275\n",
    "    mem_bufs = sum([___ for buf in model.buffers()]) # TODO: extract memory size for each buffer. Ref: https://discuss.pytorch.org/t/finding-model-size/130275\n",
    "    mem = mem_params + mem_bufs # in bytes\n",
    "    return mem / 1024**2 # convert to MB\n",
    "\n",
    "def generate_text_and_time(model, tokenizer, prompt, max_new_tokens):\n",
    "    \"\"\"Generates text and returns the generated text and latency.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    start_time = time.perf_counter() # Use perf_counter for more precise timing\n",
    "    if model.device.type == 'cuda':\n",
    "        torch.cuda.synchronize() # Ensure previous CUDA ops are done\n",
    "        \n",
    "    outputs = None # TODO: write generation logic\n",
    "    \n",
    "    if model.device.type == 'cuda':\n",
    "        torch.cuda.synchronize() # Ensure generation is done\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    generated_text = None # TODO: extract generated text from output. HINT: use the tokenizer \n",
    "    latency = end_time - start_time\n",
    "    return generated_text, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Experiment Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [] # Use a different name to avoid conflict if re-running cells\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define precision configurations to test\n",
    "configurations = [\n",
    "    ### TODO: Define the baseline configuration (FP16 for GPU or FP32 for CPU)\n",
    "    # Example: {\"name\": \"FP16 (Baseline GPU)\" if device.type == \"cuda\" else \"FP32 (Baseline CPU)\", \"load_args\": {\"torch_dtype\": torch.float16 if device.type == \"cuda\" else torch.float32}},\n",
    "    \n",
    "    ### TODO: Define the INT8 quantization configuration using bitsandbytes\n",
    "    # Example: {\"name\": \"INT8 (bitsandbytes)\", \"load_args\": {\"load_in_8bit\": True, \"device_map\": \"auto\" if device.type == \"cuda\" else None}},\n",
    "    \n",
    "    ### TODO: Define the NF4 (4-bit) quantization configuration using bitsandbytes\n",
    "    \n",
    "    ### TODO: Define the FP4 (4-bit) quantization configuration\n",
    "    ]\n",
    "\n",
    "# Adjust configurations if running on CPU (bitsandbytes quantization typically requires CUDA)\n",
    "if device.type == \"cpu\":\n",
    "    print(\"Bitsandbytes quantization (INT8, NF4, FP4) usually requires CUDA. Filtering configurations.\")\n",
    "    configurations = [config for config in configurations if \"bitsandbytes\" not in config[\"name\"]]\n",
    "\n",
    "print(f\"\\n--- Starting Experiment for Model: {MODEL_NAME} ---\")\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\nLoading model with configuration: {config['name']}\")\n",
    "    model = None # Ensure model is reset\n",
    "    try:\n",
    "        ### TODO: Load the model using AutoModelForCausalLM.from_pretrained()\n",
    "        # Use the load_args from the current 'config'.\n",
    "        # Handle device placement correctly (model.to(device) if not using device_map - e.g. in CPU usecase).\n",
    "        # Example for handling device_map on CPU (though bitsandbytes won't quantize):\n",
    "        # current_load_args = config['load_args']\n",
    "        # if \"device_map\" in current_load_args and device.type == \"cpu\":\n",
    "        #    current_load_args = {k: v for k, v in current_load_args.items() if k != \"device_map\"}\n",
    "        #    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **current_load_args)\n",
    "        #    model.to(device)\n",
    "        pass # Replace with model loading logic on GPU using bitsandbytes. Hint: for this we can directly pass \n",
    "             # the config['load_args'] to the from_pretrained method.\n",
    "\n",
    "        if model is None: # Check if model loading was skipped/failed in TODO\n",
    "            print(f\"Skipping {config['name']} due to model loading not implemented in TODO.\")\n",
    "            continue\n",
    "\n",
    "        ### TODO: Get the model memory footprint using get_model_memory_footprint\n",
    "        memory_mb = None # Placeholder\n",
    "        print(f\"Memory Footprint: {memory_mb:.2f} MB\")\n",
    "\n",
    "        avg_latencies_for_config = []\n",
    "        generated_outputs_for_prompts = {}\n",
    "\n",
    "        for i, prompt_text in enumerate(PROMPTS):\n",
    "            print(f\"  Processing prompt: '{prompt_text[:30]}...' \")\n",
    "            prompt_specific_latencies = []\n",
    "            current_generated_text = \"N/A\"\n",
    "            \n",
    "            ### TODO: Implement the timing loop (NUM_TIMING_RUNS)\n",
    "            # Only perform multiple timing runs for the first prompt to establish 'Avg Latency (s)'\n",
    "            # For other prompts, generate text once for quality assessment.\n",
    "            # Store the first generation's text in 'current_generated_text'.\n",
    "            # Accumulate latencies for the first prompt in 'prompt_specific_latencies'. Use generate_text_and_time function.\n",
    "            # Collect the generated text for each prompt in generated_outputs_for_prompts\n",
    "\n",
    "        overall_avg_latency_for_config = sum(avg_latencies_for_config) / len(avg_latencies_for_config) if avg_latencies_for_config else float('nan')\n",
    "\n",
    "        results_list.append({\n",
    "            \"Float Precision\": config[\"name\"],\n",
    "            \"Memory (MB)\": memory_mb,\n",
    "            \"Avg Latency (s)\": overall_avg_latency_for_config, # Based on first prompt's timing\n",
    "            **generated_outputs_for_prompts\n",
    "        })\n",
    "        \n",
    "        del model # Free up memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not run configuration {config['name']}. Error: {e}\")\n",
    "        results_list.append({\n",
    "            \"Float Precision\": config[\"name\"],\n",
    "            \"Memory (MB)\": \"Error\",\n",
    "            \"Avg Latency (s)\": \"Error\",\n",
    "            **{f\"Prompt {i+1} Output\": \"Error\" for i in range(len(PROMPTS))}\n",
    "        })\n",
    "\n",
    "# --- Display Results ---\n",
    "df_results = pd.DataFrame(results_list)\n",
    "print(\"\\n\\n--- Experiment Results Summary ---\")\n",
    "print(df_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis and Discussion\n",
    "\n",
    "Based on the 'Experiment Results Summary' table printed above, analyze your findings:\n",
    "\n",
    "1.  **Memory Scaling:** \n",
    "    *   **TODO**: Describe how the model's memory footprint scaled as you reduced precision. Quantify the reductions.\n",
    "\n",
    "2.  **Latency Changes:** \n",
    "    *   **TODO**: Analyze the changes in generation latency. Did latency always decrease with lower precision, or were there other factors at play? Explain potential reasons.\n",
    "\n",
    "3.  **Output Quality Degradation:** \n",
    "    *   **TODO**: Based on your subjective review of the outputs for each prompt and precision, at what point (if any) did you start to observe significant degradation in the output quality (e.g., coherence, relevance, repetitiveness)? Provide examples.\n",
    "\n",
    "4.  **Key Trade-offs:** \n",
    "    *   **TODO**: Conclude with a summary of the key trade-offs you observed between memory savings, inference speed, and text quality when applying different quantization levels to GPT-2. Which configuration seemed to offer the best balance for which scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
