{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Speculative Decoding - A Step-by-Step Look at the Logic\n",
    "\n",
    "**Welcome!**\n",
    "\n",
    "In this demo, we'll pull back the curtain on **Speculative Decoding**, one of the most clever techniques for speeding up LLM inference. We won't build a full, optimized loop, but instead, we'll walk through a **single, detailed step** to understand the core logic.\n",
    "\n",
    "**The Two Players:**\n",
    "1.  **The Draft Model (`gpt2`):** A small, fast model. Think of it as a scout that runs ahead and quickly suggests a path.\n",
    "2.  **The Target Model (`gpt2-medium`):** A larger, more accurate, but slower model. This is the general who verifies the scout's path.\n",
    "\n",
    "**Our Goal:** To see how the general (target model) can efficiently verify the scout's suggestions (draft tokens) and accept multiple tokens for the cost of just one slow operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "Now, we'll import our libraries and configure the demo. We'll set the names for our two models and decide how many tokens our fast draft model should propose in this step (`K_DRAFT_TOKENS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "TARGET_MODEL_NAME = \"gpt2-medium\"\n",
    "DRAFT_MODEL_NAME = \"gpt2\"\n",
    "K_DRAFT_TOKENS = 5 # Let's have the draft model propose 5 tokens\n",
    "\n",
    "INITIAL_CONTEXT_TEXT = \"There are different ways to optimize LLM inference. One\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Models (Our \"Players\")\n",
    "\n",
    "Let's load both the small draft model and the larger target model. We'll also load the tokenizer, which is the same for both `gpt2` and `gpt2-medium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Target Model ('The General'): gpt2-medium...\n",
      "Loading Draft Model ('The Scout'): gpt2...\n",
      "Models and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(DRAFT_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading Target Model ('The General'): {TARGET_MODEL_NAME}...\")\n",
    "target_model = AutoModelForCausalLM.from_pretrained(TARGET_MODEL_NAME).to(device)\n",
    "target_model.eval() # Set to evaluation mode\n",
    "\n",
    "print(f\"Loading Draft Model ('The Scout'): {DRAFT_MODEL_NAME}...\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(DRAFT_MODEL_NAME).to(device)\n",
    "draft_model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Models and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Speculative Decoding Step\n",
    "\n",
    "Let's begin our detailed, single-step walkthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: The Scout Runs Ahead (Draft Generation)\n",
    "\n",
    "First, our small, fast `gpt2` draft model takes the current context and quickly generates `K` candidate tokens. This happens one by one, but each step is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Draft Model generates 5 candidate tokens ---\n",
      "Initial Context: 'There are different ways to optimize LLM inference. One'\n",
      "Draft Model's Proposal: ' is to use a simple'\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Step 1: Draft Model generates {K_DRAFT_TOKENS} candidate tokens ---\")\n",
    "current_context_ids = tokenizer.encode(INITIAL_CONTEXT_TEXT, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use the draft model's generate function for simplicity\n",
    "    draft_output_ids = draft_model.generate(\n",
    "        current_context_ids,\n",
    "        max_new_tokens=K_DRAFT_TOKENS,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Isolate just the newly generated draft tokens\n",
    "draft_candidate_ids = draft_output_ids[:, current_context_ids.shape[1]:]\n",
    "\n",
    "print(f\"Initial Context: '{INITIAL_CONTEXT_TEXT}'\")\n",
    "print(f\"Draft Model's Proposal: '{tokenizer.decode(draft_candidate_ids[0], skip_special_tokens=True)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: The General Reviews the Plan (Target Model Verification)\n",
    "\n",
    "Now for the clever part. We take the **original context + the draft tokens** and feed this entire sequence to the large target model in a **single forward pass**.\n",
    "\n",
    "Because Transformers process all tokens in parallel, this single pass gives us the target model's prediction for what token should come after *each* position in the input. This is far more efficient than calling the target model K times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Target Model verifies the draft in a single pass ---\n",
      "Verification Input: 'There are different ways to optimize LLM inference. One is to use a simple'\n",
      "Target Model's Preferences: ' is to use a single'\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Step 2: Target Model verifies the draft in a single pass ---\")\n",
    "# Combine context and draft for the verification input\n",
    "verification_input_ids = torch.cat([current_context_ids, draft_candidate_ids], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    target_verification_logits = target_model(verification_input_ids).logits\n",
    "\n",
    "# Get the target model's top-1 prediction for each position\n",
    "# Note: The logits at index `t-1` predict the token at index `t`\n",
    "# So we look at logits from the start of the draft sequence onwards\n",
    "start_of_draft_in_logits = current_context_ids.shape[1] - 1\n",
    "end_of_draft_in_logits = verification_input_ids.shape[1] - 1\n",
    "\n",
    "target_preferred_ids = torch.argmax(target_verification_logits[:, start_of_draft_in_logits:end_of_draft_in_logits, :], dim=-1)\n",
    "\n",
    "print(f\"Verification Input: '{tokenizer.decode(verification_input_ids[0])}'\")\n",
    "print(f\"Target Model's Preferences: '{tokenizer.decode(target_preferred_ids[0], skip_special_tokens=True)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: The Verdict (Comparison and Acceptance)\n",
    "\n",
    "Now we compare the scout's path with the general's preferred path, one step at a time. We stop as soon as we find a mismatch.\n",
    "\n",
    "| Position | Draft Model's Token | Target Model's Preference | Result   |\n",
    "|----------|-----------------------|-----------------------------|----------|\n",
    "| 1        | ...                   | ...                         | Match?   |\n",
    "| 2        | ...                   | ...                         | Match?   |\n",
    "| ...      | ...                   | ...                         | ...      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Comparing draft against target preferences ---\n",
      "Pos 1: Draft (' is') vs Target (' is')\n",
      "  ✅ Match!\n",
      "Pos 2: Draft (' to') vs Target (' to')\n",
      "  ✅ Match!\n",
      "Pos 3: Draft (' use') vs Target (' use')\n",
      "  ✅ Match!\n",
      "Pos 4: Draft (' a') vs Target (' a')\n",
      "  ✅ Match!\n",
      "Pos 5: Draft (' simple') vs Target (' single')\n",
      "  ❌ Mismatch! Halting comparison.\n",
      "\n",
      "Number of matched tokens: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 3: Comparing draft against target preferences ---\")\n",
    "num_matched_tokens = 0\n",
    "for i in range(draft_candidate_ids.shape[1]):\n",
    "    draft_token = draft_candidate_ids[0, i]\n",
    "    target_token = target_preferred_ids[0, i]\n",
    "    \n",
    "    print(f\"Pos {i+1}: Draft ('{tokenizer.decode(draft_token)}') vs Target ('{tokenizer.decode(target_token)}')\")\n",
    "    \n",
    "    if draft_token == target_token:\n",
    "        print(\"  ✅ Match!\")\n",
    "        num_matched_tokens += 1\n",
    "    else:\n",
    "        print(\"  ❌ Mismatch! Halting comparison.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nNumber of matched tokens: {num_matched_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4: The Outcome (Constructing the Final Output)\n",
    "\n",
    "Based on the comparison, we can now form our final output for this step. The rule is:\n",
    "\n",
    "**`final_tokens = [all matched tokens] + [the target's 'correct' token at the mismatch point]`**\n",
    "\n",
    "If all tokens matched, the second part is simply the next token the target model would have generated anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Constructing the final accepted sequence for this step ---\n",
      "Matched Tokens Accepted: ' is to use a'\n",
      "Correction/Extension Token: ' single'\n",
      "\n",
      "--- SUMMARY OF THIS STEP ---\n",
      "Tokens generated this step: 5 -> ' is to use a single'\n",
      "Target Model expensive calls: 1\n",
      "New Context: 'There are different ways to optimize LLM inference. One is to use a single'\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 4: Constructing the final accepted sequence for this step ---\")\n",
    "\n",
    "# 1. Take all the tokens that matched\n",
    "accepted_ids = draft_candidate_ids[0, :num_matched_tokens]\n",
    "\n",
    "# 2. Take the target's token at the next position \n",
    "# (This is either the correction at the mismatch point, or the next token if all matched)\n",
    "if num_matched_tokens < target_preferred_ids.shape[1]:\n",
    "    next_token = target_preferred_ids[0, num_matched_tokens].unsqueeze(0)\n",
    "    final_accepted_ids = torch.cat([accepted_ids, next_token], dim=0)\n",
    "else: # This case is rare, means we ran out of target preferences\n",
    "    final_accepted_ids = accepted_ids\n",
    "\n",
    "print(f\"Matched Tokens Accepted: '{tokenizer.decode(accepted_ids)}'\")\n",
    "if num_matched_tokens < target_preferred_ids.shape[1]:\n",
    "    print(f\"Correction/Extension Token: '{tokenizer.decode(next_token)}'\")\n",
    "\n",
    "# Update our full context\n",
    "new_context_ids = torch.cat([current_context_ids, final_accepted_ids.unsqueeze(0)], dim=1)\n",
    "\n",
    "print(\"\\n--- SUMMARY OF THIS STEP ---\")\n",
    "print(f\"Tokens generated this step: {len(final_accepted_ids)} -> '{tokenizer.decode(final_accepted_ids)}'\")\n",
    "print(f\"Target Model expensive calls: 1\")\n",
    "print(f\"New Context: '{tokenizer.decode(new_context_ids[0])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "And that's the core loop! In this single step, we generated **multiple tokens** but only paid the high computational cost of the `gpt2-medium` model **once**. \n",
    "\n",
    "The potential for a 2-3x speedup comes from this simple fact: if the small draft model is a reasonably good guesser, we can accept many of its tokens in parallel, breaking the one-token-at-a-time bottleneck of traditional autoregressive generation. And because the powerful target model always gets the final say, there is **no loss in output quality**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "# Clean up models from memory\n",
    "del target_model\n",
    "del draft_model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nCleaned up models and emptied CUDA cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
