{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Post-Training Quantization (PTQ) with GPT-2 & bitsandbytes\n",
    "\n",
    "**Welcome!**\n",
    "\n",
    "In this demo, we'll explore one of the most powerful and easy-to-use techniques for making large models more efficient: **Post-Training Quantization (PTQ)**.\n",
    "\n",
    "**Our Goal:** We'll take a standard `GPT-2` model and load it in three different ways:\n",
    "1.  **Baseline:** In its standard high-precision format (FP16).\n",
    "2.  **8-bit Quantized:** A significantly smaller version.\n",
    "3.  **4-bit Quantized:** An even more aggressively compressed version.\n",
    "\n",
    "We will measure the memory footprint at each step to see the dramatic savings firsthand. The focus is on the simplicity offered by modern tools like Hugging Face `transformers` and `bitsandbytes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries. We need:\n",
    "- `transformers`: For loading our pre-trained GPT-2 model.\n",
    "- `torch`: The core deep learning framework.\n",
    "- `bitsandbytes`: The magic library that enables easy quantization on-the-fly.\n",
    "- `accelerate`: A helper library from Hugging Face that simplifies running PyTorch on different hardware (like multiple GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.2.2)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.8.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m144.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m199.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m194.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.8.0-py3-none-any.whl (365 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: safetensors, hf-xet, huggingface-hub, tokenizers, bitsandbytes, accelerate, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.8.0 bitsandbytes-0.46.0 hf-xet-1.1.4 huggingface-hub-0.33.0 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "Now, we'll import the required modules and set up some basic configurations, like the model name we want to use. We'll also check if a CUDA-enabled GPU is available, as `bitsandbytes` is optimized for this hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Define the model we want to use\n",
    "MODEL_NAME = \"gpt2\"\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(\"\\nWARNING: bitsandbytes 8-bit and 4-bit quantization is primarily designed for CUDA GPUs. Memory savings and performance gains will not be apparent on a CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Function: Measure Model Memory\n",
    "\n",
    "To accurately compare our models, we need a way to measure how much memory they occupy. This helper function will calculate the model's size in megabytes (MB) by summing up the memory used by all of its parameters and buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_footprint(model):\n",
    "    \"\"\"Calculates and returns the model's memory footprint in MB.\"\"\"\n",
    "    mem_params = sum(param.nelement() * param.element_size() for param in model.parameters())\n",
    "    mem_bufs = sum(buf.nelement() * buf.element_size() for buf in model.buffers())\n",
    "    total_mem_bytes = mem_params + mem_bufs\n",
    "    return total_mem_bytes / (1024 ** 2) # Convert bytes to MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Quantization Experiment\n",
    "\n",
    "Let's begin the experiment! We'll start by initializing our tokenizer and a dictionary to store the memory footprint results for comparison later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d8387fd2ae419b8858b865e26e0d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2640880b3144b4b19957da01db2658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33727771c6904be9b5e00cd2016ad7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa5af82a1f94a03a7d768973b1a2b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac4226d73514596b21f41fe8588cc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "memory_footprints = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Load the Baseline Model (FP16)\n",
    "\n",
    "First, we load the standard GPT-2 model. This will be our **baseline**. On a GPU, it's common to use `float16` (FP16) precision, which is already a good optimization over `float32`. This gives us a realistic starting point to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Baseline Model (FP16) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0cf118a03b4b2b8211fd353878662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e75ac0af005402c89fe1219385f2f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'FP16 (Baseline)' model.\n",
      "Memory Footprint: 249.35 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Baseline Model (FP16) ---\")\n",
    "baseline_name = \"FP16 (Baseline)\"\n",
    "\n",
    "# Load the model in float16 and move it to the GPU\n",
    "model_baseline = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=torch.float16 # Use half-precision\n",
    ").to(device)\n",
    "\n",
    "memory_baseline = get_model_memory_footprint(model_baseline)\n",
    "memory_footprints[baseline_name] = f\"{memory_baseline:.2f} MB\"\n",
    "print(f\"Loaded '{baseline_name}' model.\")\n",
    "print(f\"Memory Footprint: {memory_baseline:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Load the 8-bit Quantized Model\n",
    "\n",
    "Now for our first optimization. By adding just one argument, **`load_in_8bit=True`**, we instruct `bitsandbytes` to quantize the model's weights to 8-bit integers as it's being loaded. We also use **`device_map=\"auto\"`**, which is highly recommended to let the library handle placing the model on the correct device(s) efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Loading Model with 8-bit Quantization ---\n",
      "Loaded 'INT8 (bitsandbytes)' model.\n",
      "Memory Footprint: 168.35 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Loading Model with 8-bit Quantization ---\")\n",
    "quant_8bit_name = \"INT8 (bitsandbytes)\"\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        load_in_8bit=True, \n",
    "        device_map=\"auto\" # Recommended for bitsandbytes\n",
    "    )\n",
    "    memory_8bit = get_model_memory_footprint(model_8bit)\n",
    "    memory_footprints[quant_8bit_name] = f\"{memory_8bit:.2f} MB\"\n",
    "    print(f\"Loaded '{quant_8bit_name}' model.\")\n",
    "    print(f\"Memory Footprint: {memory_8bit:.2f} MB\")\n",
    "else:\n",
    "    print(\"Skipping 8-bit quantization as CUDA is not available.\")\n",
    "    memory_footprints[quant_8bit_name] = \"N/A (CPU)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Load the 4-bit Quantized Model (NF4)\n",
    "\n",
    "Let's get even more aggressive. We can use **`load_in_4bit=True`** to achieve a massive memory reduction. By default, this uses the **NF4** (NormalFloat 4-bit) data type, a format specifically designed to be highly efficient for the distribution of weights typically found in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Loading Model with 4-bit Quantization (NF4) ---\n",
      "Loaded 'NF4 (4-bit bitsandbytes)' model.\n",
      "Memory Footprint: 127.85 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 3. Loading Model with 4-bit Quantization (NF4) ---\")\n",
    "quant_4bit_nf4_name = \"NF4 (4-bit bitsandbytes)\"\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    model_4bit_nf4 = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        load_in_4bit=True, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    memory_4bit_nf4 = get_model_memory_footprint(model_4bit_nf4)\n",
    "    memory_footprints[quant_4bit_nf4_name] = f\"{memory_4bit_nf4:.2f} MB\"\n",
    "    print(f\"Loaded '{quant_4bit_nf4_name}' model.\")\n",
    "    print(f\"Memory Footprint: {memory_4bit_nf4:.2f} MB\")\n",
    "else:\n",
    "    print(\"Skipping 4-bit NF4 quantization as CUDA is not available.\")\n",
    "    memory_footprints[quant_4bit_nf4_name] = \"N/A (CPU)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze the Results: Memory Footprint Summary\n",
    "\n",
    "Now for the moment of truth! Let's print the memory footprints we recorded for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memory Footprint Summary ---\n",
      "FP16 (Baseline): 249.35 MB\n",
      "INT8 (bitsandbytes): 168.35 MB\n",
      "NF4 (4-bit bitsandbytes): 127.85 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Memory Footprint Summary ---\")\n",
    "for name, mem in memory_footprints.items():\n",
    "    print(f\"{name}: {mem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sanity Check: Does the Quantized Model Still Work?\n",
    "\n",
    "A smaller model is useless if it can't generate text. Let's do a quick sanity check with our most compressed model (the 4-bit NF4 version) to confirm that it's still functional and can complete a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sanity Check: Text Generation with 4-bit NF4 Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of artificial intelligence is\n",
      "Generated by NF4 model: The future of artificial intelligence is uncertain.\n",
      "\n",
      "\"I don't know if we're going to see it, but I think it's going to be\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sanity Check: Text Generation with 4-bit NF4 Model ---\")\n",
    "\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "MAX_NEW_TOKENS_DEMO = 25\n",
    "\n",
    "if 'model_4bit_nf4' in locals() and model_4bit_nf4 is not None:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_4bit_nf4.device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model_4bit_nf4.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS_DEMO, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated by NF4 model: {generated_text}\")\n",
    "else:\n",
    "    print(\"4-bit NF4 model was not loaded, skipping generation sanity check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup and Conclusion\n",
    "\n",
    "Finally, it's good practice to explicitly delete the models and clear the GPU cache to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "# Clean up models from memory\n",
    "del model_baseline\n",
    "if 'model_8bit' in locals(): del model_8bit\n",
    "if 'model_4bit_nf4' in locals(): del model_4bit_nf4\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nCleaned up models and emptied CUDA cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway from Demo:\n",
    "\n",
    "This demo powerfully illustrates how simple it is to apply Post-Training Quantization with modern libraries. With just a single argument during model loading (`load_in_8bit=True` or `load_in_4bit=True`), we drastically reduced the memory footprint of a pre-trained model, making it possible to run larger models on consumer-grade hardware.\n",
    "\n",
    "While we focused on memory here, the trade-offs with inference speed and output quality are critical real-world considerations that will be explored in the subsequent exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
