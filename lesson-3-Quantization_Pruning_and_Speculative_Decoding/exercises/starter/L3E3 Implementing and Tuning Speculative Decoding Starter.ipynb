{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3, Exercise 3: Tuning Speculation with GPT-2 - Impact of Draft Length (`K`)\n",
    "\n",
    "**Goal:**\n",
    "The purpose of this exercise is to implement a simplified speculative decoding loop using readily available GPT-2 models and to investigate how a key hyperparameter – the draft length `K` (the number of tokens speculatively generated by the draft model) – influences the overall efficiency of the generation process. You will measure performance in terms of both wall-clock time and the number of computationally expensive forward passes through the target model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "TARGET_MODEL_NAME = \"gpt2-medium\"\n",
    "DRAFT_MODEL_NAME = \"gpt2\" # Standard small GPT-2\n",
    "\n",
    "PROMPT_TEXT = \"Artificial intelligence is rapidly transforming our world by\"\n",
    "MAX_TOTAL_TOKENS_TO_GENERATE = 100 # Total new tokens to generate for each run\n",
    "K_VALUES_TO_TEST = [1, 2, 3, 4, 5, 8]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "target_model = None\n",
    "draft_model = None\n",
    "\n",
    "### TODO: Load the tokenizer (should be same for gpt2 and gpt2-medium)\n",
    "tokenizer = None\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "pass\n",
    "\n",
    "### TODO: Load the Target Model (gpt2-medium) and move to device\n",
    "target_model = None\n",
    "target_model.eval()\n",
    "pass\n",
    "\n",
    "### TODO: Load the Draft Model (gpt2) and move to device\n",
    "draft_model = None\n",
    "draft_model.eval()\n",
    "pass\n",
    "\n",
    "if not all([tokenizer, target_model, draft_model]):\n",
    "    raise ValueError(\"One or more models/tokenizer failed to load. Check TODOs.\")\n",
    "print(\"Models and tokenizer loaded successfully.\")\n",
    "\n",
    "initial_prompt_ids = tokenizer.encode(PROMPT_TEXT, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Autoregressive Generation (Using Target Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Baseline Autoregressive Generation ---\")\n",
    "baseline_generated_ids = initial_prompt_ids.clone()\n",
    "baseline_target_passes = 0\n",
    "start_time_baseline = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(MAX_TOTAL_TOKENS_TO_GENERATE):\n",
    "        # TODO: write generation loop for generating 1 token at a time\n",
    "        # 1.\tRun the target model on the current sequence of generated tokens\n",
    "        # 2.\tExtract the logits for the next token position. E.g outputs.logits[:, -1, :]\n",
    "        # 3.\tSelect the most likely next token using argmax\n",
    "        # 4.\tAppend this next token to the current sequence (in 1st iteration, it is just the prompt)\n",
    "        # 5.\tIncrement the count of target model passes\n",
    "        next_token_id = None # Implement above logic\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "end_time_baseline = time.time()\n",
    "baseline_time = end_time_baseline - start_time_baseline\n",
    "baseline_output_text = tokenizer.decode(baseline_generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Baseline Time: {baseline_time:.4f} s\")\n",
    "print(f\"Baseline Target Model Passes: {baseline_target_passes}\")\n",
    "print(f\"Baseline Output: {baseline_output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Speculative Decoding Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiment_results = []\n",
    "\n",
    "if target_model and draft_model and tokenizer and initial_prompt_ids is not None:\n",
    "    print(\"\\n--- Running Speculative Decoding Experiments for different K values ---\")\n",
    "\n",
    "    for K_val in K_VALUES_TO_TEST:\n",
    "        print(f\"\\nStarting Speculative Decoding with K = {K_val}\")\n",
    "        current_context_ids_spec = initial_prompt_ids.clone()\n",
    "        count_total_new_tokens_generated_spec = 0\n",
    "        count_target_model_passes_spec = 0\n",
    "        count_total_draft_tokens_verified_correctly = 0\n",
    "        count_verification_steps_spec = 0 # How many times the target model was called to verify\n",
    "\n",
    "        # Timing Start\n",
    "        if device.type == 'cuda': torch.cuda.synchronize()\n",
    "        time_start_speculative = time.perf_counter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while count_total_new_tokens_generated_spec < MAX_TOTAL_TOKENS_TO_GENERATE:\n",
    "                if current_context_ids_spec.shape[1] >= target_model.config.max_position_embeddings - K_val -1: # Ensure space for K draft + 1 target\n",
    "                    print(f\"K={K_val}: Approaching model's maximum length. Stopping early to prevent overflow.\")\n",
    "                    break\n",
    "\n",
    "                # --- Draft Phase ---\n",
    "                draft_tokens_generated_ids = torch.empty((1,0), dtype=torch.long, device=device)\n",
    "                temp_context_for_draft = current_context_ids_spec.clone()\n",
    "                for _ in range(K_val):\n",
    "                    if temp_context_for_draft.shape[1] >= draft_model.config.max_position_embeddings:\n",
    "                        break # Draft model also has a max length\n",
    "                    ### TODO: Generate one token using the 'draft_model' from 'temp_context_for_draft'.\n",
    "                    # Append it to 'draft_tokens_generated_ids' and update 'temp_context_for_draft'.\n",
    "                    # Break if EOS is generated by the draft model.\n",
    "                    pass # Replace with draft generation step\n",
    "                \n",
    "                num_tokens_actually_drafted = draft_tokens_generated_ids.shape[1]\n",
    "                if num_tokens_actually_drafted == 0:\n",
    "                    # If no tokens could be drafted (e.g., context too long or K=0 was somehow missed),\n",
    "                    # fall back to a single autoregressive step with the target model to make progress.\n",
    "                    # ... (Implement target model single step here, update counts, and 'continue' the outer while loop) ...\n",
    "                    print(f\"K={K_val}: No tokens drafted, performing target step. (Implement fallback)\")\n",
    "                    # For starter, just break to prevent infinite loop if not implemented\n",
    "                    break \n",
    "\n",
    "                # --- Verification Phase ---\n",
    "                verification_input_ids = torch.cat([current_context_ids_spec, draft_tokens_generated_ids], dim=1)\n",
    "                \n",
    "                ### TODO: Pass 'verification_input_ids' to the 'target_model' to get its verification outputs.\n",
    "                # Increment 'count_target_model_passes_spec'.\n",
    "                # Increment 'count_verification_steps_spec'.\n",
    "                pass # Replace with target model call\n",
    "                target_verification_outputs_logits = torch.rand(1, verification_input_ids.shape[1], tokenizer.vocab_size, device=device) # Dummy logits\n",
    "                count_target_model_passes_spec +=1; count_verification_steps_spec +=1 # Dummy increment\n",
    "\n",
    "                ### TODO: Extract the target model's preferred tokens for each of the 'num_tokens_actually_drafted' positions.\n",
    "                # These are the tokens the target model *would have chosen* if it were generating autoregressively at those steps.\n",
    "                # You'll need to look at the logits from 'target_verification_outputs.logits' at the correct indices.\n",
    "                # Store these preferred token IDs in a list or tensor called 'target_preferred_tokens_at_draft_positions'.\n",
    "                # Careful with indexing: logits at index `t` predict token `t+1`.\n",
    "                target_preferred_tokens_at_draft_positions = draft_tokens_generated_ids.squeeze().tolist() # Dummy: assumes target agrees perfectly\n",
    "\n",
    "                # --- Acceptance Logic ---\n",
    "                num_matched_tokens = 0\n",
    "                ### TODO: Compare 'draft_tokens_generated_ids' with 'target_preferred_tokens_at_draft_positions' token by token.\n",
    "                # Count how many tokens match consecutively from the beginning. Store this in 'num_matched_tokens'.\n",
    "                # for i in range(num_tokens_actually_drafted):\n",
    "                #    if ___ == ___:\n",
    "                #        num_matched_tokens += 1\n",
    "                #    else: break\n",
    "                num_matched_tokens = num_tokens_actually_drafted # Dummy: assume all match\n",
    "                count_total_draft_tokens_verified_correctly += num_matched_tokens\n",
    "\n",
    "                accepted_tokens_for_this_step = draft_tokens_generated_ids[0, :num_matched_tokens]\n",
    "\n",
    "                # Determine the next token: either the one from target model at mismatch, or one beyond matched sequence.\n",
    "                if num_matched_tokens < num_tokens_actually_drafted:\n",
    "                    # Mismatch occurred: use target's preferred token at the point of mismatch.\n",
    "                    # next_token_id_after_match = torch.tensor([[target_preferred_tokens_at_draft_positions[num_matched_tokens]]], device=device)\n",
    "                    pass # Replace, placeholder below\n",
    "                    next_token_id_after_match = torch.randint(0, tokenizer.vocab_size, (1,1), device=device) # Dummy next token\n",
    "                else: # All K draft tokens matched\n",
    "                    # Try to get one more token from the target model (the (K+1)th token).\n",
    "                    # This requires looking at the logits from 'target_verification_outputs' for the position *after* the K draft tokens.\n",
    "                    # verification_idx_for_bonus_token = current_context_ids_spec.shape[1] + num_tokens_actually_drafted -1\n",
    "                    # if verification_idx_for_bonus_token < target_verification_outputs.logits.shape[1]:\n",
    "                    #    next_token_id_after_match = torch.argmax(target_verification_outputs.logits[0, verification_idx_for_bonus_token, :]).unsqueeze(0).unsqueeze(0)\n",
    "                    # else: # Cannot get bonus token, just proceed with matched ones (or handle as error/fallback)\n",
    "                    #    next_token_id_after_match = None \n",
    "                    pass # Replace, placeholder below\n",
    "                    next_token_id_after_match = torch.randint(0, tokenizer.vocab_size, (1,1), device=device) # Dummy next token\n",
    "                \n",
    "                if next_token_id_after_match is not None:\n",
    "                     accepted_tokens_for_this_step = torch.cat([accepted_tokens_for_this_step, next_token_id_after_match.squeeze()])\n",
    "\n",
    "                if accepted_tokens_for_this_step.numel() == 0:\n",
    "                    print(f\"K={K_val}: No tokens were accepted in this step. Performing a single target model step to advance.\")\n",
    "                    # Fallback: Generate one token with target model to ensure progress.\n",
    "                    # ... (Implement target model single step here, update counts, and 'continue' the outer while loop) ...\n",
    "                    break # Simplified for starter, real fallback needed\n",
    "                \n",
    "                current_context_ids_spec = torch.cat([current_context_ids_spec, accepted_tokens_for_this_step.unsqueeze(0) if accepted_tokens_for_this_step.dim() == 1 else accepted_tokens_for_this_step], dim=1)\n",
    "                num_total_new_tokens_generated += accepted_tokens_for_this_step.shape[0] if accepted_tokens_for_this_step.dim() == 1 else accepted_tokens_for_this_step.shape[1]\n",
    "\n",
    "                if tokenizer.eos_token_id in accepted_tokens_for_this_step:\n",
    "                    print(f\"K={K_val}: EOS token generated.\")\n",
    "                    break\n",
    "        \n",
    "        # Timing End\n",
    "        if device.type == 'cuda': torch.cuda.synchronize()\n",
    "        time_end_speculative = time.perf_counter()\n",
    "\n",
    "        duration_speculative = time_end_speculative - time_start_speculative\n",
    "        text_output_speculative = tokenizer.decode(current_context_ids_spec[0], skip_special_tokens=True)\n",
    "        \n",
    "        # TODO: Calculate average accepted tokens per verification step for this K_val run\n",
    "        # This is the sum of tokens accepted from draft (num_matched) + the one corrective/bonus from target, divided by verification calls\n",
    "        # A simpler proxy: (num_total_new_tokens_generated / count_target_model_passes_spec)\n",
    "        avg_accepted_this_K = None # Replace with actual calculation\n",
    "\n",
    "        all_experiment_results.append({\n",
    "            \"K\": K_val,\n",
    "            \"Time (s)\": duration_speculative,\n",
    "            \"Target Passes\": count_target_model_passes_spec,\n",
    "            \"Avg Accepted Tokens per Verification\": avg_accepted_this_K,\n",
    "            \"Output Text Sample\": text_output_speculative[:150] + \"...\"\n",
    "        })\n",
    "        print(f\"K={K_val}: Time={duration_speculative:.4f}s, Target Passes={count_target_model_passes_spec}, Avg Accepted={avg_accepted_this_K:.2f}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipping speculative decoding experiment as models/tokenizer were not properly loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Display Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec_results = pd.DataFrame(speculative_decoding_results_list)\n",
    "print(\"\\n\\n--- Speculative Decoding Experiment Results Summary ---\")\n",
    "print(df_spec_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Discussion\n",
    "\n",
    "Based on the 'Speculative Decoding Experiment Results Summary' table:\n",
    "\n",
    "1.  **Impact of `K` on Target Model Passes:**\n",
    "    *   TODO: Analyze how varying `K` affected the total number of forward passes made by the `gpt2-medium` (Target Model) compared to the baseline.\n",
    "\n",
    "2.  **Impact of `K` on Wall-Clock Time:**\n",
    "    *   TODO: Discuss the effect of `K` on the overall wall-clock generation time. Was there an apparent optimal value of `K` for this `gpt2-medium`/`gpt2` pairing in your setup? Explain why time might increase or decrease.\n",
    "\n",
    "3.  **Impact of `K` on Average Accepted Tokens:**\n",
    "    *   TODO: Explain how the average number of tokens accepted per target model verification step changed as `K` varied. What does this metric tell you about the efficiency of the speculation?\n",
    "\n",
    "4.  **Trade-offs of `K`:**\n",
    "    *   TODO: Conclude by summarizing the trade-offs involved in selecting a small versus a large value for `K`. Consider factors like draft model accuracy, overhead of draft generation, and the probability of successful verification.\n",
    "\n",
    "5.  **Comparison to Baseline:**\n",
    "    *   TODO: How did the best speculative decoding configuration compare to the baseline autoregressive generation in terms of target model passes and wall-clock time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
