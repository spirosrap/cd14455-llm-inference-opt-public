{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3, Exercise 2: Strategic Pruning on Llama-3.2-1B - Impact of Method and Target\n",
    "\n",
    "**Goal:**\n",
    "The objective of this exercise is to explore the nuanced impact of different pruning strategies when applied to a large and powerful language model like Llama-3.2-1B. You will investigate how the choice of pruning *method* (e.g., magnitude-based vs. random) and pruning *target* (e.g., MLP layers vs. attention layers) affects the model's output quality and inference speed at similar sparsity levels, even before any fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import copy # For creating fresh model copies\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "MODEL_NAME = \"/voc/shared/models/llama/Llama-3.2-1B\"\n",
    "\n",
    "PRUNING_AMOUNT = 0.3 # Target ~30% sparsity in selected layers\n",
    "NUM_LAYERS_TO_TARGET_EXAMPLE = 2 # Target first N layers for simplicity in this example\n",
    "\n",
    "PROMPT_TEXT = \"The future of AI is\"\n",
    "MAX_NEW_TOKENS_PRUNING = 30 \n",
    "NUM_SPEED_RUNS = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "print(f\"Using device: {device}, Model dtype: {model_dtype}\")\n",
    "\n",
    "# ### TODO: Define Llama-3.2-1B Layer Names\n",
    "# Based on inspecting the model architecture (print(model) after loading once).\n",
    "LLAMA_MLP_GATE_PROJ_TARGETS = [f\"model.layers.{i}.mlp.gate_proj\" for i in range(NUM_LAYERS_TO_TARGET_EXAMPLE)]\n",
    "LLAMA_MLP_UP_PROJ_TARGETS = [____ for i in range(NUM_LAYERS_TO_TARGET_EXAMPLE)]\n",
    "LLAMA_MLP_DOWN_PROJ_TARGETS = [____ for i in range(NUM_LAYERS_TO_TARGET_EXAMPLE)]\n",
    "\n",
    "LLAMA_ATTN_Q_PROJ_TARGETS = [____ for i in range(NUM_LAYERS_TO_TARGET_EXAMPLE)]\n",
    "LLAMA_ATTN_K_PROJ_TARGETS = [____ for i in range(NUM_LAYERS_TO_TARGET_EXAMPLE)]\n",
    "LLAMA_ATTN_V_PROJ_TARGETS = [____ for i in range(NUM_LAYERS_TO_TARGET_EXAMPLE)]\n",
    "LLAMA_ATTN_O_PROJ_TARGETS = [____ for i in range(NUM_LAYERS_TO_TARGET_EXAMPLE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_by_name(model, module_name_str):\n",
    "    \"\"\"Gets a module from a model using its string name.\"\"\"\n",
    "    ### TODO: Implement this helper function\n",
    "    # Split module_name_str by '.' and recursively use getattr.\n",
    "    # Example: model.layers[0].mlp.gate_proj -> model.layers.0.mlp.gate_proj for getattr\n",
    "    # Be careful with list indexing if layers are in a ModuleList\n",
    "    names = module_name_str.split('.')\n",
    "    module = model\n",
    "    for name_part in names:\n",
    "            module = None # TODO: use getattr to fetch the relevant module part\n",
    "    return module\n",
    "\n",
    "def calculate_sparsity(module):\n",
    "    \"\"\"Calculates sparsity of a module's weight if it exists.\"\"\"\n",
    "    if hasattr(module, 'weight') and module.weight is not None:\n",
    "        fraction = None # TODO: fraction of number of weights that are zero over total number of weights\n",
    "        return fraction\n",
    "    return 0.0\n",
    "\n",
    "def apply_pruning_to_layers(model, layer_names_list, amount, method='l1_unstructured'):\n",
    "    \"\"\"Applies global unstructured pruning to a list of specified layers.\"\"\"\n",
    "    parameters_to_prune_tuples = []\n",
    "    valid_layer_names_pruned = []\n",
    "    for name_str in layer_names_list:\n",
    "        try:\n",
    "            ### TODO: Get the module using get_module_by_name\n",
    "            module = None # Placeholder\n",
    "\n",
    "            if module and hasattr(module, 'weight') and module.weight is not None:\n",
    "                 parameters_to_prune_tuples.append((module, 'weight'))\n",
    "                 valid_layer_names_pruned.append(name_str)\n",
    "            else:\n",
    "                print(f\"Warning: Layer {name_str} has no 'weight' or weight is None. Skipping.\")\n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Layer {name_str} not found in model. Make sure names are correct. Skipping.\")\n",
    "\n",
    "    if not parameters_to_prune_tuples:\n",
    "        print(\"No valid parameters found to prune for the given layer names.\")\n",
    "        return [] # Return empty list if no layers were pruned\n",
    "\n",
    "    ### TODO: Apply global_unstructured pruning using torch.nn.utils.prune\n",
    "    # Use prune.L1Unstructured or prune.RandomUnstructured based on 'method'.\n",
    "    # Example: \n",
    "    # if method == 'l1_unstructured':\n",
    "    #     pruning_method_class = prune.L1Unstructured\n",
    "    # elif method == 'random_unstructured':\n",
    "    #     pruning_method_class = prune.RandomUnstructured\n",
    "    # else: raise ValueError(\"Unsupported pruning method\")\n",
    "    pass # Replace with pruning application\n",
    "    \n",
    "    ### TODO: Make pruning permanent for all pruned parameters\n",
    "    # Iterate through parameters_to_prune_tuples and use prune.remove().\n",
    "    pass # Replace with permanent pruning\n",
    "\n",
    "    print(f\"Applied {method} pruning (amount {amount*100:.1f}%) to {len(valid_layer_names_pruned)} layers.\")\n",
    "    return valid_layer_names_pruned # Return names of layers actually processed\n",
    "\n",
    "def measure_generation_speed_and_quality(model, tokenizer, prompt, max_new_tokens, num_runs):\n",
    "    total_time = 0\n",
    "    generated_text_sample = \"Error: Generation did not run.\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device if hasattr(model, 'device') and model.device is not None else device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_runs):\n",
    "            current_device = model.device if hasattr(model, 'device') and model.device is not None else device\n",
    "            if current_device.type == 'cuda':\n",
    "                torch.cuda.synchronize(current_device) \n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            ### TODO: Generate text using model.generate()\n",
    "            # Use do_sample=False for consistent speed testing.\n",
    "            # outputs = model.generate(...)\n",
    "            outputs = None # Placeholder\n",
    "\n",
    "            if current_device.type == 'cuda':\n",
    "                torch.cuda.synchronize(current_device)\n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            if i == 0 and outputs is not None:\n",
    "                generated_text_sample = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if outputs is not None:\n",
    "                 total_time += (end_time - start_time)\n",
    "            else:\n",
    "                total_time = float('inf') # Indicate error\n",
    "                break\n",
    "\n",
    "    avg_time = total_time / num_runs if num_runs > 0 and total_time != float('inf') else float('nan')\n",
    "    return avg_time, generated_text_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Original Model and Tokenizer (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary_list = []\n",
    "original_model = None\n",
    "tokenizer = None\n",
    "\n",
    "print(f\"Loading original model: {MODEL_NAME}...\")\n",
    "try:\n",
    "    ### TODO: Load the Llama-3.2-1B model using AutoModelForCausalLM\n",
    "    ### TODO: Load the tokenizer for the model\n",
    "    original_model, tokenizer = None, None\n",
    "    \n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    if original_model is None or tokenizer is None:\n",
    "        raise ValueError(\"Original model or tokenizer not loaded. Check TODOs.\")\n",
    "    \n",
    "    print(\"Original model and tokenizer loaded.\")\n",
    "    # You can print model architecture here to verify layer names if needed: print(original_model)\n",
    "\n",
    "    ### TODO: Measure generation speed and get output for the original model using measure_generation_speed_and_quality\n",
    "    original_avg_time, original_output = float('nan'), \"N/A (TODO)\"\n",
    "\n",
    "    results_summary_list.append({\n",
    "        \"Configuration\": \"Original\",\n",
    "        \"Avg Sparsity Targeted (%)\": 0,\n",
    "        \"Avg Inference Time (s)\": f\"{original_avg_time:.4f}\",\n",
    "        \"Speed-up vs Original\": \"1.00x\",\n",
    "        \"Output Sample\": original_output\n",
    "    })\n",
    "    print(f\"Original Model: Avg Time={original_avg_time:.4f}s, Output='{original_output[:100]}...'\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR loading original model {MODEL_NAME}: {e}\")\n",
    "    print(\"Ensure correct MODEL_NAME, HF_TOKEN (if needed), and sufficient resources.\")\n",
    "    original_model = None # Prevent further execution if base model fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pruning Experiments\n",
    "\n",
    "Helper function to run a single pruning experiment configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_pruning_experiment(config_name, base_model_to_copy, target_layer_names, pruning_amt, prune_method):\n",
    "    if base_model_to_copy is None: # Guard if original model loading failed\n",
    "        print(f\"Skipping {config_name} as base model is not available.\")\n",
    "        results_summary_list.append({\"Configuration\": config_name, \"Error\": \"Base model not loaded\"})\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n--- Running: {config_name} (Method: {prune_method}) ---\")\n",
    "    pruned_model_instance = None\n",
    "    try:\n",
    "        ### TODO: Create a deepcopy of the base_model_to_copy for this experiment\n",
    "        # This is to ensure each pruning experiment starts from a fresh, unpruned state.\n",
    "        pass # Replace with model copying\n",
    "\n",
    "        if pruned_model_instance is None:\n",
    "            raise ValueError(\"Pruned model instance not created.\")\n",
    "\n",
    "        ### TODO: Apply pruning to the target_layer_names using apply_pruning_to_layers helper\n",
    "        processed_layers = [] # Placeholder\n",
    "\n",
    "        avg_sparsity_achieved = 0.0\n",
    "        if processed_layers: # Only calculate if some layers were actually pruned\n",
    "            ### TODO: Calculate the average sparsity across the processed_layers. Use get_module_by_name and calculate_sparsity\n",
    "            ###       helper functions\n",
    "            pass\n",
    "        \n",
    "        print(f\"Average sparsity in targeted layers: {avg_sparsity_achieved:.2f}%\")\n",
    "\n",
    "        ### TODO: Measure generation speed and quality for the pruned_model_instance using measure_generation_speed_and_quality \n",
    "        avg_time, output = float('nan'), \"N/A (TODO)\"\n",
    "        \n",
    "        speed_up_val = original_avg_time / avg_time if avg_time > 0 and not pd.isna(original_avg_time) else float('nan')\n",
    "\n",
    "        results_summary_list.append({\n",
    "            \"Configuration\": config_name,\n",
    "            \"Avg Sparsity Targeted (%)\": f\"{avg_sparsity_achieved:.2f}\",\n",
    "            \"Avg Inference Time (s)\": f\"{avg_time:.4f}\",\n",
    "            \"Speed-up vs Original\": f\"{speed_up_val:.2f}x\",\n",
    "            \"Output Sample\": output\n",
    "        })\n",
    "        print(f\"{config_name}: Avg Time={avg_time:.4f}s, Speed-up={speed_up_val:.2f}x, Output='{output[:100]}...'\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {config_name}: {e}\")\n",
    "        results_summary_list.append({\"Configuration\": config_name, \"Error\": str(e)})\n",
    "    finally:\n",
    "        del pruned_model_instance # Important to free memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Part A: Pruning Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_model is not None: # Proceed only if base model loaded successfully\n",
    "    print(\"\\n--- Part A: Pruning Method Comparison (Targeting MLP Gate Projections) ---\")\n",
    "    part_a_targets = LLAMA_MLP_GATE_PROJ_TARGETS # Example, choose a consistent set\n",
    "\n",
    "    ### TODO: Run experiment for Magnitude Pruning on part_a_targets using run_single_pruning_experiment function\n",
    "\n",
    "    ### TODO: Run experiment for Random Pruning on part_a_targets\n",
    "else:\n",
    "    print(\"Skipping Part A due to original model loading failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Part B: Pruning Target Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_model is not None: # Proceed only if base model loaded successfully\n",
    "    print(\"\\n\\n--- Part B: Pruning Target Comparison (All using Magnitude Pruning) ---\")\n",
    "    \n",
    "    ### TODO: Define the list of ALL MLP layers to target for Part B and run Magnitude Pruning experiment\n",
    "    mlp_layers_for_b = None # TODO: combine various MLP layers. E.g. LLAMA_MLP_GATE_PROJ_TARGETS + LLAMA_MLP_UP_PROJ_TARGETS + etc\n",
    "\n",
    "    ### TODO: Define the list of ALL Attention projection layers to target for Part B and run Magnitude Pruning experiment\n",
    "    attn_layers_for_b = None # TODO: combine various ATTN layers\n",
    "else:\n",
    "    print(\"Skipping Part B due to original model loading failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Display Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_results = pd.DataFrame(results_summary_list)\n",
    "print(\"\\n\\n--- Overall Pruning Experiment Results Summary ---\")\n",
    "print(df_final_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Discussion\n",
    "\n",
    "Based on the 'Overall Pruning Experiment Results Summary' table:\n",
    "\n",
    "1.  **Part A - Pruning Method:**\n",
    "    *   **TODO**: Compare the output quality and inference speed-up between magnitude pruning and random pruning for Llama-3.2-1B at similar sparsity levels. Did one perform better? Why might that be?\n",
    "\n",
    "2.  **Part B - Pruning Target:**\n",
    "    *   **TODO**: Compare the impact of pruning MLP layers versus attention projection layers on Llama-3.2-1B's output quality and speed. Which type of layer seemed more critical or sensitive to pruning before fine-tuning? Offer potential reasons.\n",
    "\n",
    "3.  **Challenges with Llama-3.2-1B:**\n",
    "    *   **TODO**: Reflect on any practical challenges encountered (e.g., memory usage, computation time, verifying layer names) while applying these pruning techniques to a model of Llama-3.2-1B's scale.\n",
    "\n",
    "4.  **Inference Speed-up:**\n",
    "    *   **TODO**: Was there a noticeable inference speed-up from unstructured pruning in your experiments? Discuss why or why not, considering the nature of unstructured pruning and standard hardware.\n",
    "\n",
    "5.  **Necessity of Fine-tuning:**\n",
    "    *   **TODO**: Emphasize the likely necessity of fine-tuning to make such pruned models practically useful, based on the quality of outputs you observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
