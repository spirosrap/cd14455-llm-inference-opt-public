{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implementing and Tuning Pipeline Parallelism\n",
    "\n",
    "Welcome to the exercise! In the demo, we saw how DeepSpeed can easily partition a simple `nn.Sequential` model. Now, it's your turn to apply these concepts to a more realistic scenario and analyze the performance trade-offs.\n",
    "\n",
    "### Your Goal\n",
    "Your mission is to:\n",
    "1.  **Adapt** a non-trivial, realistic PyTorch model to make it compatible with DeepSpeed's automatic pipeline partitioning.\n",
    "2.  **Implement** the baseline measurement and the DeepSpeed initialization call.\n",
    "3.  **Design and run a series of experiments** to analyze the performance impact of `micro_batch_size`.\n",
    "4.  **Analyze the results** to understand the concept of the \"pipeline bubble\" and its effect on throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries and check our environment. This exercise requires at least two GPUs to properly evaluate pipeline parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepspeed transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "import json\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"DeepSpeed version: {deepspeed.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.device_count() < 2:\n",
    "        print(\"\\n!! WARNING: This exercise requires at least 2 GPUs. Performance comparison will not be meaningful otherwise. !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The \"Realistic\" Model for Our Exercise\n",
    "\n",
    "Below is the model we will be working with. Notice that the `transformer_blocks` are stored in an `nn.ModuleList`. This is a common PyTorch pattern, but it poses a challenge for DeepSpeed's `\"uniform\"` partitioner, which often treats the entire `ModuleList` as a single, indivisible block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a mock transformer block for demonstration purposes.\n",
    "class MockTransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.relu(self.layer1(x)))\n",
    "\n",
    "class RealisticModel(nn.Module):\n",
    "    def __init__(self, hidden_size=2048, num_layers=30, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [MockTransformerBlock(hidden_size) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.output_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = self.output_head(x)\n",
    "        return x\n",
    "\n",
    "print(\"Model structure defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your Task: Create the Analysis Script\n",
    "\n",
    "Now, you will create the Python script that DeepSpeed will launch. This involves three key implementation tasks, marked with `TODO` comments inside the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile exercise_starter.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# --- Model Definition ---\n",
    "class MockTransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.relu(self.layer1(x)))\n",
    "\n",
    "class RealisticModel(nn.Module):\n",
    "    def __init__(self, hidden_size=2048, num_layers=30, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # ## TODO: STUDENT TASK 1: Fix the Model Architecture ##\n",
    "        # DeepSpeed's \"uniform\" partitioner treats `nn.ModuleList` as a single block.\n",
    "        # To allow DeepSpeed to split the transformer blocks, you must use a different container.\n",
    "        # HINT: What container presents a sequence of layers that can be called like a single function?\n",
    "        # Replace the line below with the correct implementation.\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [MockTransformerBlock(hidden_size) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.output_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # HINT: If you changed the container for `transformer_blocks`,\n",
    "        # you may need to change how you call it here.\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = self.output_head(x)\n",
    "        return x\n",
    "\n",
    "# --- Helper Function for Performance Measurement ---\n",
    "def measure_throughput(model, dummy_input, iterations):\n",
    "    # Warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_samples = dummy_input.size(0) * iterations\n",
    "    duration = end_time - start_time\n",
    "    throughput = total_samples / duration\n",
    "    return throughput\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
    "    parser = deepspeed.add_config_arguments(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # --- Setup ---\n",
    "    global_batch_size = 64\n",
    "    hidden_size = 2048\n",
    "    vocab_size = 1000\n",
    "    iterations = 20\n",
    "    is_rank_0 = args.local_rank <= 0\n",
    "\n",
    "    # --- Baseline Measurement (Single GPU) ---\n",
    "    if is_rank_0:\n",
    "        print(\"\\n--- Measuring Baseline Performance (Single GPU) ---\", flush=True)\n",
    "        # ## TODO: STUDENT TASK 2: Implement the Baseline Measurement ##\n",
    "        # 1. Instantiate the `RealisticModel`.\n",
    "        # 2. Move the model to the first GPU ('cuda:0').\n",
    "        # 3. Create a dummy input tensor on the same device.\n",
    "        # 4. Call `measure_throughput` to get the baseline performance.\n",
    "        # 5. Print the result.\n",
    "        \n",
    "        # baseline_model = ...\n",
    "        # dummy_input = ...\n",
    "        # baseline_throughput = ...\n",
    "        # print(f\"Baseline Throughput: {baseline_throughput:.2f} samples/sec\", flush=True)\n",
    "        print(\"Baseline measurement not yet implemented.\") # Remove this line\n",
    "        \n",
    "        print(\"--------------------------------------------------\\n\", flush=True)\n",
    "\n",
    "    if torch.distributed.is_initialized():\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    # --- DeepSpeed Pipeline Parallelism ---\n",
    "    print(f\"\\n--- Rank {args.local_rank}: Setting up DeepSpeed Pipeline ---\", flush=True)\n",
    "\n",
    "    ds_model = RealisticModel()\n",
    "    \n",
    "    # ## TODO: STUDENT TASK 3: Initialize the DeepSpeed Engine ##\n",
    "    # Call `deepspeed.initialize` with the correct arguments to create the `model_engine`.\n",
    "    # The engine will wrap your model and handle the pipeline parallelism.\n",
    "    \n",
    "    # model_engine, _, _, _ = deepspeed.initialize(...)\n",
    "    print(\"DeepSpeed engine not yet initialized.\") # Remove this line\n",
    "    model_engine = ds_model # Replace this line with the initializer call\n",
    "    model_engine.device = 'cpu' # Replace this line\n",
    "    model_engine.is_last_stage = lambda: True # Replace this line\n",
    "    \n",
    "    # This part will fail until the model_engine is correctly initialized.\n",
    "    dummy_input_ds = torch.randint(0, vocab_size, (global_batch_size, 128), device=model_engine.device)\n",
    "\n",
    "    pipelined_throughput = measure_throughput(model_engine, dummy_input_ds, iterations)\n",
    "\n",
    "    if model_engine.is_last_stage():\n",
    "        print(f\"\\n--- Results on Last Stage (Rank {args.local_rank}) ---\", flush=True)\n",
    "        print(f\"Pipelined Throughput: {pipelined_throughput:.2f} samples/sec\", flush=True)\n",
    "        print(\"------------------------------------------\\n\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your Task: Set Up the Experiment Configurations\n",
    "\n",
    "A key part of performance tuning is designing the experiments. Here, you will define the different `micro_batch_size` configurations you want to test. This will require you to think about the relationship between the global batch size, the number of GPUs (pipeline stages), and the micro-batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# The global batch size is 64, and we will use 2 GPUs (2 stages).\n",
    "GLOBAL_BATCH_SIZE = 64\n",
    "NUM_STAGES = 2\n",
    "\n",
    "# ## TODO: STUDENT TASK 4: Define the Experiment Configurations ##\n",
    "# Populate the `configs` dictionary below.\n",
    "# Create 4 configurations to test with different micro_batch_size values:\n",
    "# 1. A very small size (e.g., 4)\n",
    "# 2. A medium size (e.g., 8)\n",
    "# 3. A larger size (e.g., 16)\n",
    "# 4. A size equal to the per-stage batch size (global_batch_size / num_stages)\n",
    "configs = {\n",
    "    # e.g., \"ds_config_mbs_4.json\": { \"micro_batch_size\": 4, \"stages\": NUM_STAGES },\n",
    "}\n",
    "\n",
    "# --- This part is complete --- #\n",
    "# Base config template\n",
    "base_config = {\n",
    "    \"train_batch_size\": GLOBAL_BATCH_SIZE,\n",
    "    \"optimizer\": { \"type\": \"Adam\", \"params\": { \"lr\": 0.001 } },\n",
    "    \"fp16\": { \"enabled\": True }\n",
    "}\n",
    "\n",
    "for filename, pipeline_config in configs.items():\n",
    "    full_config = base_config.copy()\n",
    "    full_config[\"pipeline\"] = pipeline_config\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(full_config, f, indent=2)\n",
    "    print(f\"Created config file: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Experiments\n",
    "\n",
    "Once you have completed all the `TODO`s above, you can run this cell to execute your experiments. It will loop through the configuration files you created and launch the DeepSpeed job for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not configs:\n",
    "    print(\"Please complete 'STUDENT TASK 4' in the cell above before running the experiments.\")\n",
    "else:\n",
    "    for config_file in configs.keys():\n",
    "        print(f\"\\n\\n{'='*60}\")\n",
    "        print(f\"RUNNING EXPERIMENT WITH CONFIG: {config_file}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        # We reduce log verbosity for a cleaner output\n",
    "        !DS_LOG_LEVEL=ERROR PYTHONWARNINGS=ignore deepspeed --num_gpus {NUM_STAGES} exercise_starter.py --deepspeed_config {config_file}\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze the Results\n",
    "\n",
    "Your final task is to analyze the results from your experiments. Fill out the table below with the throughput numbers you observed from the output of the previous cell. \n",
    "\n",
    "| Micro Batch Size (`mbs`) | Pipelined Throughput (samples/sec) | Your Explanation of the Performance Trend |\n",
    "|:---:|:---:|:---|\n",
    "| 4   | **Fill this in** | *Explain why the throughput might be lower here.* |\n",
    "| 8   | **Fill this in** | *Explain why the throughput is likely increasing.* |\n",
    "| 16  | **Fill this in** | *Explain why this might be near the optimal performance.* |\n",
    "| 32  | **Fill this in** | *Explain why the throughput might decrease again here.* |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
