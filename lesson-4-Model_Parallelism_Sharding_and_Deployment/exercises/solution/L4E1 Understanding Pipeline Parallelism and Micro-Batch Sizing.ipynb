{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Understanding Pipeline Parallelism and Micro-Batch Sizing\n",
    "\n",
    "The purpose of this exercise is to demonstrate how to correctly configure a model for pipeline parallelism and to explore the performance impact of the `micro_batch_size` parameter.\n",
    "\n",
    "We will:\n",
    "1.  **Define a model** and a script to run it, highlighting a common pitfall (`nn.ModuleList` vs. `nn.Sequential`) that prevents automatic partitioning.\n",
    "2.  **Create multiple DeepSpeed configs** to test different `micro_batch_size` values.\n",
    "3.  **Run experiments** using the `deepspeed` launcher.\n",
    "4.  **Analyze the results** to understand the trade-offs of micro-batch sizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run this cell if you are running in SageMaker Notebook\n",
    "# !pip install deepspeed transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Model and Training Script\n",
    "\n",
    "Next, we define our model and the main training script. \n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "The `RealisticModel` class defines a simple transformer-like model. A key aspect for DeepSpeed's pipeline parallelism is how the model layers are structured. \n",
    "\n",
    "DeepSpeed's automatic partitioning works best with `nn.Sequential` containers, as it can easily divide the layers into stages. \n",
    "\n",
    "In this exercise, the student's task is to replace `nn.ModuleList` with `nn.Sequential` to enable efficient pipeline parallelism.\n",
    "\n",
    "`nn.ModuleList` is a list-like container for `nn.Module`'s, but it doesn't have a `forward` method and thus the layers cannot be executed sequentially by simply calling the container. In contrast, `nn.Sequential` wraps a sequence of layers and provides a `forward` method that applies each layer in order, which is what DeepSpeed's pipeline parallelism leverages for automatic partitioning.\n",
    "\n",
    "#### Throughput Measurement\n",
    "\n",
    "The `measure_throughput` function is a helper to benchmark the performance of our model. It runs several iterations of a forward pass with dummy data and calculates the number of samples processed per second.\n",
    "\n",
    "#### Main Execution Logic\n",
    "\n",
    "The `main` function handles the overall execution. It first measures the baseline performance on a single GPU. \n",
    "\n",
    "Then, it initializes DeepSpeed, which automatically partitions the model across the available GPUs based on the provided configuration. \n",
    "\n",
    "Finally, it measures the throughput with pipeline parallelism enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5r_5OP749Is9",
    "outputId": "cf1fbedb-1f04-43b2-f8a3-7d101a16985e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting exercise_solution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercise_solution.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# --- Model Definition ---\n",
    "class MockTransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.relu(self.layer1(x)))\n",
    "\n",
    "class RealisticModel(nn.Module):\n",
    "    def __init__(self, hidden_size=2048, num_layers=30, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # By switching to `nn.Sequential`, we create a single, sequential block that DeepSpeed's\n",
    "        # \"uniform\" partitioner can easily divide across multiple GPUs.\n",
    "        transformer_layer_list = [MockTransformerBlock(hidden_size) for _ in range(num_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_layer_list)\n",
    "\n",
    "        self.output_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # If using nn.Sequential, you call it directly\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.output_head(x)\n",
    "        return x\n",
    "\n",
    "# --- Helper Function for Performance Measurement ---\n",
    "def measure_throughput(model, dummy_input, iterations):\n",
    "    # Warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_samples = dummy_input.size(0) * iterations\n",
    "    duration = end_time - start_time\n",
    "    throughput = total_samples / duration\n",
    "    return throughput\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
    "    parser = deepspeed.add_config_arguments(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # --- Setup ---\n",
    "    global_batch_size = 64\n",
    "    hidden_size = 2048\n",
    "    vocab_size = 1000\n",
    "    iterations = 20\n",
    "    is_rank_0 = args.local_rank <= 0 # Rank -1 for non-distributed, 0 for distributed\n",
    "\n",
    "    # --- Baseline Measurement (Single GPU) ---\n",
    "    if is_rank_0:\n",
    "        print(\"\\n--- Measuring Baseline Performance (Single GPU) ---\", flush=True)\n",
    "        try:\n",
    "            baseline_model = RealisticModel().to('cuda:0')\n",
    "            dummy_input = torch.randint(0, vocab_size, (global_batch_size, 128), device='cuda:0')\n",
    "            baseline_throughput = measure_throughput(baseline_model, dummy_input, iterations)\n",
    "            print(f\"Baseline Throughput: {baseline_throughput:.2f} samples/sec\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not run baseline on single GPU, likely out of memory: {e}\", flush=True)\n",
    "        print(\"--------------------------------------------------\\n\", flush=True)\n",
    "\n",
    "    if torch.distributed.is_initialized():\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    # --- DeepSpeed Pipeline Parallelism ---\n",
    "    print(f\"\\n--- Rank {args.local_rank}: Setting up DeepSpeed Pipeline ---\", flush=True)\n",
    "\n",
    "    ds_model = RealisticModel()\n",
    "    model_engine, _, _, _ = deepspeed.initialize(args=args, model=ds_model, model_parameters=ds_model.parameters())\n",
    "\n",
    "    # Input needs to be integer indices for the embedding layer\n",
    "    dummy_input_ds = torch.randint(0, vocab_size, (global_batch_size, 128), device=model_engine.device)\n",
    "\n",
    "    pipelined_throughput = measure_throughput(model_engine, dummy_input_ds, iterations)\n",
    "\n",
    "    if model_engine.is_last_stage():\n",
    "        print(f\"\\n--- Results on Last Stage (Rank {args.local_rank}) ---\", flush=True)\n",
    "        print(f\"Pipelined Throughput: {pipelined_throughput:.2f} samples/sec\", flush=True)\n",
    "        print(\"------------------------------------------\\n\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create DeepSpeed Configuration Files\n",
    "\n",
    "We now create several JSON configuration files to experiment with different `micro_batch_size` values. \n",
    "\n",
    "The `micro_batch_size` is a crucial parameter in pipeline parallelism, as it determines the size of the smaller data chunks that are fed through the pipeline.\n",
    "\n",
    "The configurations we will test are:\n",
    "*   **`ds_config_mbs_4.json`**: A very small micro-batch size, which is expected to result in high pipeline bubble overhead.\n",
    "*   **`ds_config_mbs_8.json`**: A medium micro-batch size.\n",
    "*   **`ds_config_mbs_16.json`**: A potentially optimal micro-batch size.\n",
    "*   **`ds_config_mbs_32.json`**: A large micro-batch size, which may lead to reduced parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OZyLX3WBhA3",
    "outputId": "b864b5ce-8e68-4cd6-ffcd-5d81ff5ec372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created config file: ds_config_mbs_4.json\n",
      "Created config file: ds_config_mbs_8.json\n",
      "Created config file: ds_config_mbs_16.json\n",
      "Created config file: ds_config_mbs_32.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# We will create several config files to test the effect of `micro_batch_size`.\n",
    "# The global batch size is 64, and we have 4 GPUs, so each stage gets a batch of 32.\n",
    "configs = {\n",
    "    # Case 1: Very small micro-batch size. Expect high pipeline bubble overhead.\n",
    "    \"ds_config_mbs_4.json\": { \"micro_batch_size\": 4, \"stages\": 2 },\n",
    "\n",
    "    # Case 2: A medium micro-batch size.\n",
    "    \"ds_config_mbs_8.json\": { \"micro_batch_size\": 8, \"stages\": 2 },\n",
    "\n",
    "    # Case 3: Optimal micro-batch size is often global_batch_size / (2 * num_stages)\n",
    "    \"ds_config_mbs_16.json\": { \"micro_batch_size\": 16, \"stages\": 2 },\n",
    "\n",
    "    # Case 4: Large micro-batch size. Less pipelining, closer to sequential execution.\n",
    "    \"ds_config_mbs_32.json\": { \"micro_batch_size\": 32, \"stages\": 2 }\n",
    "}\n",
    "\n",
    "# Base config template\n",
    "base_config = {\n",
    "    \"train_batch_size\": 64, # This is the *global* batch size across all GPUs\n",
    "    \"optimizer\": { \"type\": \"SGD\", \"params\": { \"lr\": 0.001 } },\n",
    "    \"fp16\": { \"enabled\": True }\n",
    "}\n",
    "\n",
    "for filename, pipeline_config in configs.items():\n",
    "    full_config = base_config.copy()\n",
    "    full_config[\"pipeline\"] = pipeline_config\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(full_config, f, indent=2)\n",
    "    print(f\"Created config file: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Experiments\n",
    "\n",
    "Now we run our training script with each of the configurations created in the previous step. \n",
    "\n",
    "The `deepspeed` launcher will execute the script on 4 GPUs, and we will collect the performance data for each `micro_batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1spmu8RbBhG-",
    "outputId": "c4d0eb7d-e3d8-44f6-8cf7-678eef7a26ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "RUNNING EXPERIMENT WITH CONFIG: ds_config_mbs_4.json\n",
      "============================================================\n",
      "\n",
      "--- Measuring Baseline Performance (Single GPU) ---\n",
      "\n",
      "--- Rank 3: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Rank 1: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Rank 2: Setting up DeepSpeed Pipeline ---\n",
      "Baseline Throughput: 79.40 samples/sec\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Rank 0: Setting up DeepSpeed Pipeline ---\n",
      "--- Results on Last Stage (Rank 3) ---\n",
      "Pipelined Throughput: 453.78 samples/sec\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "RUNNING EXPERIMENT WITH CONFIG: ds_config_mbs_8.json\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Rank 3: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Rank 1: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Measuring Baseline Performance (Single GPU) ---\n",
      "\n",
      "--- Rank 2: Setting up DeepSpeed Pipeline ---\n",
      "Baseline Throughput: 78.89 samples/sec\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Rank 0: Setting up DeepSpeed Pipeline ---\n",
      "--- Results on Last Stage (Rank 3) ---\n",
      "Pipelined Throughput: 460.76 samples/sec\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "RUNNING EXPERIMENT WITH CONFIG: ds_config_mbs_16.json\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Rank 3: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Rank 1: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Rank 2: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Measuring Baseline Performance (Single GPU) ---\n",
      "Baseline Throughput: 78.58 samples/sec\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Rank 0: Setting up DeepSpeed Pipeline ---\n",
      "--- Results on Last Stage (Rank 3) ---\n",
      "Pipelined Throughput: 464.51 samples/sec\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "RUNNING EXPERIMENT WITH CONFIG: ds_config_mbs_32.json\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Rank 1: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Rank 2: Setting up DeepSpeed Pipeline ---\n",
      "\n",
      "--- Measuring Baseline Performance (Single GPU) ---\n",
      "\n",
      "--- Rank 3: Setting up DeepSpeed Pipeline ---\n",
      "Baseline Throughput: 78.55 samples/sec\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Rank 0: Setting up DeepSpeed Pipeline ---\n",
      "--- Results on Last Stage (Rank 3) ---\n",
      "Pipelined Throughput: 461.25 samples/sec\n",
      "------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will now run the exercise script for each configuration to collect performance data.\n",
    "for config_file in configs.keys():\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"RUNNING EXPERIMENT WITH CONFIG: {config_file}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    !deepspeed --num_gpus 4 exercise_solution.py --deepspeed_config {config_file}\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze the Results\n",
    "\n",
    "The table below is filled in using the output from the previous cell. Your numbers may vary slightly but should show a similar trend.\n",
    "\n",
    "| Micro Batch Size (mbs) | Pipelined Throughput (samples/sec) | Explanation of Performance                                                                                                                                                                 |\n",
    "| :--------------------- | :--------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| 4                      | 453.78                             | **High Pipeline Bubble**: With very small micro-batches, the pipeline is almost always partially empty. The \"bubble\" (idle time) dominates, leading to lower throughput compared to the optimal case. |\n",
    "| 8                      | 460.76                             | **Bubble Decreasing**: As mbs increases, the pipeline stays full for longer periods, reducing idle time and increasing throughput.                                                          |\n",
    "| 16                     | 464.51                             | **Approaching Optimal**: This mbs likely represents a good balance. The pipeline bubble is small compared to computation time, and batches are small enough for effective interleaving.        |\n",
    "| 32                     | 461.25                             | **Low Parallelism/Large Bubble**: With `mbs` equal to `train_batch_size / num_stages`, each stage processes only one large micro-batch. The pipeline is mostly idle, resembling sequential execution. This can reduce throughput. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
