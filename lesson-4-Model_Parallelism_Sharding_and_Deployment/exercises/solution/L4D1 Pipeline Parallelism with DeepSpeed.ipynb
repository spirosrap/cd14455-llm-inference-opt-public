{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Pipeline Parallelism with DeepSpeed - The \"Happy Path\"\n",
    "\n",
    "The purpose of this demo is to provide a clear, simple, and successful \"first look\" at pipeline parallelism. We will demonstrate:\n",
    "1.  How a simple, sequential model can be **automatically partitioned** by DeepSpeed across multiple GPUs.\n",
    "2.  The basic **mechanics** of launching a DeepSpeed job using a configuration file.\n",
    "3.  The **outcome**: a single model running collaboratively on multiple devices.\n",
    "\n",
    "This demo represents the ideal, \"happy path\" scenario. The model we use is structured perfectly for DeepSpeed's automatic partitioning, so we expect it to work right out of the box.\n",
    "\n",
    "### How We'll Run This in a Notebook\n",
    "The `deepspeed` command is an external launcher. To make this work seamlessly in a notebook, we will:\n",
    "1.  **Programmatically create** a JSON configuration file.\n",
    "2.  **Write our Python logic to a script** using the `%%writefile` magic command.\n",
    "3.  **Execute the `deepspeed` launcher** on that script directly from the notebook using `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "This demo requires at least 4 GPUs to see pipeline parallelism in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5r_5OP749Is9",
    "outputId": "818c9200-c76d-4559-c7f6-5a40d594390b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 04:08:19,216] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 04:08:33,980] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "PyTorch version: 2.2.2\n",
      "DeepSpeed version: 0.17.2\n",
      "CUDA is available: True\n",
      "Number of GPUs available: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"DeepSpeed version: {deepspeed.__version__}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.device_count() < 4:\n",
    "        print(\"!! WARNING: This demo is designed for 4 GPUs. It may not run correctly. !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a \"Pipeline-Friendly\" Model\n",
    "\n",
    "Next, we'll create a model that is perfectly suited for pipeline parallelism. It is a simple `nn.Sequential` model.\n",
    "\n",
    "DeepSpeed's `\"uniform\"` partition method can easily inspect this sequence of layers and divide it into `N` even chunks for `N` GPUs. This makes the setup extremely simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2m9XZ9mu9Ko5",
    "outputId": "202bd6bc-28cd-4526-d25f-3a01a16036f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure for the Demo:\n",
      "SequentialDemoModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (15): ReLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "This nn.Sequential structure is ideal for DeepSpeed's automatic partitioning.\n"
     ]
    }
   ],
   "source": [
    "# We will write this model definition directly into our script in the next step.\n",
    "# Here is the code for inspection:\n",
    "\n",
    "class SequentialDemoModel(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_layers=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # A simple list of layers\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Wrapping layers in `nn.Sequential` is the key to making partitioning easy\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Let's inspect the model structure\n",
    "model_for_inspection = SequentialDemoModel()\n",
    "print(\"Model Structure for the Demo:\")\n",
    "print(model_for_inspection)\n",
    "print(\"\\nThis nn.Sequential structure is ideal for DeepSpeed's automatic partitioning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the DeepSpeed Configuration\n",
    "\n",
    "This JSON configuration file is the control panel for DeepSpeed. It tells the launcher how to set up our job. The most important part for this demo is the `\"pipeline\"` section, where we define how the model should be split.\n",
    "\n",
    "- `stages`: The number of pipeline stages to split the model into. This should match the number of GPUs we use.\n",
    "- `partition_method`: How to partition the layers. `\"uniform\"` splits the layer sequence as evenly as possible.\n",
    "- `micro_batch_size`: The size of the smaller data chunks that flow through the pipeline to keep all GPUs busy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaOjlMha-QH9",
    "outputId": "aa5dbfc3-b61d-4933-d8d6-d1696f687f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeed configuration file 'ds_config_demo.json' created.\n",
      "\n",
      "--- Demo Config Contents ---\n",
      "{\n",
      "  \"train_batch_size\": 16,\n",
      "  \"optimizer\": {\n",
      "    \"type\": \"SGD\",\n",
      "    \"params\": {\n",
      "      \"lr\": 0.001\n",
      "    }\n",
      "  },\n",
      "  \"pipeline\": {\n",
      "    \"stages\": 2,\n",
      "    \"partition_method\": \"uniform\",\n",
      "    \"micro_batch_size\": 8\n",
      "  },\n",
      "  \"comms_logger\": {\n",
      "    \"enabled\": false,\n",
      "    \"verbose\": false,\n",
      "    \"debug\": false\n",
      "  }\n",
      "}\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "ds_config_demo = {\n",
    "  \"train_batch_size\": 16, # Global batch size\n",
    "\n",
    "  # A dummy optimizer is required by the DeepSpeed initializer\n",
    "  \"optimizer\": { \"type\": \"SGD\", \"params\": { \"lr\": 0.001 } },\n",
    "\n",
    "  # Pipeline Parallelism Configuration\n",
    "  \"pipeline\": {\n",
    "    \"stages\": 2,              # We will split the model into 2 stages for our 2 GPUs\n",
    "    \"partition_method\": \"uniform\", # Tells DeepSpeed to split layers evenly. Perfect for nn.Sequential.\n",
    "    \"micro_batch_size\": 8     # We split our global batch size into smaller micro-batches\n",
    "  },\n",
    "\n",
    "  \"comms_logger\": {\n",
    "    \"enabled\": False,\n",
    "    \"verbose\": False,\n",
    "    \"debug\": False\n",
    "  }\n",
    "}\n",
    "\n",
    "# Write the configuration to a file\n",
    "config_filename_demo = 'ds_config_demo.json'\n",
    "with open(config_filename_demo, 'w') as f:\n",
    "    json.dump(ds_config_demo, f, indent=2)\n",
    "\n",
    "print(f\"DeepSpeed configuration file '{config_filename_demo}' created.\")\n",
    "print(\"\\n--- Demo Config Contents ---\")\n",
    "print(json.dumps(ds_config_demo, indent=2))\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write the Main Execution Script\n",
    "\n",
    "Now we'll package our logic into a Python script. The `deepspeed` launcher will execute this script on each GPU. \n",
    "\n",
    "Inside, the `deepspeed.initialize()` function is the key. \n",
    "\n",
    "It reads the configuration, performs the model partitioning, and returns a wrapped `model_engine` that handles all the complex distributed logic for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8O9-mxAJ-R3u",
    "outputId": "c55a8505-e755-4b16-91db-277a64f06a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo_pipeline_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_pipeline_script.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "import argparse\n",
    "\n",
    "# The model definition is included in the script\n",
    "class SequentialDemoModel(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_layers=8):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Standard DeepSpeed argument parsing\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local rank\")\n",
    "    parser = deepspeed.add_config_arguments(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 1. Instantiate the Model\n",
    "    # DeepSpeed will handle placing it on the correct device.\n",
    "    model = SequentialDemoModel()\n",
    "\n",
    "    # 2. Initialize with DeepSpeed\n",
    "    # This is the core function where the magic happens. DeepSpeed reads the\n",
    "    # config, partitions the model, and wraps it in a `model_engine`.\n",
    "    model_engine, _, _, _ = deepspeed.initialize(\n",
    "        args=args,\n",
    "        model=model,\n",
    "        model_parameters=model.parameters(),\n",
    "    )\n",
    "\n",
    "    # 3. Print device information to verify partitioning\n",
    "    # Each rank (GPU) will print which device its partition is on.\n",
    "    print(\n",
    "        f\"Rank {model_engine.local_rank}: My model partition is on device: {model_engine.device}\",\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    # 4. Run a forward pass\n",
    "    # We create input data and pass it to the engine. DeepSpeed handles\n",
    "    # passing the data through the pipeline stages automatically.\n",
    "    batch_size = 16\n",
    "    hidden_size = 1024\n",
    "    dummy_input = torch.randn(batch_size, hidden_size, device=model_engine.device)\n",
    "\n",
    "    output = model_engine(dummy_input)\n",
    "\n",
    "    # The final output is only available on the last stage of the pipeline.\n",
    "    # We can check this to confirm the run was successful.\n",
    "    if model_engine.is_last_stage():\n",
    "        print(f\"\\nRank {model_engine.local_rank} (Last Stage): Inference successful!\", flush=True)\n",
    "        print(f\"Final output shape: {output.shape}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch the DeepSpeed Job!\n",
    "\n",
    "It's time to run our demo. We will execute the `deepspeed` launcher, telling it to use 4 GPUs. \n",
    "\n",
    "We'll also add some environment variables (`DS_LOG_LEVEL=ERROR` and `PYTHONWARNINGS=ignore`) to reduce log verbosity and keep the output clean.\n",
    "\n",
    "Pay close attention to the output logs. You should see a message from each rank confirming which GPU its model partition is on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MI82e0E8-U0Q",
    "outputId": "6d7e52f2-44ba-42ad-ad46-83d254dc851c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Launching DeepSpeed Pipeline Parallelism Demo...\n",
      "Rank 1: My model partition is on device: cuda:1\n",
      "Rank 2: My model partition is on device: cuda:2\n",
      "Rank 3: My model partition is on device: cuda:3\n",
      "Rank 0: My model partition is on device: cuda:0\n",
      "\n",
      "Rank 3 (Last Stage): Inference successful!\n",
      "Final output shape: torch.Size([16, 1024])\n",
      "\n",
      "[2025-07-14 04:53:48,894] [INFO] [launch.py:351:main] Process 25565 exits successfully.\n",
      "[2025-07-14 04:53:48,895] [INFO] [launch.py:351:main] Process 25564 exits successfully.\n",
      "[2025-07-14 04:53:48,895] [INFO] [launch.py:351:main] Process 25567 exits successfully.\n",
      "[2025-07-14 04:53:49,896] [INFO] [launch.py:351:main] Process 25566 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Launching DeepSpeed Pipeline Parallelism Demo...\")\n",
    "!DS_LOG_LEVEL=ERROR PYTHONWARNINGS=ignore deepspeed --num_gpus 4 demo_pipeline_script.py --deepspeed_config ds_config_demo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis and Conclusion\n",
    "\n",
    "You should have seen output similar to this:\n",
    "\n",
    "```\n",
    "Rank 0: My model partition is on device: cuda:0\n",
    "Rank 1: My model partition is on device: cuda:1\n",
    "...\n",
    "Rank 3 (Last Stage): Inference successful!\n",
    "Final output shape: torch.Size()\n",
    "```\n",
    "\n",
    "**Success!** This confirms that:\n",
    "- DeepSpeed successfully launched 4 processes, one for each GPU.\n",
    "- It partitioned our `SequentialDemoModel` and placed each partition on a separate GPU.\n",
    "- It correctly managed the data flow through the pipeline to produce a final result on the last stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijW_kYNe-WZE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
