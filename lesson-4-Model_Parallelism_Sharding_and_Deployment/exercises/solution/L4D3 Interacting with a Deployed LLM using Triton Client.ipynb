{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Interacting with a Deployed LLM using Triton Client\n",
    "\n",
    "In this demo, we will interact with the Triton Inference Server we set up on an EC2 instance previously, that has been running an TensorRT-LLM optimized GPT-2 model. \n",
    "\n",
    "**Our Goal:**\n",
    "1.  Connect to the remote Triton server endpoint.\n",
    "2.  Prepare a prompt and send it to the server.\n",
    "3.  Receive the generated text from our deployed GPT-2 model.\n",
    "\n",
    "This demonstrates a standard, decoupled MLOps architecture where applications interact with a production model via a network API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Server Configuration\n",
    "\n",
    "Next, we'll import the required modules and configure the connection to our Triton server. \n",
    "\n",
    "**IMPORTANT:** You must replace `\"YOUR_EC2_PUBLIC_IP_ADDRESS\"` with the actual public IP address of the EC2 instance where you deployed the Triton server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxYOA1TA-LnE",
    "outputId": "79b5aea3-8855-4719-9251-7ea4ada1f38f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Triton server at: 13.222.9.210:8000\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and Configuration\n",
    "import tritonclient.http as httpclient\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Replace with the Public IP address of your EC2 instance\n",
    "TRITON_SERVER_IP = \"YOUR_EC2_PUBLIC_IP_ADDRESS\"\n",
    "TRITON_SERVER_URL = f\"{TRITON_SERVER_IP}:8000\"\n",
    "MODEL_NAME = \"gpt2\" # This must match the model name in your Triton repository\n",
    "\n",
    "print(f\"Connecting to Triton server at: {TRITON_SERVER_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Connect to the Server and Verify Model Readiness\n",
    "\n",
    "Before sending a prompt, it's good practice to check the connection to the server and ensure our desired model is loaded and ready to accept requests. \n",
    "\n",
    "The `tritonclient` library provides simple methods to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5r_5OP749Is9",
    "outputId": "d6ae4636-455e-4b66-96df-46546eb68603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Server is live!\n",
      "✅ Model 'gpt2' is ready!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create a Triton client\n",
    "    triton_client = httpclient.InferenceServerClient(url=TRITON_SERVER_URL)\n",
    "\n",
    "    # Check if the server is live\n",
    "    if not triton_client.is_server_live():\n",
    "        print(\"❌ Server is not live. Check the IP address and EC2 security group settings.\")\n",
    "    else:\n",
    "        print(\"✅ Server is live!\")\n",
    "\n",
    "    # Check if the model is ready\n",
    "    if not triton_client.is_model_ready(MODEL_NAME):\n",
    "        print(f\"❌ Model '{MODEL_NAME}' is not ready. Check the Triton server logs on your EC2 instance.\")\n",
    "    else:\n",
    "        print(f\"✅ Model '{MODEL_NAME}' is ready!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during client setup: {e}\")\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Inference Function\n",
    "\n",
    "This function encapsulates the logic for interacting with the Triton server. It handles the complete workflow:\n",
    "1.  **Tokenization**: Converts the input text prompt into integer token IDs using the Hugging Face tokenizer.\n",
    "2.  **Prepare Inputs**: Packages the token IDs and other parameters (like desired output length) into the specific `InferInput` format that Triton requires. \n",
    "    * The names of these inputs (`\"input_ids\"`, `\"request_output_len\"`, etc.) must exactly match what is defined in the `config.pbtxt` on the server.\n",
    "3.  **Prepare Outputs**: Specifies which output tensor we want the server to return.\n",
    "4.  **Inference Call**: Sends the request to the server.\n",
    "5.  **Decode Response**: Receives the resulting token IDs and decodes them back into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2m9XZ9mu9Ko5"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define the Inference Function\n",
    "import numpy as np\n",
    "\n",
    "def call_triton_gpt2(prompt: str, max_output_len: int = 50):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Triton server and returns the generated text.\n",
    "    \"\"\"\n",
    "    print(f\"Sending prompt: '{prompt}'\")\n",
    "\n",
    "    # 1. Tokenize the input prompt and prepare other parameters as numpy arrays\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\").astype(np.int32)\n",
    "    input_len = np.array([[input_ids.shape[1]]], dtype=np.int32)\n",
    "    request_output_len = np.array([[max_output_len]], dtype=np.int32)\n",
    "\n",
    "    # 2. Prepare Triton Inputs\n",
    "    # The names of these inputs MUST match the 'input' blocks in the server's config.pbtxt\n",
    "    end_id = np.array([[tokenizer.eos_token_id]], dtype=np.int32)\n",
    "    pad_id = np.array([[tokenizer.eos_token_id]], dtype=np.int32)\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT32\"),\n",
    "        httpclient.InferInput(\"input_lengths\", input_len.shape, \"INT32\"),\n",
    "        httpclient.InferInput(\"request_output_len\", request_output_len.shape, \"INT32\"),\n",
    "        httpclient.InferInput(\"end_id\", end_id.shape, \"INT32\"),\n",
    "        httpclient.InferInput(\"pad_id\", pad_id.shape, \"INT32\"),\n",
    "    ]\n",
    "\n",
    "    # Set the data for each input\n",
    "    inputs[0].set_data_from_numpy(input_ids)\n",
    "    inputs[1].set_data_from_numpy(input_len)\n",
    "    inputs[2].set_data_from_numpy(request_output_len)\n",
    "    inputs[3].set_data_from_numpy(end_id)\n",
    "    inputs[4].set_data_from_numpy(pad_id)\n",
    "\n",
    "    # 3. Prepare Triton Outputs\n",
    "    # The name MUST match the 'output' block in the server's config.pbtxt\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"output_ids\")\n",
    "    ]\n",
    "\n",
    "    # 4. Send the inference request\n",
    "    try:\n",
    "        response = triton_client.infer(model_name=MODEL_NAME, inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # 5. Decode the response\n",
    "        output_ids = response.as_numpy(\"output_ids\")\n",
    "        # The output would also include the prompt\n",
    "        generated_text = tokenizer.decode(output_ids[0])\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during inference: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Demo!\n",
    "\n",
    "Now, let's call our function with a sample prompt and see the response from our deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaOjlMha-QH9",
    "outputId": "cf01c5e9-5766-45fd-b31f-482081b32aa3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending prompt: 'The best thing about AI is'\n",
      "\n",
      "--- Generated Output ---\n",
      "The best thing about AI is that it's not just a tool for humans to do things\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "my_prompt = \"The best thing about AI is\"\n",
    "generated_output = call_triton_gpt2(my_prompt)\n",
    "\n",
    "if generated_output:\n",
    "    print(\"\\n--- Generated Output ---\")\n",
    "    print(generated_output)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "**Success!**\n",
    "\n",
    "We have successfully sent a request from this notebook to our Triton server running on a separate EC2 instance and received a valid completion from the TensorRT-LLM optimized GPT-2 model.\n",
    "\n",
    "This two-part demo showcases a realistic and powerful pattern for production machine learning:\n",
    "- **Infrastructure as a Service (IaaS):** Using EC2 for dedicated, high-performance model serving.\n",
    "- **Platform as a Service (PaaS):** Using notebook workspace for development and client-side logic.\n",
    "- **Decoupled Architecture:** The model server and the client application are independent, communicating over a standard network API. This allows teams to work in parallel and provides scalability and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY_n2y0duuN-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
