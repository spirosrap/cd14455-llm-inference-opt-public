{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: KV-Caching in Action (Conceptual)\n",
    "\n",
    "**Goal:** This demo focuses on showing how `model.generate()` uses Key-Value (KV) Caching by default and conceptually explaining the internal process.\n",
    "\n",
    "We will walk through:\n",
    "1. **Setup:** Installing libraries and configuring the environment.\n",
    "2. **Loading:** Loading a model (`Llama-3.2-1B`) and its tokenizer.\n",
    "3. **Generation:** Calling `model.generate()` to see KV caching in action.\n",
    "4. **Explanation:** Breaking down what happens under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our model configuration. We'll use `meta-llama/Llama-3.2-1B`. We also check for an available CUDA GPU to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: meta-llama/Llama-3.2-1B\n",
      "Using device: cuda\n",
      "Using dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "model_name = \"/voc/shared/models/llama/Llama-3.2-1B\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Use bfloat16 for faster computation if supported on CUDA, otherwise use standard float32\n",
    "dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the tokenizer and the model from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fa75dc46f4488ba01518ee05e42c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f492138f13042dfabea6050f0198b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f936a417ffb24649b9105b5974d6272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d52171c2b4747ecb2aace31b9a392c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3e1f704af14e96a9f6a8e26c2b5995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eca7c1f7f74024ad588cabae705c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and moved to device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "print(\"\\nLoading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=dtype,\n",
    ").to(device)\n",
    "print(\"Model loaded successfully and moved to device.\")\n",
    "\n",
    "# Set the model to evaluation mode (disables dropout, etc.)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models don't have a `pad_token` set by default. We'll set it to the `eos_token` (end-of-sequence) to prevent warnings during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token set to EOS token.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    print(\"Pad token set to EOS token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Text with KV Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define our prompt and tokenize it, preparing it for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"The best way to optimize LLM inference is\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The best way to optimize LLM inference is\"\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the main event. We call `model.generate()`. \n",
    "\n",
    "Crucially, the `use_cache=True` argument is **on by default** for most autoregressive models. This is what enables KV caching. We are explicitly writing it here to make it clear, but you usually don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model.generate() with use_cache=True (default behavior)...\n",
      "--- Generated Text ---\n",
      "The best way to optimize LLM inference is to use a large, diverse dataset. We have a large, diverse dataset of 4.3 million English Wikipedia articles. We are using this dataset to train our LLMs on a variety of tasks, including question answering, summarization, and\n",
      "----------------------\n",
      "(Generation took: 7.5997 seconds)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running model.generate() with use_cache=True (default behavior)...\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "        use_cache=True, # This enables/confirms KV Caching\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "generated_ids = outputs[0]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(f\"--- Generated Text ---\")\n",
    "print(generated_text)\n",
    "print(\"----------------------\")\n",
    "print(f\"(Generation took: {end_time - start_time:.4f} seconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Conceptual Breakdown - What Just Happened?\n",
    "\n",
    "The `model.generate()` call did all the heavy lifting for us, but it was managing a cache behind the scenes. Let's break down how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Prompt Processing (The First Pass)\n",
    "\n",
    "When generation starts, the model first processes the entire input prompt (`\"The best way to optimize LLM inference is\"`) in a single forward pass.\n",
    "\n",
    "- For **every token** in this prompt, the model calculates its corresponding **Key (K)** and **Value (V)** vectors.\n",
    "- These K and V vectors (for the entire prompt) are then stored in a cache, often called `past_key_values`.\n",
    "\n",
    "This initial step is computationally intensive but is only done **once**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Autoregressive Generation (The Token-by-Token Loop)\n",
    "\n",
    "Now, the model generates the rest of the text one token at a time. This is where the cache becomes critical.\n",
    "\n",
    "- **To generate the 1st new token:**\n",
    "    - The model only needs to process the *last token of the prompt*.\n",
    "    - It calculates the **Query (Q)** for this token and uses it to attend to *all the K and V vectors already in the cache*.\n",
    "    - After predicting the new token, it calculates the K and V for *this new token only* and **appends** them to the cache.\n",
    "\n",
    "- **To generate the 2nd new token:**\n",
    "    - The model only processes the *1st new token* it just generated.\n",
    "    - It calculates its Q vector and attends to the *entire, updated cache* (prompt tokens + 1st new token).\n",
    "    - It then computes the K and V for the 2nd new token and appends them to the cache.\n",
    "\n",
    "This loop continues, and at each step, the model avoids re-calculating K and V for all previous tokens. It's a massive computational saving!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Magic of Library Abstraction\n",
    "\n",
    "Libraries like Hugging Face Transformers abstract away this complex state management. By simply using `model.generate()`, we automatically get the benefits of KV Caching without needing to manually handle the `past_key_values` object at each step.\n",
    "\n",
    "### Final Takeaways\n",
    "\n",
    "- **What it is:** A technique to store and reuse Key/Value vectors of past tokens during autoregressive generation.\n",
    "- **Why it's used:** To dramatically **reduce computation** and **lower latency** (speed up generation time per token).\n",
    "- **The Trade-off:** It consumes **more memory (VRAM)** because the cache grows linearly with every new token generated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
