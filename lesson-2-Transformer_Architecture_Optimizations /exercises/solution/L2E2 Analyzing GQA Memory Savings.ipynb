{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Analyzing GQA Memory Savings\n",
    "\n",
    "**Goal:** To understand and quantify the memory efficiency of Grouped Query Attention (GQA) by inspecting the configurations of two models: `meta-llama/Llama-3.2-1B` (which uses GQA) and `openai-community/gpt2-xl` (which uses Multi-Head Attention - MHA).\n",
    "\n",
    "**Task:** You will write a Python script to load the configurations of the two specified models. From these configurations, you'll extract key parameters like the number of layers, number of query heads, number of key/value heads, hidden size, and head dimension. Using these, you will calculate the memory required by the KV cache for each new token generated, specifically focusing on a per-layer basis for a normalized comparison, and then the total per token. Finally, you will estimate the impact of these cache sizes on the maximum supportable sequence length given a fixed VRAM budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Environment\n",
    "\n",
    "This exercise is purely computational based on model configurations and does not require a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analysis Helper Function\n",
    "\n",
    "**Logic:** To avoid repetitive code, we will create a single, robust function, `analyze_model_kv_cache`. This function will encapsulate the entire process for one model: loading its configuration, extracting the required parameters, performing the cache size calculations, and returning a structured dictionary of the results. This makes our main script clean and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_kv_cache(model_name, model_label):\n",
    "    \"\"\"Loads a model's config, extracts params, and calculates its KV cache size.\"\"\"\n",
    "    print(f\"\\n--- Analyzing {model_label}: {model_name} ---\")\n",
    "    \n",
    "    # --- Parameter Extraction ---\n",
    "    # config = AutoConfig.from_pretrained(model_name)\n",
    "    # model_dtype = getattr(config, 'torch_dtype', torch.float16) # Assume float16 if not specified\n",
    "\n",
    "    temp_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model_dtype = temp_model.dtype\n",
    "    config = temp_model.config\n",
    "    del temp_model # Free up memory\n",
    "\n",
    "    L = config.num_hidden_layers\n",
    "    N_q = config.num_attention_heads\n",
    "    # For MHA models like GPT-2, N_kv is not specified, so we default to N_q.\n",
    "    N_kv = getattr(config, 'num_key_value_heads', N_q)\n",
    "    D_model = config.hidden_size\n",
    "    D_head = D_model // N_q\n",
    "    dtype_size_bytes = torch.finfo(model_dtype).bits // 8 if model_dtype.is_floating_point else 2\n",
    "\n",
    "    print(f\"  Number of Layers (L):        {L}\")\n",
    "    print(f\"  Query Heads (N_q):             {N_q}\")\n",
    "    print(f\"  Key/Value Heads (N_kv):        {N_kv}\")\n",
    "    print(f\"  Head Dimension (D_head):       {D_head}\")\n",
    "    print(f\"  Data Type Size (bytes):      {dtype_size_bytes} ({model_dtype})\")\n",
    "\n",
    "    # --- Computation ---\n",
    "    # KV Cache size (bytes) = 2 (K&V) * L * N_kv * D_head * dtype_size\n",
    "    total_cache_per_token_bytes = 2 * L * N_kv * D_head * dtype_size_bytes\n",
    "    cache_per_layer_per_token_bytes = 2 * N_kv * D_head * dtype_size_bytes\n",
    "    \n",
    "    # --- Identify Attention Type ---\n",
    "    if N_kv == N_q:\n",
    "        attention_type = \"Multi-Head Attention (MHA)\"\n",
    "    elif 1 < N_kv < N_q:\n",
    "        attention_type = \"Grouped-Query Attention (GQA)\"\n",
    "    else:\n",
    "        attention_type = \"Multi-Query Attention (MQA)\"\n",
    "\n",
    "    print(f\"  Identified Attention Type:     {attention_type}\")\n",
    "\n",
    "    results = {\n",
    "        \"label\": model_label, \"L\": L, \"N_q\": N_q, \"N_kv\": N_kv, \"D_head\": D_head,\n",
    "        \"dtype_size_bytes\": dtype_size_bytes,\n",
    "        \"attention_type\": attention_type,\n",
    "        \"total_cache_per_token_bytes\": total_cache_per_token_bytes,\n",
    "        \"cache_per_layer_per_token_bytes\": cache_per_layer_per_token_bytes\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the Analysis\n",
    "\n",
    "**Logic:** We define the models we want to compare and then call our helper function for each one. This will print the extracted parameters and calculated values for both the GQA model (Llama-3.2-1B) and the MHA model (GPT-2 XL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Llama-3.2-1B (GQA): meta-llama/Llama-3.2-1B ---\n",
      "  Number of Layers (L):        16\n",
      "  Query Heads (N_q):             32\n",
      "  Key/Value Heads (N_kv):        8\n",
      "  Head Dimension (D_head):       64\n",
      "  Data Type Size (bytes):      4 (torch.float32)\n",
      "  Identified Attention Type:     Grouped-Query Attention (GQA)\n",
      "\n",
      "--- Analyzing GPT-2 XL (MHA): openai-community/gpt2-xl ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49727da28f87410685e2c05ad73eef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848fe350e21d45e0befae2b117cc330d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of Layers (L):        48\n",
      "  Query Heads (N_q):             25\n",
      "  Key/Value Heads (N_kv):        25\n",
      "  Head Dimension (D_head):       64\n",
      "  Data Type Size (bytes):      4 (torch.float32)\n",
      "  Identified Attention Type:     Multi-Head Attention (MHA)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "model_name_gqa = \"/voc/shared/models/llama/Llama-3.2-1B\"\n",
    "model_name_mha = \"openai-community/gpt2-xl\"\n",
    "\n",
    "results_gqa = analyze_model_kv_cache(model_name_gqa, \"Llama-3.2-1B (GQA)\")\n",
    "results_mha = analyze_model_kv_cache(model_name_mha, \"GPT-2 XL (MHA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare Results and Quantify Savings\n",
    "\n",
    "**Logic:** Now we will process the dictionaries returned by our analysis function to present a clear, side-by-side comparison. We will focus on two key metrics:\n",
    "1.  **Per-Layer Cache Size:** This normalizes the comparison by ignoring the total number of layers, allowing us to see the direct architectural advantage of GQA vs. MHA.\n",
    "2.  **Internal Saving Factor:** For the Llama model, we calculate how much memory it saves with GQA compared to a hypothetical version of *itself* with MHA. This isolates the benefit of GQA within a single architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- DETAILED ANALYSIS & COMPARISON ---\n",
      "\n",
      "--- Direct Comparison (Per Layer, Per Token KV Cache) ---\n",
      "  Llama-3.2-1B (GQA): 4.00 KB/layer/token\n",
      "  GPT-2 XL (MHA):     12.50 KB/layer/token\n",
      "\n",
      "--- GQA Internal Saving Factor (for Llama-3.2-1B) ---\n",
      "  Llama-3.2-1B uses 8 KV heads for 32 Query heads.\n",
      "  This provides an internal KV cache saving factor of 4.00x compared to if it used MHA.\n"
     ]
    }
   ],
   "source": [
    "if results_gqa and results_mha:\n",
    "    print(\"\\n\\n--- DETAILED ANALYSIS & COMPARISON ---\")\n",
    "    \n",
    "    # --- Per-Layer Comparison ---\n",
    "    gqa_cache_per_layer_kb = results_gqa['cache_per_layer_per_token_bytes'] / 1024\n",
    "    mha_cache_per_layer_kb = results_mha['cache_per_layer_per_token_bytes'] / 1024\n",
    "    print(\"\\n--- Direct Comparison (Per Layer, Per Token KV Cache) ---\")\n",
    "    print(f\"  Llama-3.2-1B (GQA): {gqa_cache_per_layer_kb:.2f} KB/layer/token\")\n",
    "    print(f\"  GPT-2 XL (MHA):     {mha_cache_per_layer_kb:.2f} KB/layer/token\")\n",
    "\n",
    "    # --- GQA Internal Saving Factor ---\n",
    "    internal_saving_factor = results_gqa['N_q'] / results_gqa['N_kv']\n",
    "    print(\"\\n--- GQA Internal Saving Factor (for Llama-3.2-1B) ---\")\n",
    "    print(f\"  Llama-3.2-1B uses {results_gqa['N_kv']} KV heads for {results_gqa['N_q']} Query heads.\")\n",
    "    print(f\"  This provides an internal KV cache saving factor of {internal_saving_factor:.2f}x compared to if it used MHA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Practical Implications\n",
    "\n",
    "**Logic:** To make these theoretical numbers concrete, we'll estimate a real-world consequence: the maximum sequence length each model can support. We use the **total** KV cache size per token (which accounts for all layers) and a fixed VRAM budget to see how GQA's efficiency translates into a practical advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Max Sequence Length Estimation (with 6 GB VRAM for Cache) ---\n",
      "  Llama-3.2-1B (GQA) can support a sequence length of ~98,304 tokens.\n",
      "  GPT-2 XL (MHA) can support a sequence length of     ~10,485 tokens.\n"
     ]
    }
   ],
   "source": [
    "if results_gqa and results_mha:\n",
    "    vram_budget_mb = 6 * 1024  # 6 GB expressed in MB\n",
    "    \n",
    "    gqa_total_mb_per_token = results_gqa['total_cache_per_token_bytes'] / (1024*1024)\n",
    "    mha_total_mb_per_token = results_mha['total_cache_per_token_bytes'] / (1024*1024)\n",
    "    \n",
    "    max_tokens_gqa = vram_budget_mb / gqa_total_mb_per_token\n",
    "    max_tokens_mha = vram_budget_mb / mha_total_mb_per_token\n",
    "    \n",
    "    print(f\"\\n--- Max Sequence Length Estimation (with {vram_budget_mb / 1024:.0f} GB VRAM for Cache) ---\")\n",
    "    print(f\"  Llama-3.2-1B (GQA) can support a sequence length of ~{int(max_tokens_gqa):,} tokens.\")\n",
    "    print(f\"  GPT-2 XL (MHA) can support a sequence length of     ~{int(max_tokens_mha):,} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Conclusion\n",
    "\n",
    "This exercise quantified and compared the theoretical KV cache memory usage for `meta-llama/Llama-3.2-1B` (GQA) and `openai-community/gpt2-xl` (MHA).\n",
    "\n",
    "#### **Model 1: `meta-llama/Llama-3.2-1B` (GQA)**\n",
    "- **Configuration:** L=16, N_q=32, N_kv=8, D_head=64, dtype=float16 (2 bytes).\n",
    "- **Attention Mechanism:** Identified as **Grouped Query Attention (GQA)** with a **Grouping Factor of 4** (32 Query heads / 8 KV heads).\n",
    "- **KV Cache Memory:**\n",
    "    - Per Layer, Per Token: **2.00 KB** (calculated as `2 * 8 * 64 * 2 bytes`).\n",
    "    - Total Per Token (all 16 layers): **0.0313 MB**.\n",
    "\n",
    "#### **Model 2: `openai-community/gpt2-xl` (MHA)**\n",
    "- **Configuration:** L=48, N_q=25, N_kv=25, D_head=64, dtype=float16 (2 bytes).\n",
    "- **Attention Mechanism:** Identified as **Multi-Head Attention (MHA)**.\n",
    "- **KV Cache Memory:**\n",
    "    - Per Layer, Per Token: **6.25 KB** (calculated as `2 * 25 * 64 * 2 bytes`).\n",
    "    - Total Per Token (all 48 layers): **0.2930 MB**.\n",
    "\n",
    "#### **Direct Comparison and Savings**\n",
    "- **Per-Layer Efficiency:** The per-layer KV cache for Llama-3.2-1B (GQA) at **2.00 KB** is significantly smaller than GPT-2 XL's MHA cache at **6.25 KB**. This demonstrates that GQA's architecture is inherently more memory-efficient at the layer level, primarily due to using far fewer K/V heads (8 vs. 25).\n",
    "- **Internal Saving Factor:** Llama-3.2-1B's GQA provides a **4x** memory saving for its KV cache compared to if it had used a traditional MHA design (`32 / 8 = 4`).\n",
    "\n",
    "#### **Practical Implication (Max Sequence Length with 6GB VRAM for Cache)**\n",
    "- **Llama-3.2-1B (GQA):** Could theoretically support a sequence of ~**196,608 tokens**.\n",
    "- **GPT-2 XL (MHA):** Could theoretically support a sequence of ~**20,971 tokens**.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "The exercise successfully demonstrates that **Grouped-Query Attention is a highly effective technique for reducing the memory footprint of the KV Cache**. By using fewer Key/Value heads than Query heads, GQA significantly lowers the amount of data that must be stored for each generated token. This architectural improvement, as shown by the comparison, directly translates into a much larger maximum context length, enabling modern models like Llama-3.2-1B to handle longer sequences far more efficiently than older MHA-based models like GPT-2 XL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
