{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Observing Attention Configuration (MHA, MQA, GQA)\n",
    "\n",
    "**Goal:** This demo shows how to inspect a model's configuration to determine its attention mechanism (Multi-Head, Multi-Query, or Grouped-Query Attention). Understanding this is key to predicting a model's memory usage for the KV Cache.\n",
    "\n",
    "We will:\n",
    "1. **Setup:** Install libraries and log in to Hugging Face.\n",
    "2. **Load Configuration:** Efficiently load only the model's configuration file.\n",
    "3. **Inspect Heads:** Extract the number of query heads and key/value heads.\n",
    "4. **Determine Type:** Apply logic to identify the attention type.\n",
    "5. **Interpret Results:** Understand the implications of the finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model Configuration\n",
    "\n",
    "For this task, we don't need to load the entire model (which can be billions of parameters and take up a lot of VRAM). We only need its architectural details, which are stored in the `config.json` file. The `AutoConfig` class lets us load just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration for: meta-llama/Llama-3.2-1B\n",
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "model_name = \"/voc/shared/models/llama/Llama-3.2-1B\"\n",
    "\n",
    "print(f\"Loading configuration for: {model_name}\")\n",
    "try:\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    print(\"Configuration loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inspect Key Attention Attributes\n",
    "\n",
    "The two most important attributes for determining the attention type are:\n",
    "- `num_attention_heads`: The number of attention heads for the **Query (Q)** projections.\n",
    "- `num_key_value_heads`: The number of attention heads for the **Key (K) and Value (V)** projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Attributes for 'meta-llama/Llama-3.2-1B':\n",
      "  Number of Query Heads (N_q):         32\n",
      "  Number of Key/Value Heads (N_kv):    8\n"
     ]
    }
   ],
   "source": [
    "# Extract the number of heads from the configuration object\n",
    "num_q_heads = config.num_attention_heads\n",
    "num_kv_heads = config.num_key_value_heads # This field is specific to MQA/GQA models\n",
    "\n",
    "print(f\"Extracted Attributes for '{model_name}':\")\n",
    "print(f\"  Number of Query Heads (N_q):         {num_q_heads}\")\n",
    "print(f\"  Number of Key/Value Heads (N_kv):    {num_kv_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Determine the Attention Type\n",
    "\n",
    "Now we can apply simple logic based on the two numbers we just extracted:\n",
    "- If `N_q == N_kv`, it's **Multi-Head Attention (MHA)**.\n",
    "- If `N_kv == 1`, it's **Multi-Query Attention (MQA)**.\n",
    "- If `1 < N_kv < N_q`, it's **Grouped-Query Attention (GQA)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the head counts, the detected attention type is: Grouped-Query Attention (GQA)\n"
     ]
    }
   ],
   "source": [
    "attention_type = \"Unknown\"\n",
    "\n",
    "if num_kv_heads == num_q_heads:\n",
    "    attention_type = \"Multi-Head Attention (MHA)\"\n",
    "elif num_kv_heads == 1:\n",
    "    attention_type = \"Multi-Query Attention (MQA)\"\n",
    "elif 1 < num_kv_heads < num_q_heads:\n",
    "    attention_type = \"Grouped-Query Attention (GQA)\"\n",
    "\n",
    "print(f\"Based on the head counts, the detected attention type is: {attention_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Grouping Factor for GQA\n",
    "\n",
    "Since we detected GQA, we can calculate the *grouping factor*â€”that is, how many Query heads share a single Key/Value head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping Factor = 32 (Query Heads) / 8 (KV Heads) = 4\n",
      "This means every 4 Query heads share a single set of Key and Value heads.\n"
     ]
    }
   ],
   "source": [
    "if attention_type == \"Grouped-Query Attention (GQA)\":\n",
    "    if num_q_heads % num_kv_heads == 0:\n",
    "        group_factor = num_q_heads // num_kv_heads\n",
    "        print(f\"Grouping Factor = {num_q_heads} (Query Heads) / {num_kv_heads} (KV Heads) = {group_factor}\")\n",
    "        print(f\"This means every {group_factor} Query heads share a single set of Key and Value heads.\")\n",
    "    else:\n",
    "        print(\"GQA detected, but heads are not evenly divisible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Interpretation\n",
    "\n",
    "For the model **`meta-llama/Llama-3.2-1B`**, we have confirmed it uses **Grouped-Query Attention (GQA)**.\n",
    "\n",
    "**Why this matters:**\n",
    "- **Reduced Memory:** A standard MHA model would have needed 32 sets of Key/Value heads in its KV Cache. By using only 8, GQA reduces the KV Cache size by a factor of 4 (32 / 8).\n",
    "- **Faster Inference:** A smaller KV Cache means less data needs to be read from slow GPU memory (HBM) at each generation step, which reduces the memory bandwidth bottleneck and speeds up inference.\n",
    "- **Longer Context:** The memory savings from GQA allow the model to handle longer sequences of text before running out of VRAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
